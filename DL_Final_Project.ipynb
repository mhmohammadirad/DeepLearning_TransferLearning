{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsfZPk3vt69Y"
      },
      "source": [
        "# Task A: Fine-tune a pretrained model using the DeepDRiD dataset.\n",
        "1.\n",
        "Download the DeepDRiD dataset from the provided link along with the template code which would provide you with an explanation on diabetic retinopathy and the dataset itself. All the images for training, validation, and evaluation are 512 by 512 in size.\n",
        "\n",
        "2.\n",
        "Fine-tune an ImageNet pretrained model (e.g., ResNet18, ResNet34, VGG, EfficientNet, DenseNet) on the DeepDRiD dataset. Evaluate and test it on the validation and test sets. Your goal here is to reach the highest Cohen Kappa score you can get.\n",
        "\n",
        "3.\n",
        "Play around different image augmentation techniques and check whether it boosts your evaluation metrics or decreases them.\n",
        "\n",
        "4.\n",
        "Save the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T57ohnC7RVLF"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from sklearn.metrics import cohen_kappa_score, precision_score, recall_score, accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "eREBQBPrzpTH",
        "outputId": "418a3313-0a3d-496e-b465-21198c2578a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.12.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1PK8X-rkPtE1fsSSF_nLUyjghzL7VPT-0\n",
            "From (redirected): https://drive.google.com/uc?id=1PK8X-rkPtE1fsSSF_nLUyjghzL7VPT-0&confirm=t&uuid=e8a6703a-a195-42b8-a2db-e55be34483cd\n",
            "To: /content/DeepDRiD.zip\n",
            "100%|██████████| 99.1M/99.1M [00:01<00:00, 59.3MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DeepDRiD.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "!pip install gdown\n",
        "file_id = \"1PK8X-rkPtE1fsSSF_nLUyjghzL7VPT-0\"\n",
        "url = f\"https://drive.google.com/uc?id=1PK8X-rkPtE1fsSSF_nLUyjghzL7VPT-0\"\n",
        "output = \"DeepDRiD.zip\"\n",
        "gdown.download(url, output, quiet=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zMuoMENpTVUd",
        "outputId": "195aca17-cc88-45ff-f1d4-053acccc31ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  DeepDRiD.zip\n",
            "   creating: DeepDRiD/\n",
            "  inflating: DeepDRiD/sample_submission.csv  \n",
            "   creating: DeepDRiD/test/\n",
            "   creating: DeepDRiD/test/347/\n",
            "  inflating: DeepDRiD/test/347/347_l1.jpg  \n",
            "  inflating: DeepDRiD/test/347/347_l2.jpg  \n",
            "  inflating: DeepDRiD/test/347/347_r1.jpg  \n",
            "  inflating: DeepDRiD/test/347/347_r2.jpg  \n",
            "   creating: DeepDRiD/test/353/\n",
            "  inflating: DeepDRiD/test/353/353_l1.jpg  \n",
            "  inflating: DeepDRiD/test/353/353_l2.jpg  \n",
            "  inflating: DeepDRiD/test/353/353_r1.jpg  \n",
            "  inflating: DeepDRiD/test/353/353_r2.jpg  \n",
            "   creating: DeepDRiD/test/354/\n",
            "  inflating: DeepDRiD/test/354/354_l1.jpg  \n",
            "  inflating: DeepDRiD/test/354/354_l2.jpg  \n",
            "  inflating: DeepDRiD/test/354/354_r1.jpg  \n",
            "  inflating: DeepDRiD/test/354/354_r2.jpg  \n",
            "   creating: DeepDRiD/test/366/\n",
            "  inflating: DeepDRiD/test/366/366_l1.jpg  \n",
            "  inflating: DeepDRiD/test/366/366_l2.jpg  \n",
            "  inflating: DeepDRiD/test/366/366_r1.jpg  \n",
            "  inflating: DeepDRiD/test/366/366_r2.jpg  \n",
            "   creating: DeepDRiD/test/368/\n",
            "  inflating: DeepDRiD/test/368/368_l1.jpg  \n",
            "  inflating: DeepDRiD/test/368/368_l2.jpg  \n",
            "  inflating: DeepDRiD/test/368/368_r1.jpg  \n",
            "  inflating: DeepDRiD/test/368/368_r2.jpg  \n",
            "   creating: DeepDRiD/test/371/\n",
            "  inflating: DeepDRiD/test/371/371_l1.jpg  \n",
            "  inflating: DeepDRiD/test/371/371_l2.jpg  \n",
            "  inflating: DeepDRiD/test/371/371_r1.jpg  \n",
            "  inflating: DeepDRiD/test/371/371_r2.jpg  \n",
            "   creating: DeepDRiD/test/377/\n",
            "  inflating: DeepDRiD/test/377/377_l1.jpg  \n",
            "  inflating: DeepDRiD/test/377/377_l2.jpg  \n",
            "  inflating: DeepDRiD/test/377/377_r1.jpg  \n",
            "  inflating: DeepDRiD/test/377/377_r2.jpg  \n",
            "   creating: DeepDRiD/test/383/\n",
            "  inflating: DeepDRiD/test/383/383_l1.jpg  \n",
            "  inflating: DeepDRiD/test/383/383_l2.jpg  \n",
            "  inflating: DeepDRiD/test/383/383_r1.jpg  \n",
            "  inflating: DeepDRiD/test/383/383_r2.jpg  \n",
            "   creating: DeepDRiD/test/384/\n",
            "  inflating: DeepDRiD/test/384/384_l1.jpg  \n",
            "  inflating: DeepDRiD/test/384/384_l2.jpg  \n",
            "  inflating: DeepDRiD/test/384/384_r1.jpg  \n",
            "  inflating: DeepDRiD/test/384/384_r2.jpg  \n",
            "   creating: DeepDRiD/test/386/\n",
            "  inflating: DeepDRiD/test/386/386_l1.jpg  \n",
            "  inflating: DeepDRiD/test/386/386_l2.jpg  \n",
            "  inflating: DeepDRiD/test/386/386_r1.jpg  \n",
            "  inflating: DeepDRiD/test/386/386_r2.jpg  \n",
            "   creating: DeepDRiD/test/387/\n",
            "  inflating: DeepDRiD/test/387/387_l1.jpg  \n",
            "  inflating: DeepDRiD/test/387/387_l2.jpg  \n",
            "  inflating: DeepDRiD/test/387/387_r1.jpg  \n",
            "  inflating: DeepDRiD/test/387/387_r2.jpg  \n",
            "   creating: DeepDRiD/test/389/\n",
            "  inflating: DeepDRiD/test/389/389_l1.jpg  \n",
            "  inflating: DeepDRiD/test/389/389_l2.jpg  \n",
            "  inflating: DeepDRiD/test/389/389_r1.jpg  \n",
            "  inflating: DeepDRiD/test/389/389_r2.jpg  \n",
            "   creating: DeepDRiD/test/391/\n",
            "  inflating: DeepDRiD/test/391/391_l1.jpg  \n",
            "  inflating: DeepDRiD/test/391/391_l2.jpg  \n",
            "  inflating: DeepDRiD/test/391/391_r1.jpg  \n",
            "  inflating: DeepDRiD/test/391/391_r2.jpg  \n",
            "   creating: DeepDRiD/test/396/\n",
            "  inflating: DeepDRiD/test/396/396_l1.jpg  \n",
            "  inflating: DeepDRiD/test/396/396_l2.jpg  \n",
            "  inflating: DeepDRiD/test/396/396_r1.jpg  \n",
            "  inflating: DeepDRiD/test/396/396_r2.jpg  \n",
            "   creating: DeepDRiD/test/398/\n",
            "  inflating: DeepDRiD/test/398/398_l1.jpg  \n",
            "  inflating: DeepDRiD/test/398/398_l2.jpg  \n",
            "  inflating: DeepDRiD/test/398/398_r1.jpg  \n",
            "  inflating: DeepDRiD/test/398/398_r2.jpg  \n",
            "   creating: DeepDRiD/test/403/\n",
            "  inflating: DeepDRiD/test/403/403_l1.jpg  \n",
            "  inflating: DeepDRiD/test/403/403_l2.jpg  \n",
            "  inflating: DeepDRiD/test/403/403_r1.jpg  \n",
            "  inflating: DeepDRiD/test/403/403_r2.jpg  \n",
            "   creating: DeepDRiD/test/408/\n",
            "  inflating: DeepDRiD/test/408/408_l1.jpg  \n",
            "  inflating: DeepDRiD/test/408/408_l2.jpg  \n",
            "  inflating: DeepDRiD/test/408/408_r1.jpg  \n",
            "  inflating: DeepDRiD/test/408/408_r2.jpg  \n",
            "   creating: DeepDRiD/test/409/\n",
            "  inflating: DeepDRiD/test/409/409_l1.jpg  \n",
            "  inflating: DeepDRiD/test/409/409_l2.jpg  \n",
            "  inflating: DeepDRiD/test/409/409_r1.jpg  \n",
            "  inflating: DeepDRiD/test/409/409_r2.jpg  \n",
            "   creating: DeepDRiD/test/411/\n",
            "  inflating: DeepDRiD/test/411/411_l1.jpg  \n",
            "  inflating: DeepDRiD/test/411/411_l2.jpg  \n",
            "  inflating: DeepDRiD/test/411/411_r1.jpg  \n",
            "  inflating: DeepDRiD/test/411/411_r2.jpg  \n",
            "   creating: DeepDRiD/test/412/\n",
            "  inflating: DeepDRiD/test/412/412_l1.jpg  \n",
            "  inflating: DeepDRiD/test/412/412_l2.jpg  \n",
            "  inflating: DeepDRiD/test/412/412_r1.jpg  \n",
            "  inflating: DeepDRiD/test/412/412_r2.jpg  \n",
            "   creating: DeepDRiD/test/417/\n",
            "  inflating: DeepDRiD/test/417/417_l1.jpg  \n",
            "  inflating: DeepDRiD/test/417/417_l2.jpg  \n",
            "  inflating: DeepDRiD/test/417/417_r1.jpg  \n",
            "  inflating: DeepDRiD/test/417/417_r2.jpg  \n",
            "   creating: DeepDRiD/test/418/\n",
            "  inflating: DeepDRiD/test/418/418_l1.jpg  \n",
            "  inflating: DeepDRiD/test/418/418_l2.jpg  \n",
            "  inflating: DeepDRiD/test/418/418_r1.jpg  \n",
            "  inflating: DeepDRiD/test/418/418_r2.jpg  \n",
            "   creating: DeepDRiD/test/419/\n",
            "  inflating: DeepDRiD/test/419/419_l1.jpg  \n",
            "  inflating: DeepDRiD/test/419/419_l2.jpg  \n",
            "  inflating: DeepDRiD/test/419/419_r1.jpg  \n",
            "  inflating: DeepDRiD/test/419/419_r2.jpg  \n",
            "   creating: DeepDRiD/test/420/\n",
            "  inflating: DeepDRiD/test/420/420_l1.jpg  \n",
            "  inflating: DeepDRiD/test/420/420_l2.jpg  \n",
            "  inflating: DeepDRiD/test/420/420_r1.jpg  \n",
            "  inflating: DeepDRiD/test/420/420_r2.jpg  \n",
            "   creating: DeepDRiD/test/421/\n",
            "  inflating: DeepDRiD/test/421/421_l1.jpg  \n",
            "  inflating: DeepDRiD/test/421/421_l2.jpg  \n",
            "  inflating: DeepDRiD/test/421/421_r1.jpg  \n",
            "  inflating: DeepDRiD/test/421/421_r2.jpg  \n",
            "   creating: DeepDRiD/test/423/\n",
            "  inflating: DeepDRiD/test/423/423_l1.jpg  \n",
            "  inflating: DeepDRiD/test/423/423_l2.jpg  \n",
            "  inflating: DeepDRiD/test/423/423_r1.jpg  \n",
            "  inflating: DeepDRiD/test/423/423_r2.jpg  \n",
            "   creating: DeepDRiD/test/424/\n",
            "  inflating: DeepDRiD/test/424/424_l1.jpg  \n",
            "  inflating: DeepDRiD/test/424/424_l2.jpg  \n",
            "  inflating: DeepDRiD/test/424/424_r1.jpg  \n",
            "  inflating: DeepDRiD/test/424/424_r2.jpg  \n",
            "   creating: DeepDRiD/test/425/\n",
            "  inflating: DeepDRiD/test/425/425_l1.jpg  \n",
            "  inflating: DeepDRiD/test/425/425_l2.jpg  \n",
            "  inflating: DeepDRiD/test/425/425_r1.jpg  \n",
            "  inflating: DeepDRiD/test/425/425_r2.jpg  \n",
            "   creating: DeepDRiD/test/426/\n",
            "  inflating: DeepDRiD/test/426/426_l1.jpg  \n",
            "  inflating: DeepDRiD/test/426/426_l2.jpg  \n",
            "  inflating: DeepDRiD/test/426/426_r1.jpg  \n",
            "  inflating: DeepDRiD/test/426/426_r2.jpg  \n",
            "   creating: DeepDRiD/test/427/\n",
            "  inflating: DeepDRiD/test/427/427_l1.jpg  \n",
            "  inflating: DeepDRiD/test/427/427_l2.jpg  \n",
            "  inflating: DeepDRiD/test/427/427_r1.jpg  \n",
            "  inflating: DeepDRiD/test/427/427_r2.jpg  \n",
            "   creating: DeepDRiD/test/428/\n",
            "  inflating: DeepDRiD/test/428/428_l1.jpg  \n",
            "  inflating: DeepDRiD/test/428/428_l2.jpg  \n",
            "  inflating: DeepDRiD/test/428/428_r1.jpg  \n",
            "  inflating: DeepDRiD/test/428/428_r2.jpg  \n",
            "   creating: DeepDRiD/test/429/\n",
            "  inflating: DeepDRiD/test/429/429_l1.jpg  \n",
            "  inflating: DeepDRiD/test/429/429_l2.jpg  \n",
            "  inflating: DeepDRiD/test/429/429_r1.jpg  \n",
            "  inflating: DeepDRiD/test/429/429_r2.jpg  \n",
            "   creating: DeepDRiD/test/432/\n",
            "  inflating: DeepDRiD/test/432/432_l1.jpg  \n",
            "  inflating: DeepDRiD/test/432/432_l2.jpg  \n",
            "  inflating: DeepDRiD/test/432/432_r1.jpg  \n",
            "  inflating: DeepDRiD/test/432/432_r2.jpg  \n",
            "   creating: DeepDRiD/test/434/\n",
            "  inflating: DeepDRiD/test/434/434_l1.jpg  \n",
            "  inflating: DeepDRiD/test/434/434_l2.jpg  \n",
            "  inflating: DeepDRiD/test/434/434_r1.jpg  \n",
            "  inflating: DeepDRiD/test/434/434_r2.jpg  \n",
            "   creating: DeepDRiD/test/435/\n",
            "  inflating: DeepDRiD/test/435/435_l1.jpg  \n",
            "  inflating: DeepDRiD/test/435/435_l2.jpg  \n",
            "  inflating: DeepDRiD/test/435/435_r1.jpg  \n",
            "  inflating: DeepDRiD/test/435/435_r2.jpg  \n",
            "   creating: DeepDRiD/test/436/\n",
            "  inflating: DeepDRiD/test/436/436_l1.jpg  \n",
            "  inflating: DeepDRiD/test/436/436_l2.jpg  \n",
            "  inflating: DeepDRiD/test/436/436_r1.jpg  \n",
            "  inflating: DeepDRiD/test/436/436_r2.jpg  \n",
            "   creating: DeepDRiD/test/437/\n",
            "  inflating: DeepDRiD/test/437/437_l1.jpg  \n",
            "  inflating: DeepDRiD/test/437/437_l2.jpg  \n",
            "  inflating: DeepDRiD/test/437/437_r1.jpg  \n",
            "  inflating: DeepDRiD/test/437/437_r2.jpg  \n",
            "   creating: DeepDRiD/test/438/\n",
            "  inflating: DeepDRiD/test/438/438_l1.jpg  \n",
            "  inflating: DeepDRiD/test/438/438_l2.jpg  \n",
            "  inflating: DeepDRiD/test/438/438_r1.jpg  \n",
            "  inflating: DeepDRiD/test/438/438_r2.jpg  \n",
            "   creating: DeepDRiD/test/439/\n",
            "  inflating: DeepDRiD/test/439/439_l1.jpg  \n",
            "  inflating: DeepDRiD/test/439/439_l2.jpg  \n",
            "  inflating: DeepDRiD/test/439/439_r1.jpg  \n",
            "  inflating: DeepDRiD/test/439/439_r2.jpg  \n",
            "   creating: DeepDRiD/test/440/\n",
            "  inflating: DeepDRiD/test/440/440_l1.jpg  \n",
            "  inflating: DeepDRiD/test/440/440_l2.jpg  \n",
            "  inflating: DeepDRiD/test/440/440_r1.jpg  \n",
            "  inflating: DeepDRiD/test/440/440_r2.jpg  \n",
            "   creating: DeepDRiD/test/441/\n",
            "  inflating: DeepDRiD/test/441/441_l1.jpg  \n",
            "  inflating: DeepDRiD/test/441/441_l2.jpg  \n",
            "  inflating: DeepDRiD/test/441/441_r1.jpg  \n",
            "  inflating: DeepDRiD/test/441/441_r2.jpg  \n",
            "   creating: DeepDRiD/test/442/\n",
            "  inflating: DeepDRiD/test/442/442_l1.jpg  \n",
            "  inflating: DeepDRiD/test/442/442_l2.jpg  \n",
            "  inflating: DeepDRiD/test/442/442_r1.jpg  \n",
            "  inflating: DeepDRiD/test/442/442_r2.jpg  \n",
            "   creating: DeepDRiD/test/443/\n",
            "  inflating: DeepDRiD/test/443/443_l1.jpg  \n",
            "  inflating: DeepDRiD/test/443/443_l2.jpg  \n",
            "  inflating: DeepDRiD/test/443/443_r1.jpg  \n",
            "  inflating: DeepDRiD/test/443/443_r2.jpg  \n",
            "   creating: DeepDRiD/test/444/\n",
            "  inflating: DeepDRiD/test/444/444_l1.jpg  \n",
            "  inflating: DeepDRiD/test/444/444_l2.jpg  \n",
            "  inflating: DeepDRiD/test/444/444_r1.jpg  \n",
            "  inflating: DeepDRiD/test/444/444_r2.jpg  \n",
            "   creating: DeepDRiD/test/445/\n",
            "  inflating: DeepDRiD/test/445/445_l1.jpg  \n",
            "  inflating: DeepDRiD/test/445/445_l2.jpg  \n",
            "  inflating: DeepDRiD/test/445/445_r1.jpg  \n",
            "  inflating: DeepDRiD/test/445/445_r2.jpg  \n",
            "   creating: DeepDRiD/test/446/\n",
            "  inflating: DeepDRiD/test/446/446_l1.jpg  \n",
            "  inflating: DeepDRiD/test/446/446_l2.jpg  \n",
            "  inflating: DeepDRiD/test/446/446_r1.jpg  \n",
            "  inflating: DeepDRiD/test/446/446_r2.jpg  \n",
            "   creating: DeepDRiD/test/447/\n",
            "  inflating: DeepDRiD/test/447/447_l1.jpg  \n",
            "  inflating: DeepDRiD/test/447/447_l2.jpg  \n",
            "  inflating: DeepDRiD/test/447/447_r1.jpg  \n",
            "  inflating: DeepDRiD/test/447/447_r2.jpg  \n",
            "   creating: DeepDRiD/test/448/\n",
            "  inflating: DeepDRiD/test/448/448_l1.jpg  \n",
            "  inflating: DeepDRiD/test/448/448_l2.jpg  \n",
            "  inflating: DeepDRiD/test/448/448_r1.jpg  \n",
            "  inflating: DeepDRiD/test/448/448_r2.jpg  \n",
            "   creating: DeepDRiD/test/449/\n",
            "  inflating: DeepDRiD/test/449/449_l1.jpg  \n",
            "  inflating: DeepDRiD/test/449/449_l2.jpg  \n",
            "  inflating: DeepDRiD/test/449/449_r1.jpg  \n",
            "  inflating: DeepDRiD/test/449/449_r2.jpg  \n",
            "   creating: DeepDRiD/test/450/\n",
            "  inflating: DeepDRiD/test/450/450_l1.jpg  \n",
            "  inflating: DeepDRiD/test/450/450_l2.jpg  \n",
            "  inflating: DeepDRiD/test/450/450_r1.jpg  \n",
            "  inflating: DeepDRiD/test/450/450_r2.jpg  \n",
            "   creating: DeepDRiD/test/451/\n",
            "  inflating: DeepDRiD/test/451/451_l1.jpg  \n",
            "  inflating: DeepDRiD/test/451/451_l2.jpg  \n",
            "  inflating: DeepDRiD/test/451/451_r1.jpg  \n",
            "  inflating: DeepDRiD/test/451/451_r2.jpg  \n",
            "   creating: DeepDRiD/test/452/\n",
            "  inflating: DeepDRiD/test/452/452_l1.jpg  \n",
            "  inflating: DeepDRiD/test/452/452_l2.jpg  \n",
            "  inflating: DeepDRiD/test/452/452_r1.jpg  \n",
            "  inflating: DeepDRiD/test/452/452_r2.jpg  \n",
            "   creating: DeepDRiD/test/453/\n",
            "  inflating: DeepDRiD/test/453/453_l1.jpg  \n",
            "  inflating: DeepDRiD/test/453/453_l2.jpg  \n",
            "  inflating: DeepDRiD/test/453/453_r1.jpg  \n",
            "  inflating: DeepDRiD/test/453/453_r2.jpg  \n",
            "   creating: DeepDRiD/test/454/\n",
            "  inflating: DeepDRiD/test/454/454_l1.jpg  \n",
            "  inflating: DeepDRiD/test/454/454_l2.jpg  \n",
            "  inflating: DeepDRiD/test/454/454_r1.jpg  \n",
            "  inflating: DeepDRiD/test/454/454_r2.jpg  \n",
            "   creating: DeepDRiD/test/455/\n",
            "  inflating: DeepDRiD/test/455/455_l1.jpg  \n",
            "  inflating: DeepDRiD/test/455/455_l2.jpg  \n",
            "  inflating: DeepDRiD/test/455/455_r1.jpg  \n",
            "  inflating: DeepDRiD/test/455/455_r2.jpg  \n",
            "   creating: DeepDRiD/test/456/\n",
            "  inflating: DeepDRiD/test/456/456_l1.jpg  \n",
            "  inflating: DeepDRiD/test/456/456_l2.jpg  \n",
            "  inflating: DeepDRiD/test/456/456_r1.jpg  \n",
            "  inflating: DeepDRiD/test/456/456_r2.jpg  \n",
            "   creating: DeepDRiD/test/457/\n",
            "  inflating: DeepDRiD/test/457/457_l1.jpg  \n",
            "  inflating: DeepDRiD/test/457/457_l2.jpg  \n",
            "  inflating: DeepDRiD/test/457/457_r1.jpg  \n",
            "  inflating: DeepDRiD/test/457/457_r2.jpg  \n",
            "   creating: DeepDRiD/test/458/\n",
            "  inflating: DeepDRiD/test/458/458_l1.jpg  \n",
            "  inflating: DeepDRiD/test/458/458_l2.jpg  \n",
            "  inflating: DeepDRiD/test/458/458_r1.jpg  \n",
            "  inflating: DeepDRiD/test/458/458_r2.jpg  \n",
            "   creating: DeepDRiD/test/459/\n",
            "  inflating: DeepDRiD/test/459/459_l1.jpg  \n",
            "  inflating: DeepDRiD/test/459/459_l2.jpg  \n",
            "  inflating: DeepDRiD/test/459/459_r1.jpg  \n",
            "  inflating: DeepDRiD/test/459/459_r2.jpg  \n",
            "   creating: DeepDRiD/test/460/\n",
            "  inflating: DeepDRiD/test/460/460_l1.jpg  \n",
            "  inflating: DeepDRiD/test/460/460_l2.jpg  \n",
            "  inflating: DeepDRiD/test/460/460_r1.jpg  \n",
            "  inflating: DeepDRiD/test/460/460_r2.jpg  \n",
            "   creating: DeepDRiD/test/461/\n",
            "  inflating: DeepDRiD/test/461/461_l1.jpg  \n",
            "  inflating: DeepDRiD/test/461/461_l2.jpg  \n",
            "  inflating: DeepDRiD/test/461/461_r1.jpg  \n",
            "  inflating: DeepDRiD/test/461/461_r2.jpg  \n",
            "   creating: DeepDRiD/test/462/\n",
            "  inflating: DeepDRiD/test/462/462_l1.jpg  \n",
            "  inflating: DeepDRiD/test/462/462_l2.jpg  \n",
            "  inflating: DeepDRiD/test/462/462_r1.jpg  \n",
            "  inflating: DeepDRiD/test/462/462_r2.jpg  \n",
            "   creating: DeepDRiD/test/463/\n",
            "  inflating: DeepDRiD/test/463/463_l1.jpg  \n",
            "  inflating: DeepDRiD/test/463/463_l2.jpg  \n",
            "  inflating: DeepDRiD/test/463/463_r1.jpg  \n",
            "  inflating: DeepDRiD/test/463/463_r2.jpg  \n",
            "   creating: DeepDRiD/test/464/\n",
            "  inflating: DeepDRiD/test/464/464_l1.jpg  \n",
            "  inflating: DeepDRiD/test/464/464_l2.jpg  \n",
            "  inflating: DeepDRiD/test/464/464_r1.jpg  \n",
            "  inflating: DeepDRiD/test/464/464_r2.jpg  \n",
            "   creating: DeepDRiD/test/465/\n",
            "  inflating: DeepDRiD/test/465/465_l1.jpg  \n",
            "  inflating: DeepDRiD/test/465/465_l2.jpg  \n",
            "  inflating: DeepDRiD/test/465/465_r1.jpg  \n",
            "  inflating: DeepDRiD/test/465/465_r2.jpg  \n",
            "   creating: DeepDRiD/test/466/\n",
            "  inflating: DeepDRiD/test/466/466_l1.jpg  \n",
            "  inflating: DeepDRiD/test/466/466_l2.jpg  \n",
            "  inflating: DeepDRiD/test/466/466_r1.jpg  \n",
            "  inflating: DeepDRiD/test/466/466_r2.jpg  \n",
            "   creating: DeepDRiD/test/467/\n",
            "  inflating: DeepDRiD/test/467/467_l1.jpg  \n",
            "  inflating: DeepDRiD/test/467/467_l2.jpg  \n",
            "  inflating: DeepDRiD/test/467/467_r1.jpg  \n",
            "  inflating: DeepDRiD/test/467/467_r2.jpg  \n",
            "   creating: DeepDRiD/test/468/\n",
            "  inflating: DeepDRiD/test/468/468_l1.jpg  \n",
            "  inflating: DeepDRiD/test/468/468_l2.jpg  \n",
            "  inflating: DeepDRiD/test/468/468_r1.jpg  \n",
            "  inflating: DeepDRiD/test/468/468_r2.jpg  \n",
            "   creating: DeepDRiD/test/469/\n",
            "  inflating: DeepDRiD/test/469/469_l1.jpg  \n",
            "  inflating: DeepDRiD/test/469/469_l2.jpg  \n",
            "  inflating: DeepDRiD/test/469/469_r1.jpg  \n",
            "  inflating: DeepDRiD/test/469/469_r2.jpg  \n",
            "   creating: DeepDRiD/test/470/\n",
            "  inflating: DeepDRiD/test/470/470_l1.jpg  \n",
            "  inflating: DeepDRiD/test/470/470_l2.jpg  \n",
            "  inflating: DeepDRiD/test/470/470_r1.jpg  \n",
            "  inflating: DeepDRiD/test/470/470_r2.jpg  \n",
            "   creating: DeepDRiD/test/471/\n",
            "  inflating: DeepDRiD/test/471/471_l1.jpg  \n",
            "  inflating: DeepDRiD/test/471/471_l2.jpg  \n",
            "  inflating: DeepDRiD/test/471/471_r1.jpg  \n",
            "  inflating: DeepDRiD/test/471/471_r2.jpg  \n",
            "   creating: DeepDRiD/test/472/\n",
            "  inflating: DeepDRiD/test/472/472_l1.jpg  \n",
            "  inflating: DeepDRiD/test/472/472_l2.jpg  \n",
            "  inflating: DeepDRiD/test/472/472_r1.jpg  \n",
            "  inflating: DeepDRiD/test/472/472_r2.jpg  \n",
            "   creating: DeepDRiD/test/473/\n",
            "  inflating: DeepDRiD/test/473/473_l1.jpg  \n",
            "  inflating: DeepDRiD/test/473/473_l2.jpg  \n",
            "  inflating: DeepDRiD/test/473/473_r1.jpg  \n",
            "  inflating: DeepDRiD/test/473/473_r2.jpg  \n",
            "   creating: DeepDRiD/test/474/\n",
            "  inflating: DeepDRiD/test/474/474_l1.jpg  \n",
            "  inflating: DeepDRiD/test/474/474_l2.jpg  \n",
            "  inflating: DeepDRiD/test/474/474_r1.jpg  \n",
            "  inflating: DeepDRiD/test/474/474_r2.jpg  \n",
            "   creating: DeepDRiD/test/475/\n",
            "  inflating: DeepDRiD/test/475/475_l1.jpg  \n",
            "  inflating: DeepDRiD/test/475/475_l2.jpg  \n",
            "  inflating: DeepDRiD/test/475/475_r1.jpg  \n",
            "  inflating: DeepDRiD/test/475/475_r2.jpg  \n",
            "   creating: DeepDRiD/test/476/\n",
            "  inflating: DeepDRiD/test/476/476_l1.jpg  \n",
            "  inflating: DeepDRiD/test/476/476_l2.jpg  \n",
            "  inflating: DeepDRiD/test/476/476_r1.jpg  \n",
            "  inflating: DeepDRiD/test/476/476_r2.jpg  \n",
            "   creating: DeepDRiD/test/477/\n",
            "  inflating: DeepDRiD/test/477/477_l1.jpg  \n",
            "  inflating: DeepDRiD/test/477/477_l2.jpg  \n",
            "  inflating: DeepDRiD/test/477/477_r1.jpg  \n",
            "  inflating: DeepDRiD/test/477/477_r2.jpg  \n",
            "   creating: DeepDRiD/test/478/\n",
            "  inflating: DeepDRiD/test/478/478_l1.jpg  \n",
            "  inflating: DeepDRiD/test/478/478_l2.jpg  \n",
            "  inflating: DeepDRiD/test/478/478_r1.jpg  \n",
            "  inflating: DeepDRiD/test/478/478_r2.jpg  \n",
            "   creating: DeepDRiD/test/479/\n",
            "  inflating: DeepDRiD/test/479/479_l1.jpg  \n",
            "  inflating: DeepDRiD/test/479/479_l2.jpg  \n",
            "  inflating: DeepDRiD/test/479/479_r1.jpg  \n",
            "  inflating: DeepDRiD/test/479/479_r2.jpg  \n",
            "   creating: DeepDRiD/test/480/\n",
            "  inflating: DeepDRiD/test/480/480_l1.jpg  \n",
            "  inflating: DeepDRiD/test/480/480_l2.jpg  \n",
            "  inflating: DeepDRiD/test/480/480_r1.jpg  \n",
            "  inflating: DeepDRiD/test/480/480_r2.jpg  \n",
            "   creating: DeepDRiD/test/481/\n",
            "  inflating: DeepDRiD/test/481/481_l1.jpg  \n",
            "  inflating: DeepDRiD/test/481/481_l2.jpg  \n",
            "  inflating: DeepDRiD/test/481/481_r1.jpg  \n",
            "  inflating: DeepDRiD/test/481/481_r2.jpg  \n",
            "   creating: DeepDRiD/test/482/\n",
            "  inflating: DeepDRiD/test/482/482_l1.jpg  \n",
            "  inflating: DeepDRiD/test/482/482_l2.jpg  \n",
            "  inflating: DeepDRiD/test/482/482_r1.jpg  \n",
            "  inflating: DeepDRiD/test/482/482_r2.jpg  \n",
            "   creating: DeepDRiD/test/483/\n",
            "  inflating: DeepDRiD/test/483/483_l1.jpg  \n",
            "  inflating: DeepDRiD/test/483/483_l2.jpg  \n",
            "  inflating: DeepDRiD/test/483/483_r1.jpg  \n",
            "  inflating: DeepDRiD/test/483/483_r2.jpg  \n",
            "   creating: DeepDRiD/test/484/\n",
            "  inflating: DeepDRiD/test/484/484_l1.jpg  \n",
            "  inflating: DeepDRiD/test/484/484_l2.jpg  \n",
            "  inflating: DeepDRiD/test/484/484_r1.jpg  \n",
            "  inflating: DeepDRiD/test/484/484_r2.jpg  \n",
            "   creating: DeepDRiD/test/485/\n",
            "  inflating: DeepDRiD/test/485/485_l1.jpg  \n",
            "  inflating: DeepDRiD/test/485/485_l2.jpg  \n",
            "  inflating: DeepDRiD/test/485/485_r1.jpg  \n",
            "  inflating: DeepDRiD/test/485/485_r2.jpg  \n",
            "   creating: DeepDRiD/test/486/\n",
            "  inflating: DeepDRiD/test/486/486_l1.jpg  \n",
            "  inflating: DeepDRiD/test/486/486_l2.jpg  \n",
            "  inflating: DeepDRiD/test/486/486_r1.jpg  \n",
            "  inflating: DeepDRiD/test/486/486_r2.jpg  \n",
            "   creating: DeepDRiD/test/487/\n",
            "  inflating: DeepDRiD/test/487/487_l1.jpg  \n",
            "  inflating: DeepDRiD/test/487/487_l2.jpg  \n",
            "  inflating: DeepDRiD/test/487/487_r1.jpg  \n",
            "  inflating: DeepDRiD/test/487/487_r2.jpg  \n",
            "   creating: DeepDRiD/test/488/\n",
            "  inflating: DeepDRiD/test/488/488_l1.jpg  \n",
            "  inflating: DeepDRiD/test/488/488_l2.jpg  \n",
            "  inflating: DeepDRiD/test/488/488_r1.jpg  \n",
            "  inflating: DeepDRiD/test/488/488_r2.jpg  \n",
            "   creating: DeepDRiD/test/489/\n",
            "  inflating: DeepDRiD/test/489/489_l1.jpg  \n",
            "  inflating: DeepDRiD/test/489/489_l2.jpg  \n",
            "  inflating: DeepDRiD/test/489/489_r1.jpg  \n",
            "  inflating: DeepDRiD/test/489/489_r2.jpg  \n",
            "   creating: DeepDRiD/test/490/\n",
            "  inflating: DeepDRiD/test/490/490_l1.jpg  \n",
            "  inflating: DeepDRiD/test/490/490_l2.jpg  \n",
            "  inflating: DeepDRiD/test/490/490_r1.jpg  \n",
            "  inflating: DeepDRiD/test/490/490_r2.jpg  \n",
            "   creating: DeepDRiD/test/491/\n",
            "  inflating: DeepDRiD/test/491/491_l1.jpg  \n",
            "  inflating: DeepDRiD/test/491/491_l2.jpg  \n",
            "  inflating: DeepDRiD/test/491/491_r1.jpg  \n",
            "  inflating: DeepDRiD/test/491/491_r2.jpg  \n",
            "   creating: DeepDRiD/test/492/\n",
            "  inflating: DeepDRiD/test/492/492_l1.jpg  \n",
            "  inflating: DeepDRiD/test/492/492_l2.jpg  \n",
            "  inflating: DeepDRiD/test/492/492_r1.jpg  \n",
            "  inflating: DeepDRiD/test/492/492_r2.jpg  \n",
            "   creating: DeepDRiD/test/493/\n",
            "  inflating: DeepDRiD/test/493/493_l1.jpg  \n",
            "  inflating: DeepDRiD/test/493/493_l2.jpg  \n",
            "  inflating: DeepDRiD/test/493/493_r1.jpg  \n",
            "  inflating: DeepDRiD/test/493/493_r2.jpg  \n",
            "   creating: DeepDRiD/test/494/\n",
            "  inflating: DeepDRiD/test/494/494_l1.jpg  \n",
            "  inflating: DeepDRiD/test/494/494_l2.jpg  \n",
            "  inflating: DeepDRiD/test/494/494_r1.jpg  \n",
            "  inflating: DeepDRiD/test/494/494_r2.jpg  \n",
            "   creating: DeepDRiD/test/495/\n",
            "  inflating: DeepDRiD/test/495/495_l1.jpg  \n",
            "  inflating: DeepDRiD/test/495/495_l2.jpg  \n",
            "  inflating: DeepDRiD/test/495/495_r1.jpg  \n",
            "  inflating: DeepDRiD/test/495/495_r2.jpg  \n",
            "   creating: DeepDRiD/test/496/\n",
            "  inflating: DeepDRiD/test/496/496_l1.jpg  \n",
            "  inflating: DeepDRiD/test/496/496_l2.jpg  \n",
            "  inflating: DeepDRiD/test/496/496_r1.jpg  \n",
            "  inflating: DeepDRiD/test/496/496_r2.jpg  \n",
            "   creating: DeepDRiD/test/497/\n",
            "  inflating: DeepDRiD/test/497/497_l1.jpg  \n",
            "  inflating: DeepDRiD/test/497/497_l2.jpg  \n",
            "  inflating: DeepDRiD/test/497/497_r1.jpg  \n",
            "  inflating: DeepDRiD/test/497/497_r2.jpg  \n",
            "   creating: DeepDRiD/test/498/\n",
            "  inflating: DeepDRiD/test/498/498_l1.jpg  \n",
            "  inflating: DeepDRiD/test/498/498_l2.jpg  \n",
            "  inflating: DeepDRiD/test/498/498_r1.jpg  \n",
            "  inflating: DeepDRiD/test/498/498_r2.jpg  \n",
            "   creating: DeepDRiD/test/499/\n",
            "  inflating: DeepDRiD/test/499/499_l1.jpg  \n",
            "  inflating: DeepDRiD/test/499/499_l2.jpg  \n",
            "  inflating: DeepDRiD/test/499/499_r1.jpg  \n",
            "  inflating: DeepDRiD/test/499/499_r2.jpg  \n",
            "   creating: DeepDRiD/test/500/\n",
            "  inflating: DeepDRiD/test/500/500_l1.jpg  \n",
            "  inflating: DeepDRiD/test/500/500_l2.jpg  \n",
            "  inflating: DeepDRiD/test/500/500_r1.jpg  \n",
            "  inflating: DeepDRiD/test/500/500_r2.jpg  \n",
            "  inflating: DeepDRiD/test.csv       \n",
            "   creating: DeepDRiD/train/\n",
            "   creating: DeepDRiD/train/1/\n",
            "  inflating: DeepDRiD/train/1/1_l1.jpg  \n",
            "  inflating: DeepDRiD/train/1/1_l2.jpg  \n",
            "  inflating: DeepDRiD/train/1/1_r1.jpg  \n",
            "  inflating: DeepDRiD/train/1/1_r2.jpg  \n",
            "   creating: DeepDRiD/train/10/\n",
            "  inflating: DeepDRiD/train/10/10_l1.jpg  \n",
            "  inflating: DeepDRiD/train/10/10_l2.jpg  \n",
            "  inflating: DeepDRiD/train/10/10_r1.jpg  \n",
            "  inflating: DeepDRiD/train/10/10_r2.jpg  \n",
            "   creating: DeepDRiD/train/100/\n",
            "  inflating: DeepDRiD/train/100/100_l1.jpg  \n",
            "  inflating: DeepDRiD/train/100/100_l2.jpg  \n",
            "  inflating: DeepDRiD/train/100/100_r1.jpg  \n",
            "  inflating: DeepDRiD/train/100/100_r2.jpg  \n",
            "   creating: DeepDRiD/train/101/\n",
            "  inflating: DeepDRiD/train/101/101_l1.jpg  \n",
            "  inflating: DeepDRiD/train/101/101_l2.jpg  \n",
            "  inflating: DeepDRiD/train/101/101_r1.jpg  \n",
            "  inflating: DeepDRiD/train/101/101_r2.jpg  \n",
            "   creating: DeepDRiD/train/102/\n",
            "  inflating: DeepDRiD/train/102/102_l1.jpg  \n",
            "  inflating: DeepDRiD/train/102/102_l2.jpg  \n",
            "  inflating: DeepDRiD/train/102/102_r1.jpg  \n",
            "  inflating: DeepDRiD/train/102/102_r2.jpg  \n",
            "   creating: DeepDRiD/train/103/\n",
            "  inflating: DeepDRiD/train/103/103_l1.jpg  \n",
            "  inflating: DeepDRiD/train/103/103_l2.jpg  \n",
            "  inflating: DeepDRiD/train/103/103_r1.jpg  \n",
            "  inflating: DeepDRiD/train/103/103_r2.jpg  \n",
            "   creating: DeepDRiD/train/104/\n",
            "  inflating: DeepDRiD/train/104/104_l1.jpg  \n",
            "  inflating: DeepDRiD/train/104/104_l2.jpg  \n",
            "  inflating: DeepDRiD/train/104/104_r1.jpg  \n",
            "  inflating: DeepDRiD/train/104/104_r2.jpg  \n",
            "   creating: DeepDRiD/train/105/\n",
            "  inflating: DeepDRiD/train/105/105_l1.jpg  \n",
            "  inflating: DeepDRiD/train/105/105_l2.jpg  \n",
            "  inflating: DeepDRiD/train/105/105_r1.jpg  \n",
            "  inflating: DeepDRiD/train/105/105_r2.jpg  \n",
            "   creating: DeepDRiD/train/106/\n",
            "  inflating: DeepDRiD/train/106/106_l1.jpg  \n",
            "  inflating: DeepDRiD/train/106/106_l2.jpg  \n",
            "  inflating: DeepDRiD/train/106/106_r1.jpg  \n",
            "  inflating: DeepDRiD/train/106/106_r2.jpg  \n",
            "   creating: DeepDRiD/train/107/\n",
            "  inflating: DeepDRiD/train/107/107_l1.jpg  \n",
            "  inflating: DeepDRiD/train/107/107_l2.jpg  \n",
            "  inflating: DeepDRiD/train/107/107_r1.jpg  \n",
            "  inflating: DeepDRiD/train/107/107_r2.jpg  \n",
            "   creating: DeepDRiD/train/108/\n",
            "  inflating: DeepDRiD/train/108/108_l1.jpg  \n",
            "  inflating: DeepDRiD/train/108/108_l2.jpg  \n",
            "  inflating: DeepDRiD/train/108/108_r1.jpg  \n",
            "  inflating: DeepDRiD/train/108/108_r2.jpg  \n",
            "   creating: DeepDRiD/train/109/\n",
            "  inflating: DeepDRiD/train/109/109_l1.jpg  \n",
            "  inflating: DeepDRiD/train/109/109_l2.jpg  \n",
            "  inflating: DeepDRiD/train/109/109_r1.jpg  \n",
            "  inflating: DeepDRiD/train/109/109_r2.jpg  \n",
            "   creating: DeepDRiD/train/11/\n",
            "  inflating: DeepDRiD/train/11/11_l1.jpg  \n",
            "  inflating: DeepDRiD/train/11/11_l2.jpg  \n",
            "  inflating: DeepDRiD/train/11/11_r1.jpg  \n",
            "  inflating: DeepDRiD/train/11/11_r2.jpg  \n",
            "   creating: DeepDRiD/train/110/\n",
            "  inflating: DeepDRiD/train/110/110_l1.jpg  \n",
            "  inflating: DeepDRiD/train/110/110_l2.jpg  \n",
            "  inflating: DeepDRiD/train/110/110_r1.jpg  \n",
            "  inflating: DeepDRiD/train/110/110_r2.jpg  \n",
            "   creating: DeepDRiD/train/111/\n",
            "  inflating: DeepDRiD/train/111/111_l1.jpg  \n",
            "  inflating: DeepDRiD/train/111/111_l2.jpg  \n",
            "  inflating: DeepDRiD/train/111/111_r1.jpg  \n",
            "  inflating: DeepDRiD/train/111/111_r2.jpg  \n",
            "   creating: DeepDRiD/train/112/\n",
            "  inflating: DeepDRiD/train/112/112_l1.jpg  \n",
            "  inflating: DeepDRiD/train/112/112_l2.jpg  \n",
            "  inflating: DeepDRiD/train/112/112_r1.jpg  \n",
            "  inflating: DeepDRiD/train/112/112_r2.jpg  \n",
            "   creating: DeepDRiD/train/113/\n",
            "  inflating: DeepDRiD/train/113/113_l1.jpg  \n",
            "  inflating: DeepDRiD/train/113/113_l2.jpg  \n",
            "  inflating: DeepDRiD/train/113/113_r1.jpg  \n",
            "  inflating: DeepDRiD/train/113/113_r2.jpg  \n",
            "   creating: DeepDRiD/train/114/\n",
            "  inflating: DeepDRiD/train/114/114_l1.jpg  \n",
            "  inflating: DeepDRiD/train/114/114_l2.jpg  \n",
            "  inflating: DeepDRiD/train/114/114_r1.jpg  \n",
            "  inflating: DeepDRiD/train/114/114_r2.jpg  \n",
            "   creating: DeepDRiD/train/115/\n",
            "  inflating: DeepDRiD/train/115/115_l1.jpg  \n",
            "  inflating: DeepDRiD/train/115/115_l2.jpg  \n",
            "  inflating: DeepDRiD/train/115/115_r1.jpg  \n",
            "  inflating: DeepDRiD/train/115/115_r2.jpg  \n",
            "   creating: DeepDRiD/train/116/\n",
            "  inflating: DeepDRiD/train/116/116_l1.jpg  \n",
            "  inflating: DeepDRiD/train/116/116_l2.jpg  \n",
            "  inflating: DeepDRiD/train/116/116_r1.jpg  \n",
            "  inflating: DeepDRiD/train/116/116_r2.jpg  \n",
            "   creating: DeepDRiD/train/117/\n",
            "  inflating: DeepDRiD/train/117/117_l1.jpg  \n",
            "  inflating: DeepDRiD/train/117/117_l2.jpg  \n",
            "  inflating: DeepDRiD/train/117/117_r1.jpg  \n",
            "  inflating: DeepDRiD/train/117/117_r2.jpg  \n",
            "   creating: DeepDRiD/train/118/\n",
            "  inflating: DeepDRiD/train/118/118_l1.jpg  \n",
            "  inflating: DeepDRiD/train/118/118_l2.jpg  \n",
            "  inflating: DeepDRiD/train/118/118_r1.jpg  \n",
            "  inflating: DeepDRiD/train/118/118_r2.jpg  \n",
            "   creating: DeepDRiD/train/119/\n",
            "  inflating: DeepDRiD/train/119/119_l1.jpg  \n",
            "  inflating: DeepDRiD/train/119/119_l2.jpg  \n",
            "  inflating: DeepDRiD/train/119/119_r1.jpg  \n",
            "  inflating: DeepDRiD/train/119/119_r2.jpg  \n",
            "   creating: DeepDRiD/train/12/\n",
            "  inflating: DeepDRiD/train/12/12_l1.jpg  \n",
            "  inflating: DeepDRiD/train/12/12_l2.jpg  \n",
            "  inflating: DeepDRiD/train/12/12_r1.jpg  \n",
            "  inflating: DeepDRiD/train/12/12_r2.jpg  \n",
            "   creating: DeepDRiD/train/120/\n",
            "  inflating: DeepDRiD/train/120/120_l1.jpg  \n",
            "  inflating: DeepDRiD/train/120/120_l2.jpg  \n",
            "  inflating: DeepDRiD/train/120/120_r1.jpg  \n",
            "  inflating: DeepDRiD/train/120/120_r2.jpg  \n",
            "   creating: DeepDRiD/train/121/\n",
            "  inflating: DeepDRiD/train/121/121_l1.jpg  \n",
            "  inflating: DeepDRiD/train/121/121_l2.jpg  \n",
            "  inflating: DeepDRiD/train/121/121_r1.jpg  \n",
            "  inflating: DeepDRiD/train/121/121_r2.jpg  \n",
            "   creating: DeepDRiD/train/122/\n",
            "  inflating: DeepDRiD/train/122/122_l1.jpg  \n",
            "  inflating: DeepDRiD/train/122/122_l2.jpg  \n",
            "  inflating: DeepDRiD/train/122/122_r1.jpg  \n",
            "  inflating: DeepDRiD/train/122/122_r2.jpg  \n",
            "   creating: DeepDRiD/train/123/\n",
            "  inflating: DeepDRiD/train/123/123_l1.jpg  \n",
            "  inflating: DeepDRiD/train/123/123_l2.jpg  \n",
            "  inflating: DeepDRiD/train/123/123_r1.jpg  \n",
            "  inflating: DeepDRiD/train/123/123_r2.jpg  \n",
            "   creating: DeepDRiD/train/124/\n",
            "  inflating: DeepDRiD/train/124/124_l1.jpg  \n",
            "  inflating: DeepDRiD/train/124/124_l2.jpg  \n",
            "  inflating: DeepDRiD/train/124/124_r1.jpg  \n",
            "  inflating: DeepDRiD/train/124/124_r2.jpg  \n",
            "   creating: DeepDRiD/train/125/\n",
            "  inflating: DeepDRiD/train/125/125_l1.jpg  \n",
            "  inflating: DeepDRiD/train/125/125_l2.jpg  \n",
            "  inflating: DeepDRiD/train/125/125_r1.jpg  \n",
            "  inflating: DeepDRiD/train/125/125_r2.jpg  \n",
            "   creating: DeepDRiD/train/126/\n",
            "  inflating: DeepDRiD/train/126/126_l1.jpg  \n",
            "  inflating: DeepDRiD/train/126/126_l2.jpg  \n",
            "  inflating: DeepDRiD/train/126/126_r1.jpg  \n",
            "  inflating: DeepDRiD/train/126/126_r2.jpg  \n",
            "   creating: DeepDRiD/train/127/\n",
            "  inflating: DeepDRiD/train/127/127_l1.jpg  \n",
            "  inflating: DeepDRiD/train/127/127_l2.jpg  \n",
            "  inflating: DeepDRiD/train/127/127_r1.jpg  \n",
            "  inflating: DeepDRiD/train/127/127_r2.jpg  \n",
            "   creating: DeepDRiD/train/128/\n",
            "  inflating: DeepDRiD/train/128/128_l1.jpg  \n",
            "  inflating: DeepDRiD/train/128/128_l2.jpg  \n",
            "  inflating: DeepDRiD/train/128/128_r1.jpg  \n",
            "  inflating: DeepDRiD/train/128/128_r2.jpg  \n",
            "   creating: DeepDRiD/train/129/\n",
            "  inflating: DeepDRiD/train/129/129_l1.jpg  \n",
            "  inflating: DeepDRiD/train/129/129_l2.jpg  \n",
            "  inflating: DeepDRiD/train/129/129_r1.jpg  \n",
            "  inflating: DeepDRiD/train/129/129_r2.jpg  \n",
            "   creating: DeepDRiD/train/13/\n",
            "  inflating: DeepDRiD/train/13/13_l1.jpg  \n",
            "  inflating: DeepDRiD/train/13/13_l2.jpg  \n",
            "  inflating: DeepDRiD/train/13/13_r1.jpg  \n",
            "  inflating: DeepDRiD/train/13/13_r2.jpg  \n",
            "   creating: DeepDRiD/train/130/\n",
            "  inflating: DeepDRiD/train/130/130_l1.jpg  \n",
            "  inflating: DeepDRiD/train/130/130_l2.jpg  \n",
            "  inflating: DeepDRiD/train/130/130_r1.jpg  \n",
            "  inflating: DeepDRiD/train/130/130_r2.jpg  \n",
            "   creating: DeepDRiD/train/131/\n",
            "  inflating: DeepDRiD/train/131/131_l1.jpg  \n",
            "  inflating: DeepDRiD/train/131/131_l2.jpg  \n",
            "  inflating: DeepDRiD/train/131/131_r1.jpg  \n",
            "  inflating: DeepDRiD/train/131/131_r2.jpg  \n",
            "   creating: DeepDRiD/train/132/\n",
            "  inflating: DeepDRiD/train/132/132_l1.jpg  \n",
            "  inflating: DeepDRiD/train/132/132_l2.jpg  \n",
            "  inflating: DeepDRiD/train/132/132_r1.jpg  \n",
            "  inflating: DeepDRiD/train/132/132_r2.jpg  \n",
            "   creating: DeepDRiD/train/133/\n",
            "  inflating: DeepDRiD/train/133/133_l1.jpg  \n",
            "  inflating: DeepDRiD/train/133/133_l2.jpg  \n",
            "  inflating: DeepDRiD/train/133/133_r1.jpg  \n",
            "  inflating: DeepDRiD/train/133/133_r2.jpg  \n",
            "   creating: DeepDRiD/train/134/\n",
            "  inflating: DeepDRiD/train/134/134_l1.jpg  \n",
            "  inflating: DeepDRiD/train/134/134_l2.jpg  \n",
            "  inflating: DeepDRiD/train/134/134_r1.jpg  \n",
            "  inflating: DeepDRiD/train/134/134_r2.jpg  \n",
            "   creating: DeepDRiD/train/135/\n",
            "  inflating: DeepDRiD/train/135/135_l1.jpg  \n",
            "  inflating: DeepDRiD/train/135/135_l2.jpg  \n",
            "  inflating: DeepDRiD/train/135/135_r1.jpg  \n",
            "  inflating: DeepDRiD/train/135/135_r2.jpg  \n",
            "   creating: DeepDRiD/train/136/\n",
            "  inflating: DeepDRiD/train/136/136_l1.jpg  \n",
            "  inflating: DeepDRiD/train/136/136_l2.jpg  \n",
            "  inflating: DeepDRiD/train/136/136_r1.jpg  \n",
            "  inflating: DeepDRiD/train/136/136_r2.jpg  \n",
            "   creating: DeepDRiD/train/137/\n",
            "  inflating: DeepDRiD/train/137/137_l1.jpg  \n",
            "  inflating: DeepDRiD/train/137/137_l2.jpg  \n",
            "  inflating: DeepDRiD/train/137/137_r1.jpg  \n",
            "  inflating: DeepDRiD/train/137/137_r2.jpg  \n",
            "   creating: DeepDRiD/train/138/\n",
            "  inflating: DeepDRiD/train/138/138_l1.jpg  \n",
            "  inflating: DeepDRiD/train/138/138_l2.jpg  \n",
            "  inflating: DeepDRiD/train/138/138_r1.jpg  \n",
            "  inflating: DeepDRiD/train/138/138_r2.jpg  \n",
            "   creating: DeepDRiD/train/139/\n",
            "  inflating: DeepDRiD/train/139/139_l1.jpg  \n",
            "  inflating: DeepDRiD/train/139/139_l2.jpg  \n",
            "  inflating: DeepDRiD/train/139/139_r1.jpg  \n",
            "  inflating: DeepDRiD/train/139/139_r2.jpg  \n",
            "   creating: DeepDRiD/train/14/\n",
            "  inflating: DeepDRiD/train/14/14_l1.jpg  \n",
            "  inflating: DeepDRiD/train/14/14_l2.jpg  \n",
            "  inflating: DeepDRiD/train/14/14_r1.jpg  \n",
            "  inflating: DeepDRiD/train/14/14_r2.jpg  \n",
            "   creating: DeepDRiD/train/140/\n",
            "  inflating: DeepDRiD/train/140/140_l1.jpg  \n",
            "  inflating: DeepDRiD/train/140/140_l2.jpg  \n",
            "  inflating: DeepDRiD/train/140/140_r1.jpg  \n",
            "  inflating: DeepDRiD/train/140/140_r2.jpg  \n",
            "   creating: DeepDRiD/train/141/\n",
            "  inflating: DeepDRiD/train/141/141_l1.jpg  \n",
            "  inflating: DeepDRiD/train/141/141_l2.jpg  \n",
            "  inflating: DeepDRiD/train/141/141_r1.jpg  \n",
            "  inflating: DeepDRiD/train/141/141_r2.jpg  \n",
            "   creating: DeepDRiD/train/142/\n",
            "  inflating: DeepDRiD/train/142/142_l1.jpg  \n",
            "  inflating: DeepDRiD/train/142/142_l2.jpg  \n",
            "  inflating: DeepDRiD/train/142/142_r1.jpg  \n",
            "  inflating: DeepDRiD/train/142/142_r2.jpg  \n",
            "   creating: DeepDRiD/train/143/\n",
            "  inflating: DeepDRiD/train/143/143_l1.jpg  \n",
            "  inflating: DeepDRiD/train/143/143_l2.jpg  \n",
            "  inflating: DeepDRiD/train/143/143_r1.jpg  \n",
            "  inflating: DeepDRiD/train/143/143_r2.jpg  \n",
            "   creating: DeepDRiD/train/144/\n",
            "  inflating: DeepDRiD/train/144/144_l1.jpg  \n",
            "  inflating: DeepDRiD/train/144/144_l2.jpg  \n",
            "  inflating: DeepDRiD/train/144/144_r1.jpg  \n",
            "  inflating: DeepDRiD/train/144/144_r2.jpg  \n",
            "   creating: DeepDRiD/train/145/\n",
            "  inflating: DeepDRiD/train/145/145_l1.jpg  \n",
            "  inflating: DeepDRiD/train/145/145_l2.jpg  \n",
            "  inflating: DeepDRiD/train/145/145_r1.jpg  \n",
            "  inflating: DeepDRiD/train/145/145_r2.jpg  \n",
            "   creating: DeepDRiD/train/146/\n",
            "  inflating: DeepDRiD/train/146/146_l1.jpg  \n",
            "  inflating: DeepDRiD/train/146/146_l2.jpg  \n",
            "  inflating: DeepDRiD/train/146/146_r1.jpg  \n",
            "  inflating: DeepDRiD/train/146/146_r2.jpg  \n",
            "   creating: DeepDRiD/train/147/\n",
            "  inflating: DeepDRiD/train/147/147_l1.jpg  \n",
            "  inflating: DeepDRiD/train/147/147_l2.jpg  \n",
            "  inflating: DeepDRiD/train/147/147_r1.jpg  \n",
            "  inflating: DeepDRiD/train/147/147_r2.jpg  \n",
            "   creating: DeepDRiD/train/148/\n",
            "  inflating: DeepDRiD/train/148/148_l1.jpg  \n",
            "  inflating: DeepDRiD/train/148/148_l2.jpg  \n",
            "  inflating: DeepDRiD/train/148/148_r1.jpg  \n",
            "  inflating: DeepDRiD/train/148/148_r2.jpg  \n",
            "   creating: DeepDRiD/train/149/\n",
            "  inflating: DeepDRiD/train/149/149_l1.jpg  \n",
            "  inflating: DeepDRiD/train/149/149_l2.jpg  \n",
            "  inflating: DeepDRiD/train/149/149_r1.jpg  \n",
            "  inflating: DeepDRiD/train/149/149_r2.jpg  \n",
            "   creating: DeepDRiD/train/15/\n",
            "  inflating: DeepDRiD/train/15/15_l1.jpg  \n",
            "  inflating: DeepDRiD/train/15/15_l2.jpg  \n",
            "  inflating: DeepDRiD/train/15/15_r1.jpg  \n",
            "  inflating: DeepDRiD/train/15/15_r2.jpg  \n",
            "   creating: DeepDRiD/train/150/\n",
            "  inflating: DeepDRiD/train/150/150_l1.jpg  \n",
            "  inflating: DeepDRiD/train/150/150_l2.jpg  \n",
            "  inflating: DeepDRiD/train/150/150_r1.jpg  \n",
            "  inflating: DeepDRiD/train/150/150_r2.jpg  \n",
            "   creating: DeepDRiD/train/151/\n",
            "  inflating: DeepDRiD/train/151/151_l1.jpg  \n",
            "  inflating: DeepDRiD/train/151/151_l2.jpg  \n",
            "  inflating: DeepDRiD/train/151/151_r1.jpg  \n",
            "  inflating: DeepDRiD/train/151/151_r2.jpg  \n",
            "   creating: DeepDRiD/train/152/\n",
            "  inflating: DeepDRiD/train/152/152_l1.jpg  \n",
            "  inflating: DeepDRiD/train/152/152_l2.jpg  \n",
            "  inflating: DeepDRiD/train/152/152_r1.jpg  \n",
            "  inflating: DeepDRiD/train/152/152_r2.jpg  \n",
            "   creating: DeepDRiD/train/153/\n",
            "  inflating: DeepDRiD/train/153/153_l1.jpg  \n",
            "  inflating: DeepDRiD/train/153/153_l2.jpg  \n",
            "  inflating: DeepDRiD/train/153/153_r1.jpg  \n",
            "  inflating: DeepDRiD/train/153/153_r2.jpg  \n",
            "   creating: DeepDRiD/train/154/\n",
            "  inflating: DeepDRiD/train/154/154_l1.jpg  \n",
            "  inflating: DeepDRiD/train/154/154_l2.jpg  \n",
            "  inflating: DeepDRiD/train/154/154_r1.jpg  \n",
            "  inflating: DeepDRiD/train/154/154_r2.jpg  \n",
            "   creating: DeepDRiD/train/155/\n",
            "  inflating: DeepDRiD/train/155/155_l1.jpg  \n",
            "  inflating: DeepDRiD/train/155/155_l2.jpg  \n",
            "  inflating: DeepDRiD/train/155/155_r1.jpg  \n",
            "  inflating: DeepDRiD/train/155/155_r2.jpg  \n",
            "   creating: DeepDRiD/train/156/\n",
            "  inflating: DeepDRiD/train/156/156_l1.jpg  \n",
            "  inflating: DeepDRiD/train/156/156_l2.jpg  \n",
            "  inflating: DeepDRiD/train/156/156_r1.jpg  \n",
            "  inflating: DeepDRiD/train/156/156_r2.jpg  \n",
            "   creating: DeepDRiD/train/157/\n",
            "  inflating: DeepDRiD/train/157/157_l1.jpg  \n",
            "  inflating: DeepDRiD/train/157/157_l2.jpg  \n",
            "  inflating: DeepDRiD/train/157/157_r1.jpg  \n",
            "  inflating: DeepDRiD/train/157/157_r2.jpg  \n",
            "   creating: DeepDRiD/train/158/\n",
            "  inflating: DeepDRiD/train/158/158_l1.jpg  \n",
            "  inflating: DeepDRiD/train/158/158_l2.jpg  \n",
            "  inflating: DeepDRiD/train/158/158_r1.jpg  \n",
            "  inflating: DeepDRiD/train/158/158_r2.jpg  \n",
            "   creating: DeepDRiD/train/159/\n",
            "  inflating: DeepDRiD/train/159/159_l1.jpg  \n",
            "  inflating: DeepDRiD/train/159/159_l2.jpg  \n",
            "  inflating: DeepDRiD/train/159/159_r1.jpg  \n",
            "  inflating: DeepDRiD/train/159/159_r2.jpg  \n",
            "   creating: DeepDRiD/train/16/\n",
            "  inflating: DeepDRiD/train/16/16_l1.jpg  \n",
            "  inflating: DeepDRiD/train/16/16_l2.jpg  \n",
            "  inflating: DeepDRiD/train/16/16_r1.jpg  \n",
            "  inflating: DeepDRiD/train/16/16_r2.jpg  \n",
            "   creating: DeepDRiD/train/160/\n",
            "  inflating: DeepDRiD/train/160/160_l1.jpg  \n",
            "  inflating: DeepDRiD/train/160/160_l2.jpg  \n",
            "  inflating: DeepDRiD/train/160/160_r1.jpg  \n",
            "  inflating: DeepDRiD/train/160/160_r2.jpg  \n",
            "   creating: DeepDRiD/train/161/\n",
            "  inflating: DeepDRiD/train/161/161_l1.jpg  \n",
            "  inflating: DeepDRiD/train/161/161_l2.jpg  \n",
            "  inflating: DeepDRiD/train/161/161_r1.jpg  \n",
            "  inflating: DeepDRiD/train/161/161_r2.jpg  \n",
            "   creating: DeepDRiD/train/162/\n",
            "  inflating: DeepDRiD/train/162/162_l1.jpg  \n",
            "  inflating: DeepDRiD/train/162/162_l2.jpg  \n",
            "  inflating: DeepDRiD/train/162/162_r1.jpg  \n",
            "  inflating: DeepDRiD/train/162/162_r2.jpg  \n",
            "   creating: DeepDRiD/train/163/\n",
            "  inflating: DeepDRiD/train/163/163_l1.jpg  \n",
            "  inflating: DeepDRiD/train/163/163_l2.jpg  \n",
            "  inflating: DeepDRiD/train/163/163_r1.jpg  \n",
            "  inflating: DeepDRiD/train/163/163_r2.jpg  \n",
            "   creating: DeepDRiD/train/164/\n",
            "  inflating: DeepDRiD/train/164/164_l1.jpg  \n",
            "  inflating: DeepDRiD/train/164/164_l2.jpg  \n",
            "  inflating: DeepDRiD/train/164/164_r1.jpg  \n",
            "  inflating: DeepDRiD/train/164/164_r2.jpg  \n",
            "   creating: DeepDRiD/train/165/\n",
            "  inflating: DeepDRiD/train/165/165_l1.jpg  \n",
            "  inflating: DeepDRiD/train/165/165_l2.jpg  \n",
            "  inflating: DeepDRiD/train/165/165_r1.jpg  \n",
            "  inflating: DeepDRiD/train/165/165_r2.jpg  \n",
            "   creating: DeepDRiD/train/166/\n",
            "  inflating: DeepDRiD/train/166/166_l1.jpg  \n",
            "  inflating: DeepDRiD/train/166/166_l2.jpg  \n",
            "  inflating: DeepDRiD/train/166/166_r1.jpg  \n",
            "  inflating: DeepDRiD/train/166/166_r2.jpg  \n",
            "   creating: DeepDRiD/train/167/\n",
            "  inflating: DeepDRiD/train/167/167_l2.jpg  \n",
            "  inflating: DeepDRiD/train/167/167_l3.jpg  \n",
            "  inflating: DeepDRiD/train/167/167_r2.jpg  \n",
            "  inflating: DeepDRiD/train/167/167_r3.jpg  \n",
            "   creating: DeepDRiD/train/168/\n",
            "  inflating: DeepDRiD/train/168/168_l1.jpg  \n",
            "  inflating: DeepDRiD/train/168/168_l2.jpg  \n",
            "  inflating: DeepDRiD/train/168/168_r1.jpg  \n",
            "  inflating: DeepDRiD/train/168/168_r2.jpg  \n",
            "   creating: DeepDRiD/train/169/\n",
            "  inflating: DeepDRiD/train/169/169_l1.jpg  \n",
            "  inflating: DeepDRiD/train/169/169_l2.jpg  \n",
            "  inflating: DeepDRiD/train/169/169_r1.jpg  \n",
            "  inflating: DeepDRiD/train/169/169_r2.jpg  \n",
            "   creating: DeepDRiD/train/17/\n",
            "  inflating: DeepDRiD/train/17/17_l1.jpg  \n",
            "  inflating: DeepDRiD/train/17/17_l2.jpg  \n",
            "  inflating: DeepDRiD/train/17/17_r1.jpg  \n",
            "  inflating: DeepDRiD/train/17/17_r2.jpg  \n",
            "   creating: DeepDRiD/train/170/\n",
            "  inflating: DeepDRiD/train/170/170_l1.jpg  \n",
            "  inflating: DeepDRiD/train/170/170_l2.jpg  \n",
            "  inflating: DeepDRiD/train/170/170_r1.jpg  \n",
            "  inflating: DeepDRiD/train/170/170_r2.jpg  \n",
            "   creating: DeepDRiD/train/171/\n",
            "  inflating: DeepDRiD/train/171/171_l1.jpg  \n",
            "  inflating: DeepDRiD/train/171/171_l2.jpg  \n",
            "  inflating: DeepDRiD/train/171/171_r1.jpg  \n",
            "  inflating: DeepDRiD/train/171/171_r2.jpg  \n",
            "   creating: DeepDRiD/train/172/\n",
            "  inflating: DeepDRiD/train/172/172_l1.jpg  \n",
            "  inflating: DeepDRiD/train/172/172_l2.jpg  \n",
            "  inflating: DeepDRiD/train/172/172_r1.jpg  \n",
            "  inflating: DeepDRiD/train/172/172_r2.jpg  \n",
            "   creating: DeepDRiD/train/173/\n",
            "  inflating: DeepDRiD/train/173/173_l1.jpg  \n",
            "  inflating: DeepDRiD/train/173/173_l2.jpg  \n",
            "  inflating: DeepDRiD/train/173/173_r1.jpg  \n",
            "  inflating: DeepDRiD/train/173/173_r2.jpg  \n",
            "   creating: DeepDRiD/train/174/\n",
            "  inflating: DeepDRiD/train/174/174_l1.jpg  \n",
            "  inflating: DeepDRiD/train/174/174_l2.jpg  \n",
            "  inflating: DeepDRiD/train/174/174_r1.jpg  \n",
            "  inflating: DeepDRiD/train/174/174_r2.jpg  \n",
            "   creating: DeepDRiD/train/175/\n",
            "  inflating: DeepDRiD/train/175/175_l1.jpg  \n",
            "  inflating: DeepDRiD/train/175/175_l2.jpg  \n",
            "  inflating: DeepDRiD/train/175/175_r1.jpg  \n",
            "  inflating: DeepDRiD/train/175/175_r2.jpg  \n",
            "   creating: DeepDRiD/train/176/\n",
            "  inflating: DeepDRiD/train/176/176_l1.jpg  \n",
            "  inflating: DeepDRiD/train/176/176_l2.jpg  \n",
            "  inflating: DeepDRiD/train/176/176_r1.jpg  \n",
            "  inflating: DeepDRiD/train/176/176_r2.jpg  \n",
            "   creating: DeepDRiD/train/177/\n",
            "  inflating: DeepDRiD/train/177/177_l1.jpg  \n",
            "  inflating: DeepDRiD/train/177/177_l2.jpg  \n",
            "  inflating: DeepDRiD/train/177/177_r1.jpg  \n",
            "  inflating: DeepDRiD/train/177/177_r2.jpg  \n",
            "   creating: DeepDRiD/train/178/\n",
            "  inflating: DeepDRiD/train/178/178_l1.jpg  \n",
            "  inflating: DeepDRiD/train/178/178_l2.jpg  \n",
            "  inflating: DeepDRiD/train/178/178_r1.jpg  \n",
            "  inflating: DeepDRiD/train/178/178_r2.jpg  \n",
            "   creating: DeepDRiD/train/179/\n",
            "  inflating: DeepDRiD/train/179/179_l1.jpg  \n",
            "  inflating: DeepDRiD/train/179/179_l2.jpg  \n",
            "  inflating: DeepDRiD/train/179/179_r1.jpg  \n",
            "  inflating: DeepDRiD/train/179/179_r2.jpg  \n",
            "   creating: DeepDRiD/train/18/\n",
            "  inflating: DeepDRiD/train/18/18_l1.jpg  \n",
            "  inflating: DeepDRiD/train/18/18_l2.jpg  \n",
            "  inflating: DeepDRiD/train/18/18_r1.jpg  \n",
            "  inflating: DeepDRiD/train/18/18_r2.jpg  \n",
            "   creating: DeepDRiD/train/180/\n",
            "  inflating: DeepDRiD/train/180/180_l1.jpg  \n",
            "  inflating: DeepDRiD/train/180/180_l2.jpg  \n",
            "  inflating: DeepDRiD/train/180/180_r1.jpg  \n",
            "  inflating: DeepDRiD/train/180/180_r2.jpg  \n",
            "   creating: DeepDRiD/train/181/\n",
            "  inflating: DeepDRiD/train/181/181_l1.jpg  \n",
            "  inflating: DeepDRiD/train/181/181_l2.jpg  \n",
            "  inflating: DeepDRiD/train/181/181_r1.jpg  \n",
            "  inflating: DeepDRiD/train/181/181_r2.jpg  \n",
            "   creating: DeepDRiD/train/182/\n",
            "  inflating: DeepDRiD/train/182/182_l1.jpg  \n",
            "  inflating: DeepDRiD/train/182/182_l2.jpg  \n",
            "  inflating: DeepDRiD/train/182/182_r1.jpg  \n",
            "  inflating: DeepDRiD/train/182/182_r2.jpg  \n",
            "   creating: DeepDRiD/train/183/\n",
            "  inflating: DeepDRiD/train/183/183_l1.jpg  \n",
            "  inflating: DeepDRiD/train/183/183_l2.jpg  \n",
            "  inflating: DeepDRiD/train/183/183_r1.jpg  \n",
            "  inflating: DeepDRiD/train/183/183_r2.jpg  \n",
            "   creating: DeepDRiD/train/184/\n",
            "  inflating: DeepDRiD/train/184/184_l1.jpg  \n",
            "  inflating: DeepDRiD/train/184/184_l2.jpg  \n",
            "  inflating: DeepDRiD/train/184/184_r1.jpg  \n",
            "  inflating: DeepDRiD/train/184/184_r2.jpg  \n",
            "   creating: DeepDRiD/train/185/\n",
            "  inflating: DeepDRiD/train/185/185_l1.jpg  \n",
            "  inflating: DeepDRiD/train/185/185_l2.jpg  \n",
            "  inflating: DeepDRiD/train/185/185_r1.jpg  \n",
            "  inflating: DeepDRiD/train/185/185_r2.jpg  \n",
            "   creating: DeepDRiD/train/186/\n",
            "  inflating: DeepDRiD/train/186/186_l1.jpg  \n",
            "  inflating: DeepDRiD/train/186/186_l2.jpg  \n",
            "  inflating: DeepDRiD/train/186/186_r1.jpg  \n",
            "  inflating: DeepDRiD/train/186/186_r2.jpg  \n",
            "   creating: DeepDRiD/train/187/\n",
            "  inflating: DeepDRiD/train/187/187_l1.jpg  \n",
            "  inflating: DeepDRiD/train/187/187_l2.jpg  \n",
            "  inflating: DeepDRiD/train/187/187_r1.jpg  \n",
            "  inflating: DeepDRiD/train/187/187_r2.jpg  \n",
            "   creating: DeepDRiD/train/188/\n",
            "  inflating: DeepDRiD/train/188/188_l1.jpg  \n",
            "  inflating: DeepDRiD/train/188/188_l2.jpg  \n",
            "  inflating: DeepDRiD/train/188/188_r1.jpg  \n",
            "  inflating: DeepDRiD/train/188/188_r2.jpg  \n",
            "   creating: DeepDRiD/train/189/\n",
            "  inflating: DeepDRiD/train/189/189_l1.jpg  \n",
            "  inflating: DeepDRiD/train/189/189_l2.jpg  \n",
            "  inflating: DeepDRiD/train/189/189_r1.jpg  \n",
            "  inflating: DeepDRiD/train/189/189_r2.jpg  \n",
            "   creating: DeepDRiD/train/19/\n",
            "  inflating: DeepDRiD/train/19/19_l1.jpg  \n",
            "  inflating: DeepDRiD/train/19/19_l2.jpg  \n",
            "  inflating: DeepDRiD/train/19/19_r1.jpg  \n",
            "  inflating: DeepDRiD/train/19/19_r2.jpg  \n",
            "   creating: DeepDRiD/train/190/\n",
            "  inflating: DeepDRiD/train/190/190_l1.jpg  \n",
            "  inflating: DeepDRiD/train/190/190_l2.jpg  \n",
            "  inflating: DeepDRiD/train/190/190_r1.jpg  \n",
            "  inflating: DeepDRiD/train/190/190_r2.jpg  \n",
            "   creating: DeepDRiD/train/191/\n",
            "  inflating: DeepDRiD/train/191/191_l1.jpg  \n",
            "  inflating: DeepDRiD/train/191/191_l2.jpg  \n",
            "  inflating: DeepDRiD/train/191/191_r1.jpg  \n",
            "  inflating: DeepDRiD/train/191/191_r2.jpg  \n",
            "   creating: DeepDRiD/train/192/\n",
            "  inflating: DeepDRiD/train/192/192_l1.jpg  \n",
            "  inflating: DeepDRiD/train/192/192_l2.jpg  \n",
            "  inflating: DeepDRiD/train/192/192_r1.jpg  \n",
            "  inflating: DeepDRiD/train/192/192_r2.jpg  \n",
            "   creating: DeepDRiD/train/193/\n",
            "  inflating: DeepDRiD/train/193/193_l1.jpg  \n",
            "  inflating: DeepDRiD/train/193/193_l2.jpg  \n",
            "  inflating: DeepDRiD/train/193/193_r1.jpg  \n",
            "  inflating: DeepDRiD/train/193/193_r2.jpg  \n",
            "   creating: DeepDRiD/train/194/\n",
            "  inflating: DeepDRiD/train/194/194_l1.jpg  \n",
            "  inflating: DeepDRiD/train/194/194_l2.jpg  \n",
            "  inflating: DeepDRiD/train/194/194_r1.jpg  \n",
            "  inflating: DeepDRiD/train/194/194_r2.jpg  \n",
            "   creating: DeepDRiD/train/195/\n",
            "  inflating: DeepDRiD/train/195/195_l1.jpg  \n",
            "  inflating: DeepDRiD/train/195/195_l2.jpg  \n",
            "  inflating: DeepDRiD/train/195/195_r1.jpg  \n",
            "  inflating: DeepDRiD/train/195/195_r2.jpg  \n",
            "   creating: DeepDRiD/train/196/\n",
            "  inflating: DeepDRiD/train/196/196_l1.jpg  \n",
            "  inflating: DeepDRiD/train/196/196_l2.jpg  \n",
            "  inflating: DeepDRiD/train/196/196_r1.jpg  \n",
            "  inflating: DeepDRiD/train/196/196_r2.jpg  \n",
            "   creating: DeepDRiD/train/197/\n",
            "  inflating: DeepDRiD/train/197/197_l1.jpg  \n",
            "  inflating: DeepDRiD/train/197/197_l2.jpg  \n",
            "  inflating: DeepDRiD/train/197/197_r1.jpg  \n",
            "  inflating: DeepDRiD/train/197/197_r2.jpg  \n",
            "   creating: DeepDRiD/train/198/\n",
            "  inflating: DeepDRiD/train/198/198_l1.jpg  \n",
            "  inflating: DeepDRiD/train/198/198_l2.jpg  \n",
            "  inflating: DeepDRiD/train/198/198_r1.jpg  \n",
            "  inflating: DeepDRiD/train/198/198_r2.jpg  \n",
            "   creating: DeepDRiD/train/199/\n",
            "  inflating: DeepDRiD/train/199/199_l1.jpg  \n",
            "  inflating: DeepDRiD/train/199/199_l2.jpg  \n",
            "  inflating: DeepDRiD/train/199/199_r1.jpg  \n",
            "  inflating: DeepDRiD/train/199/199_r2.jpg  \n",
            "   creating: DeepDRiD/train/2/\n",
            "  inflating: DeepDRiD/train/2/2_l1.jpg  \n",
            "  inflating: DeepDRiD/train/2/2_l2.jpg  \n",
            "  inflating: DeepDRiD/train/2/2_r1.jpg  \n",
            "  inflating: DeepDRiD/train/2/2_r2.jpg  \n",
            "   creating: DeepDRiD/train/20/\n",
            "  inflating: DeepDRiD/train/20/20_l1.jpg  \n",
            "  inflating: DeepDRiD/train/20/20_l2.jpg  \n",
            "  inflating: DeepDRiD/train/20/20_r1.jpg  \n",
            "  inflating: DeepDRiD/train/20/20_r2.jpg  \n",
            "   creating: DeepDRiD/train/200/\n",
            "  inflating: DeepDRiD/train/200/200_l1.jpg  \n",
            "  inflating: DeepDRiD/train/200/200_l2.jpg  \n",
            "  inflating: DeepDRiD/train/200/200_r1.jpg  \n",
            "  inflating: DeepDRiD/train/200/200_r2.jpg  \n",
            "   creating: DeepDRiD/train/201/\n",
            "  inflating: DeepDRiD/train/201/201_l1.jpg  \n",
            "  inflating: DeepDRiD/train/201/201_l2.jpg  \n",
            "  inflating: DeepDRiD/train/201/201_r1.jpg  \n",
            "  inflating: DeepDRiD/train/201/201_r2.jpg  \n",
            "   creating: DeepDRiD/train/202/\n",
            "  inflating: DeepDRiD/train/202/202_l1.jpg  \n",
            "  inflating: DeepDRiD/train/202/202_l2.jpg  \n",
            "  inflating: DeepDRiD/train/202/202_r1.jpg  \n",
            "  inflating: DeepDRiD/train/202/202_r2.jpg  \n",
            "   creating: DeepDRiD/train/203/\n",
            "  inflating: DeepDRiD/train/203/203_l1.jpg  \n",
            "  inflating: DeepDRiD/train/203/203_l2.jpg  \n",
            "  inflating: DeepDRiD/train/203/203_r1.jpg  \n",
            "  inflating: DeepDRiD/train/203/203_r2.jpg  \n",
            "   creating: DeepDRiD/train/204/\n",
            "  inflating: DeepDRiD/train/204/204_l1.jpg  \n",
            "  inflating: DeepDRiD/train/204/204_l2.jpg  \n",
            "  inflating: DeepDRiD/train/204/204_r1.jpg  \n",
            "  inflating: DeepDRiD/train/204/204_r2.jpg  \n",
            "   creating: DeepDRiD/train/205/\n",
            "  inflating: DeepDRiD/train/205/205_l1.jpg  \n",
            "  inflating: DeepDRiD/train/205/205_l2.jpg  \n",
            "  inflating: DeepDRiD/train/205/205_r1.jpg  \n",
            "  inflating: DeepDRiD/train/205/205_r2.jpg  \n",
            "   creating: DeepDRiD/train/206/\n",
            "  inflating: DeepDRiD/train/206/206_l1.jpg  \n",
            "  inflating: DeepDRiD/train/206/206_l2.jpg  \n",
            "  inflating: DeepDRiD/train/206/206_r1.jpg  \n",
            "  inflating: DeepDRiD/train/206/206_r2.jpg  \n",
            "   creating: DeepDRiD/train/207/\n",
            "  inflating: DeepDRiD/train/207/207_l1.jpg  \n",
            "  inflating: DeepDRiD/train/207/207_l2.jpg  \n",
            "  inflating: DeepDRiD/train/207/207_r1.jpg  \n",
            "  inflating: DeepDRiD/train/207/207_r2.jpg  \n",
            "   creating: DeepDRiD/train/208/\n",
            "  inflating: DeepDRiD/train/208/208_l1.jpg  \n",
            "  inflating: DeepDRiD/train/208/208_l2.jpg  \n",
            "  inflating: DeepDRiD/train/208/208_r1.jpg  \n",
            "  inflating: DeepDRiD/train/208/208_r2.jpg  \n",
            "   creating: DeepDRiD/train/209/\n",
            "  inflating: DeepDRiD/train/209/209_l1.jpg  \n",
            "  inflating: DeepDRiD/train/209/209_l2.jpg  \n",
            "  inflating: DeepDRiD/train/209/209_r1.jpg  \n",
            "  inflating: DeepDRiD/train/209/209_r2.jpg  \n",
            "   creating: DeepDRiD/train/21/\n",
            "  inflating: DeepDRiD/train/21/21_l1.jpg  \n",
            "  inflating: DeepDRiD/train/21/21_l2.jpg  \n",
            "  inflating: DeepDRiD/train/21/21_r1.jpg  \n",
            "  inflating: DeepDRiD/train/21/21_r2.jpg  \n",
            "   creating: DeepDRiD/train/210/\n",
            "  inflating: DeepDRiD/train/210/210_l1.jpg  \n",
            "  inflating: DeepDRiD/train/210/210_l2.jpg  \n",
            "  inflating: DeepDRiD/train/210/210_r1.jpg  \n",
            "  inflating: DeepDRiD/train/210/210_r2.jpg  \n",
            "   creating: DeepDRiD/train/211/\n",
            "  inflating: DeepDRiD/train/211/211_l1.jpg  \n",
            "  inflating: DeepDRiD/train/211/211_l2.jpg  \n",
            "  inflating: DeepDRiD/train/211/211_r1.jpg  \n",
            "  inflating: DeepDRiD/train/211/211_r2.jpg  \n",
            "   creating: DeepDRiD/train/212/\n",
            "  inflating: DeepDRiD/train/212/212_l1.jpg  \n",
            "  inflating: DeepDRiD/train/212/212_l2.jpg  \n",
            "  inflating: DeepDRiD/train/212/212_r1.jpg  \n",
            "  inflating: DeepDRiD/train/212/212_r2.jpg  \n",
            "   creating: DeepDRiD/train/213/\n",
            "  inflating: DeepDRiD/train/213/213_l1.jpg  \n",
            "  inflating: DeepDRiD/train/213/213_l2.jpg  \n",
            "  inflating: DeepDRiD/train/213/213_r1.jpg  \n",
            "  inflating: DeepDRiD/train/213/213_r2.jpg  \n",
            "   creating: DeepDRiD/train/214/\n",
            "  inflating: DeepDRiD/train/214/214_l1.jpg  \n",
            "  inflating: DeepDRiD/train/214/214_l2.jpg  \n",
            "  inflating: DeepDRiD/train/214/214_r1.jpg  \n",
            "  inflating: DeepDRiD/train/214/214_r2.jpg  \n",
            "   creating: DeepDRiD/train/215/\n",
            "  inflating: DeepDRiD/train/215/215_l1.jpg  \n",
            "  inflating: DeepDRiD/train/215/215_l2.jpg  \n",
            "  inflating: DeepDRiD/train/215/215_r1.jpg  \n",
            "  inflating: DeepDRiD/train/215/215_r2.jpg  \n",
            "   creating: DeepDRiD/train/216/\n",
            "  inflating: DeepDRiD/train/216/216_l1.jpg  \n",
            "  inflating: DeepDRiD/train/216/216_l2.jpg  \n",
            "  inflating: DeepDRiD/train/216/216_r1.jpg  \n",
            "  inflating: DeepDRiD/train/216/216_r2.jpg  \n",
            "   creating: DeepDRiD/train/217/\n",
            "  inflating: DeepDRiD/train/217/217_l1.jpg  \n",
            "  inflating: DeepDRiD/train/217/217_l2.jpg  \n",
            "  inflating: DeepDRiD/train/217/217_r1.jpg  \n",
            "  inflating: DeepDRiD/train/217/217_r2.jpg  \n",
            "   creating: DeepDRiD/train/218/\n",
            "  inflating: DeepDRiD/train/218/218_l1.jpg  \n",
            "  inflating: DeepDRiD/train/218/218_l2.jpg  \n",
            "  inflating: DeepDRiD/train/218/218_r1.jpg  \n",
            "  inflating: DeepDRiD/train/218/218_r2.jpg  \n",
            "   creating: DeepDRiD/train/219/\n",
            "  inflating: DeepDRiD/train/219/219_l1.jpg  \n",
            "  inflating: DeepDRiD/train/219/219_l2.jpg  \n",
            "  inflating: DeepDRiD/train/219/219_r1.jpg  \n",
            "  inflating: DeepDRiD/train/219/219_r2.jpg  \n",
            "   creating: DeepDRiD/train/22/\n",
            "  inflating: DeepDRiD/train/22/22_l1.jpg  \n",
            "  inflating: DeepDRiD/train/22/22_l2.jpg  \n",
            "  inflating: DeepDRiD/train/22/22_r1.jpg  \n",
            "  inflating: DeepDRiD/train/22/22_r2.jpg  \n",
            "   creating: DeepDRiD/train/220/\n",
            "  inflating: DeepDRiD/train/220/220_l1.jpg  \n",
            "  inflating: DeepDRiD/train/220/220_l2.jpg  \n",
            "  inflating: DeepDRiD/train/220/220_r1.jpg  \n",
            "  inflating: DeepDRiD/train/220/220_r2.jpg  \n",
            "   creating: DeepDRiD/train/221/\n",
            "  inflating: DeepDRiD/train/221/221_l1.jpg  \n",
            "  inflating: DeepDRiD/train/221/221_l2.jpg  \n",
            "  inflating: DeepDRiD/train/221/221_r1.jpg  \n",
            "  inflating: DeepDRiD/train/221/221_r2.jpg  \n",
            "   creating: DeepDRiD/train/222/\n",
            "  inflating: DeepDRiD/train/222/222_l1.jpg  \n",
            "  inflating: DeepDRiD/train/222/222_l2.jpg  \n",
            "  inflating: DeepDRiD/train/222/222_r1.jpg  \n",
            "  inflating: DeepDRiD/train/222/222_r2.jpg  \n",
            "   creating: DeepDRiD/train/223/\n",
            "  inflating: DeepDRiD/train/223/223_l1.jpg  \n",
            "  inflating: DeepDRiD/train/223/223_l2.jpg  \n",
            "  inflating: DeepDRiD/train/223/223_r1.jpg  \n",
            "  inflating: DeepDRiD/train/223/223_r2.jpg  \n",
            "   creating: DeepDRiD/train/224/\n",
            "  inflating: DeepDRiD/train/224/224_l1.jpg  \n",
            "  inflating: DeepDRiD/train/224/224_l2.jpg  \n",
            "  inflating: DeepDRiD/train/224/224_r1.jpg  \n",
            "  inflating: DeepDRiD/train/224/224_r2.jpg  \n",
            "   creating: DeepDRiD/train/225/\n",
            "  inflating: DeepDRiD/train/225/225_l1.jpg  \n",
            "  inflating: DeepDRiD/train/225/225_l2.jpg  \n",
            "  inflating: DeepDRiD/train/225/225_r1.jpg  \n",
            "  inflating: DeepDRiD/train/225/225_r2.jpg  \n",
            "   creating: DeepDRiD/train/226/\n",
            "  inflating: DeepDRiD/train/226/226_l1.jpg  \n",
            "  inflating: DeepDRiD/train/226/226_l2.jpg  \n",
            "  inflating: DeepDRiD/train/226/226_r1.jpg  \n",
            "  inflating: DeepDRiD/train/226/226_r2.jpg  \n",
            "   creating: DeepDRiD/train/227/\n",
            "  inflating: DeepDRiD/train/227/227_l1.jpg  \n",
            "  inflating: DeepDRiD/train/227/227_l2.jpg  \n",
            "  inflating: DeepDRiD/train/227/227_r1.jpg  \n",
            "  inflating: DeepDRiD/train/227/227_r2.jpg  \n",
            "   creating: DeepDRiD/train/228/\n",
            "  inflating: DeepDRiD/train/228/228_l1.jpg  \n",
            "  inflating: DeepDRiD/train/228/228_l2.jpg  \n",
            "  inflating: DeepDRiD/train/228/228_r1.jpg  \n",
            "  inflating: DeepDRiD/train/228/228_r2.jpg  \n",
            "   creating: DeepDRiD/train/229/\n",
            "  inflating: DeepDRiD/train/229/229_l1.jpg  \n",
            "  inflating: DeepDRiD/train/229/229_l2.jpg  \n",
            "  inflating: DeepDRiD/train/229/229_r1.jpg  \n",
            "  inflating: DeepDRiD/train/229/229_r2.jpg  \n",
            "   creating: DeepDRiD/train/23/\n",
            "  inflating: DeepDRiD/train/23/23_l1.jpg  \n",
            "  inflating: DeepDRiD/train/23/23_l2.jpg  \n",
            "  inflating: DeepDRiD/train/23/23_r1.jpg  \n",
            "  inflating: DeepDRiD/train/23/23_r2.jpg  \n",
            "   creating: DeepDRiD/train/230/\n",
            "  inflating: DeepDRiD/train/230/230_l1.jpg  \n",
            "  inflating: DeepDRiD/train/230/230_l2.jpg  \n",
            "  inflating: DeepDRiD/train/230/230_r1.jpg  \n",
            "  inflating: DeepDRiD/train/230/230_r2.jpg  \n",
            "   creating: DeepDRiD/train/231/\n",
            "  inflating: DeepDRiD/train/231/231_l1.jpg  \n",
            "  inflating: DeepDRiD/train/231/231_l2.jpg  \n",
            "  inflating: DeepDRiD/train/231/231_r1.jpg  \n",
            "  inflating: DeepDRiD/train/231/231_r2.jpg  \n",
            "   creating: DeepDRiD/train/232/\n",
            "  inflating: DeepDRiD/train/232/232_l1.jpg  \n",
            "  inflating: DeepDRiD/train/232/232_l2.jpg  \n",
            "  inflating: DeepDRiD/train/232/232_r1.jpg  \n",
            "  inflating: DeepDRiD/train/232/232_r2.jpg  \n",
            "   creating: DeepDRiD/train/233/\n",
            "  inflating: DeepDRiD/train/233/233_l1.jpg  \n",
            "  inflating: DeepDRiD/train/233/233_l2.jpg  \n",
            "  inflating: DeepDRiD/train/233/233_r1.jpg  \n",
            "  inflating: DeepDRiD/train/233/233_r2.jpg  \n",
            "   creating: DeepDRiD/train/234/\n",
            "  inflating: DeepDRiD/train/234/234_l1.jpg  \n",
            "  inflating: DeepDRiD/train/234/234_l2.jpg  \n",
            "  inflating: DeepDRiD/train/234/234_r1.jpg  \n",
            "  inflating: DeepDRiD/train/234/234_r2.jpg  \n",
            "   creating: DeepDRiD/train/235/\n",
            "  inflating: DeepDRiD/train/235/235_l1.jpg  \n",
            "  inflating: DeepDRiD/train/235/235_l2.jpg  \n",
            "  inflating: DeepDRiD/train/235/235_r1.jpg  \n",
            "  inflating: DeepDRiD/train/235/235_r2.jpg  \n",
            "   creating: DeepDRiD/train/236/\n",
            "  inflating: DeepDRiD/train/236/236_l1.jpg  \n",
            "  inflating: DeepDRiD/train/236/236_l2.jpg  \n",
            "  inflating: DeepDRiD/train/236/236_r1.jpg  \n",
            "  inflating: DeepDRiD/train/236/236_r2.jpg  \n",
            "   creating: DeepDRiD/train/237/\n",
            "  inflating: DeepDRiD/train/237/237_l1.jpg  \n",
            "  inflating: DeepDRiD/train/237/237_l2.jpg  \n",
            "  inflating: DeepDRiD/train/237/237_r1.jpg  \n",
            "  inflating: DeepDRiD/train/237/237_r2.jpg  \n",
            "   creating: DeepDRiD/train/238/\n",
            "  inflating: DeepDRiD/train/238/238_l1.jpg  \n",
            "  inflating: DeepDRiD/train/238/238_l2.jpg  \n",
            "  inflating: DeepDRiD/train/238/238_r1.jpg  \n",
            "  inflating: DeepDRiD/train/238/238_r2.jpg  \n",
            "   creating: DeepDRiD/train/239/\n",
            "  inflating: DeepDRiD/train/239/239_l1.jpg  \n",
            "  inflating: DeepDRiD/train/239/239_l2.jpg  \n",
            "  inflating: DeepDRiD/train/239/239_r1.jpg  \n",
            "  inflating: DeepDRiD/train/239/239_r2.jpg  \n",
            "   creating: DeepDRiD/train/24/\n",
            "  inflating: DeepDRiD/train/24/24_l1.jpg  \n",
            "  inflating: DeepDRiD/train/24/24_l2.jpg  \n",
            "  inflating: DeepDRiD/train/24/24_r1.jpg  \n",
            "  inflating: DeepDRiD/train/24/24_r2.jpg  \n",
            "   creating: DeepDRiD/train/240/\n",
            "  inflating: DeepDRiD/train/240/240_l1.jpg  \n",
            "  inflating: DeepDRiD/train/240/240_l2.jpg  \n",
            "  inflating: DeepDRiD/train/240/240_r1.jpg  \n",
            "  inflating: DeepDRiD/train/240/240_r2.jpg  \n",
            "   creating: DeepDRiD/train/241/\n",
            "  inflating: DeepDRiD/train/241/241_l1.jpg  \n",
            "  inflating: DeepDRiD/train/241/241_l2.jpg  \n",
            "  inflating: DeepDRiD/train/241/241_r1.jpg  \n",
            "  inflating: DeepDRiD/train/241/241_r2.jpg  \n",
            "   creating: DeepDRiD/train/242/\n",
            "  inflating: DeepDRiD/train/242/242_l1.jpg  \n",
            "  inflating: DeepDRiD/train/242/242_l2.jpg  \n",
            "  inflating: DeepDRiD/train/242/242_r1.jpg  \n",
            "  inflating: DeepDRiD/train/242/242_r2.jpg  \n",
            "   creating: DeepDRiD/train/243/\n",
            "  inflating: DeepDRiD/train/243/243_l1.jpg  \n",
            "  inflating: DeepDRiD/train/243/243_l2.jpg  \n",
            "  inflating: DeepDRiD/train/243/243_r1.jpg  \n",
            "  inflating: DeepDRiD/train/243/243_r2.jpg  \n",
            "   creating: DeepDRiD/train/244/\n",
            "  inflating: DeepDRiD/train/244/244_l1.jpg  \n",
            "  inflating: DeepDRiD/train/244/244_l2.jpg  \n",
            "  inflating: DeepDRiD/train/244/244_r1.jpg  \n",
            "  inflating: DeepDRiD/train/244/244_r2.jpg  \n",
            "   creating: DeepDRiD/train/245/\n",
            "  inflating: DeepDRiD/train/245/245_l1.jpg  \n",
            "  inflating: DeepDRiD/train/245/245_l2.jpg  \n",
            "  inflating: DeepDRiD/train/245/245_r1.jpg  \n",
            "  inflating: DeepDRiD/train/245/245_r2.jpg  \n",
            "   creating: DeepDRiD/train/246/\n",
            "  inflating: DeepDRiD/train/246/246_l1.jpg  \n",
            "  inflating: DeepDRiD/train/246/246_l2.jpg  \n",
            "  inflating: DeepDRiD/train/246/246_r1.jpg  \n",
            "  inflating: DeepDRiD/train/246/246_r2.jpg  \n",
            "   creating: DeepDRiD/train/247/\n",
            "  inflating: DeepDRiD/train/247/247_l1.jpg  \n",
            "  inflating: DeepDRiD/train/247/247_l2.jpg  \n",
            "  inflating: DeepDRiD/train/247/247_r1.jpg  \n",
            "  inflating: DeepDRiD/train/247/247_r2.jpg  \n",
            "   creating: DeepDRiD/train/248/\n",
            "  inflating: DeepDRiD/train/248/248_l1.jpg  \n",
            "  inflating: DeepDRiD/train/248/248_l2.jpg  \n",
            "  inflating: DeepDRiD/train/248/248_r1.jpg  \n",
            "  inflating: DeepDRiD/train/248/248_r2.jpg  \n",
            "   creating: DeepDRiD/train/249/\n",
            "  inflating: DeepDRiD/train/249/249_l1.jpg  \n",
            "  inflating: DeepDRiD/train/249/249_l2.jpg  \n",
            "  inflating: DeepDRiD/train/249/249_r1.jpg  \n",
            "  inflating: DeepDRiD/train/249/249_r2.jpg  \n",
            "   creating: DeepDRiD/train/25/\n",
            "  inflating: DeepDRiD/train/25/25_l1.jpg  \n",
            "  inflating: DeepDRiD/train/25/25_l2.jpg  \n",
            "  inflating: DeepDRiD/train/25/25_r1.jpg  \n",
            "  inflating: DeepDRiD/train/25/25_r2.jpg  \n",
            "   creating: DeepDRiD/train/250/\n",
            "  inflating: DeepDRiD/train/250/250_l1.jpg  \n",
            "  inflating: DeepDRiD/train/250/250_l2.jpg  \n",
            "  inflating: DeepDRiD/train/250/250_r1.jpg  \n",
            "  inflating: DeepDRiD/train/250/250_r2.jpg  \n",
            "   creating: DeepDRiD/train/251/\n",
            "  inflating: DeepDRiD/train/251/251_l1.jpg  \n",
            "  inflating: DeepDRiD/train/251/251_l2.jpg  \n",
            "  inflating: DeepDRiD/train/251/251_r1.jpg  \n",
            "  inflating: DeepDRiD/train/251/251_r2.jpg  \n",
            "   creating: DeepDRiD/train/252/\n",
            "  inflating: DeepDRiD/train/252/252_l1.jpg  \n",
            "  inflating: DeepDRiD/train/252/252_l2.jpg  \n",
            "  inflating: DeepDRiD/train/252/252_r1.jpg  \n",
            "  inflating: DeepDRiD/train/252/252_r2.jpg  \n",
            "   creating: DeepDRiD/train/253/\n",
            "  inflating: DeepDRiD/train/253/253_l1.jpg  \n",
            "  inflating: DeepDRiD/train/253/253_l2.jpg  \n",
            "  inflating: DeepDRiD/train/253/253_r1.jpg  \n",
            "  inflating: DeepDRiD/train/253/253_r2.jpg  \n",
            "   creating: DeepDRiD/train/254/\n",
            "  inflating: DeepDRiD/train/254/254_l1.jpg  \n",
            "  inflating: DeepDRiD/train/254/254_l2.jpg  \n",
            "  inflating: DeepDRiD/train/254/254_r1.jpg  \n",
            "  inflating: DeepDRiD/train/254/254_r2.jpg  \n",
            "   creating: DeepDRiD/train/255/\n",
            "  inflating: DeepDRiD/train/255/255_l1.jpg  \n",
            "  inflating: DeepDRiD/train/255/255_l2.jpg  \n",
            "  inflating: DeepDRiD/train/255/255_r1.jpg  \n",
            "  inflating: DeepDRiD/train/255/255_r2.jpg  \n",
            "   creating: DeepDRiD/train/256/\n",
            "  inflating: DeepDRiD/train/256/256_l1.jpg  \n",
            "  inflating: DeepDRiD/train/256/256_l2.jpg  \n",
            "  inflating: DeepDRiD/train/256/256_r1.jpg  \n",
            "  inflating: DeepDRiD/train/256/256_r2.jpg  \n",
            "   creating: DeepDRiD/train/257/\n",
            "  inflating: DeepDRiD/train/257/257_l1.jpg  \n",
            "  inflating: DeepDRiD/train/257/257_l2.jpg  \n",
            "  inflating: DeepDRiD/train/257/257_r1.jpg  \n",
            "  inflating: DeepDRiD/train/257/257_r2.jpg  \n",
            "   creating: DeepDRiD/train/258/\n",
            "  inflating: DeepDRiD/train/258/258_l1.jpg  \n",
            "  inflating: DeepDRiD/train/258/258_l2.jpg  \n",
            "  inflating: DeepDRiD/train/258/258_r1.jpg  \n",
            "  inflating: DeepDRiD/train/258/258_r2.jpg  \n",
            "   creating: DeepDRiD/train/259/\n",
            "  inflating: DeepDRiD/train/259/259_l1.jpg  \n",
            "  inflating: DeepDRiD/train/259/259_l2.jpg  \n",
            "  inflating: DeepDRiD/train/259/259_r1.jpg  \n",
            "  inflating: DeepDRiD/train/259/259_r2.jpg  \n",
            "   creating: DeepDRiD/train/26/\n",
            "  inflating: DeepDRiD/train/26/26_l1.jpg  \n",
            "  inflating: DeepDRiD/train/26/26_l2.jpg  \n",
            "  inflating: DeepDRiD/train/26/26_r1.jpg  \n",
            "  inflating: DeepDRiD/train/26/26_r2.jpg  \n",
            "   creating: DeepDRiD/train/260/\n",
            "  inflating: DeepDRiD/train/260/260_l1.jpg  \n",
            "  inflating: DeepDRiD/train/260/260_l2.jpg  \n",
            "  inflating: DeepDRiD/train/260/260_r1.jpg  \n",
            "  inflating: DeepDRiD/train/260/260_r2.jpg  \n",
            "   creating: DeepDRiD/train/261/\n",
            "  inflating: DeepDRiD/train/261/261_l1.jpg  \n",
            "  inflating: DeepDRiD/train/261/261_l2.jpg  \n",
            "  inflating: DeepDRiD/train/261/261_r1.jpg  \n",
            "  inflating: DeepDRiD/train/261/261_r2.jpg  \n",
            "   creating: DeepDRiD/train/262/\n",
            "  inflating: DeepDRiD/train/262/262_l1.jpg  \n",
            "  inflating: DeepDRiD/train/262/262_l2.jpg  \n",
            "  inflating: DeepDRiD/train/262/262_r1.jpg  \n",
            "  inflating: DeepDRiD/train/262/262_r2.jpg  \n",
            "   creating: DeepDRiD/train/263/\n",
            "  inflating: DeepDRiD/train/263/263_l1.jpg  \n",
            "  inflating: DeepDRiD/train/263/263_l2.jpg  \n",
            "  inflating: DeepDRiD/train/263/263_r1.jpg  \n",
            "  inflating: DeepDRiD/train/263/263_r2.jpg  \n",
            "   creating: DeepDRiD/train/264/\n",
            "  inflating: DeepDRiD/train/264/264_l1.jpg  \n",
            "  inflating: DeepDRiD/train/264/264_l2.jpg  \n",
            "  inflating: DeepDRiD/train/264/264_r1.jpg  \n",
            "  inflating: DeepDRiD/train/264/264_r2.jpg  \n",
            "   creating: DeepDRiD/train/266/\n",
            "  inflating: DeepDRiD/train/266/266_l1.jpg  \n",
            "  inflating: DeepDRiD/train/266/266_l2.jpg  \n",
            "  inflating: DeepDRiD/train/266/266_r1.jpg  \n",
            "  inflating: DeepDRiD/train/266/266_r2.jpg  \n",
            "   creating: DeepDRiD/train/268/\n",
            "  inflating: DeepDRiD/train/268/268_l1.jpg  \n",
            "  inflating: DeepDRiD/train/268/268_l2.jpg  \n",
            "  inflating: DeepDRiD/train/268/268_r1.jpg  \n",
            "  inflating: DeepDRiD/train/268/268_r2.jpg  \n",
            "   creating: DeepDRiD/train/269/\n",
            "  inflating: DeepDRiD/train/269/269_l1.jpg  \n",
            "  inflating: DeepDRiD/train/269/269_l2.jpg  \n",
            "  inflating: DeepDRiD/train/269/269_r1.jpg  \n",
            "  inflating: DeepDRiD/train/269/269_r2.jpg  \n",
            "   creating: DeepDRiD/train/27/\n",
            "  inflating: DeepDRiD/train/27/27_l1.jpg  \n",
            "  inflating: DeepDRiD/train/27/27_l2.jpg  \n",
            "  inflating: DeepDRiD/train/27/27_r1.jpg  \n",
            "  inflating: DeepDRiD/train/27/27_r2.jpg  \n",
            "   creating: DeepDRiD/train/270/\n",
            "  inflating: DeepDRiD/train/270/270_l1.jpg  \n",
            "  inflating: DeepDRiD/train/270/270_l2.jpg  \n",
            "  inflating: DeepDRiD/train/270/270_r1.jpg  \n",
            "  inflating: DeepDRiD/train/270/270_r2.jpg  \n",
            "   creating: DeepDRiD/train/271/\n",
            "  inflating: DeepDRiD/train/271/271_l1.jpg  \n",
            "  inflating: DeepDRiD/train/271/271_l2.jpg  \n",
            "  inflating: DeepDRiD/train/271/271_r1.jpg  \n",
            "  inflating: DeepDRiD/train/271/271_r2.jpg  \n",
            "   creating: DeepDRiD/train/272/\n",
            "  inflating: DeepDRiD/train/272/272_l1.jpg  \n",
            "  inflating: DeepDRiD/train/272/272_l2.jpg  \n",
            "  inflating: DeepDRiD/train/272/272_r1.jpg  \n",
            "  inflating: DeepDRiD/train/272/272_r2.jpg  \n",
            "   creating: DeepDRiD/train/273/\n",
            "  inflating: DeepDRiD/train/273/273_l1.jpg  \n",
            "  inflating: DeepDRiD/train/273/273_l2.jpg  \n",
            "  inflating: DeepDRiD/train/273/273_r1.jpg  \n",
            "  inflating: DeepDRiD/train/273/273_r2.jpg  \n",
            "   creating: DeepDRiD/train/274/\n",
            "  inflating: DeepDRiD/train/274/274_l1.jpg  \n",
            "  inflating: DeepDRiD/train/274/274_l2.jpg  \n",
            "  inflating: DeepDRiD/train/274/274_r1.jpg  \n",
            "  inflating: DeepDRiD/train/274/274_r2.jpg  \n",
            "   creating: DeepDRiD/train/275/\n",
            "  inflating: DeepDRiD/train/275/275_l1.jpg  \n",
            "  inflating: DeepDRiD/train/275/275_l2.jpg  \n",
            "  inflating: DeepDRiD/train/275/275_r1.jpg  \n",
            "  inflating: DeepDRiD/train/275/275_r2.jpg  \n",
            "   creating: DeepDRiD/train/276/\n",
            "  inflating: DeepDRiD/train/276/276_l1.jpg  \n",
            "  inflating: DeepDRiD/train/276/276_l2.jpg  \n",
            "  inflating: DeepDRiD/train/276/276_r1.jpg  \n",
            "  inflating: DeepDRiD/train/276/276_r2.jpg  \n",
            "   creating: DeepDRiD/train/278/\n",
            "  inflating: DeepDRiD/train/278/278_l1.jpg  \n",
            "  inflating: DeepDRiD/train/278/278_l2.jpg  \n",
            "  inflating: DeepDRiD/train/278/278_r1.jpg  \n",
            "  inflating: DeepDRiD/train/278/278_r2.jpg  \n",
            "   creating: DeepDRiD/train/279/\n",
            "  inflating: DeepDRiD/train/279/279_l1.jpg  \n",
            "  inflating: DeepDRiD/train/279/279_l2.jpg  \n",
            "  inflating: DeepDRiD/train/279/279_r1.jpg  \n",
            "  inflating: DeepDRiD/train/279/279_r2.jpg  \n",
            "   creating: DeepDRiD/train/28/\n",
            "  inflating: DeepDRiD/train/28/28_l1.jpg  \n",
            "  inflating: DeepDRiD/train/28/28_l2.jpg  \n",
            "  inflating: DeepDRiD/train/28/28_r1.jpg  \n",
            "  inflating: DeepDRiD/train/28/28_r2.jpg  \n",
            "   creating: DeepDRiD/train/280/\n",
            "  inflating: DeepDRiD/train/280/280_l1.jpg  \n",
            "  inflating: DeepDRiD/train/280/280_l2.jpg  \n",
            "  inflating: DeepDRiD/train/280/280_r1.jpg  \n",
            "  inflating: DeepDRiD/train/280/280_r2.jpg  \n",
            "   creating: DeepDRiD/train/281/\n",
            "  inflating: DeepDRiD/train/281/281_l1.jpg  \n",
            "  inflating: DeepDRiD/train/281/281_l2.jpg  \n",
            "  inflating: DeepDRiD/train/281/281_r1.jpg  \n",
            "  inflating: DeepDRiD/train/281/281_r2.jpg  \n",
            "   creating: DeepDRiD/train/282/\n",
            "  inflating: DeepDRiD/train/282/282_l1.jpg  \n",
            "  inflating: DeepDRiD/train/282/282_l2.jpg  \n",
            "  inflating: DeepDRiD/train/282/282_r1.jpg  \n",
            "  inflating: DeepDRiD/train/282/282_r2.jpg  \n",
            "   creating: DeepDRiD/train/283/\n",
            "  inflating: DeepDRiD/train/283/283_l1.jpg  \n",
            "  inflating: DeepDRiD/train/283/283_l2.jpg  \n",
            "  inflating: DeepDRiD/train/283/283_r1.jpg  \n",
            "  inflating: DeepDRiD/train/283/283_r2.jpg  \n",
            "   creating: DeepDRiD/train/284/\n",
            "  inflating: DeepDRiD/train/284/284_l1.jpg  \n",
            "  inflating: DeepDRiD/train/284/284_l2.jpg  \n",
            "  inflating: DeepDRiD/train/284/284_r1.jpg  \n",
            "  inflating: DeepDRiD/train/284/284_r2.jpg  \n",
            "   creating: DeepDRiD/train/285/\n",
            "  inflating: DeepDRiD/train/285/285_l1.jpg  \n",
            "  inflating: DeepDRiD/train/285/285_l2.jpg  \n",
            "  inflating: DeepDRiD/train/285/285_r1.jpg  \n",
            "  inflating: DeepDRiD/train/285/285_r2.jpg  \n",
            "   creating: DeepDRiD/train/286/\n",
            "  inflating: DeepDRiD/train/286/286_l1.jpg  \n",
            "  inflating: DeepDRiD/train/286/286_l2.jpg  \n",
            "  inflating: DeepDRiD/train/286/286_r1.jpg  \n",
            "  inflating: DeepDRiD/train/286/286_r2.jpg  \n",
            "   creating: DeepDRiD/train/287/\n",
            "  inflating: DeepDRiD/train/287/287_l1.jpg  \n",
            "  inflating: DeepDRiD/train/287/287_l2.jpg  \n",
            "  inflating: DeepDRiD/train/287/287_r1.jpg  \n",
            "  inflating: DeepDRiD/train/287/287_r2.jpg  \n",
            "   creating: DeepDRiD/train/288/\n",
            "  inflating: DeepDRiD/train/288/288_l1.jpg  \n",
            "  inflating: DeepDRiD/train/288/288_l2.jpg  \n",
            "  inflating: DeepDRiD/train/288/288_r1.jpg  \n",
            "  inflating: DeepDRiD/train/288/288_r2.jpg  \n",
            "   creating: DeepDRiD/train/289/\n",
            "  inflating: DeepDRiD/train/289/289_l1.jpg  \n",
            "  inflating: DeepDRiD/train/289/289_l2.jpg  \n",
            "  inflating: DeepDRiD/train/289/289_r1.jpg  \n",
            "  inflating: DeepDRiD/train/289/289_r2.jpg  \n",
            "   creating: DeepDRiD/train/29/\n",
            "  inflating: DeepDRiD/train/29/29_l1.jpg  \n",
            "  inflating: DeepDRiD/train/29/29_l2.jpg  \n",
            "  inflating: DeepDRiD/train/29/29_r1.jpg  \n",
            "  inflating: DeepDRiD/train/29/29_r2.jpg  \n",
            "   creating: DeepDRiD/train/290/\n",
            "  inflating: DeepDRiD/train/290/290_l1.jpg  \n",
            "  inflating: DeepDRiD/train/290/290_l2.jpg  \n",
            "  inflating: DeepDRiD/train/290/290_r1.jpg  \n",
            "  inflating: DeepDRiD/train/290/290_r2.jpg  \n",
            "   creating: DeepDRiD/train/291/\n",
            "  inflating: DeepDRiD/train/291/291_l1.jpg  \n",
            "  inflating: DeepDRiD/train/291/291_l2.jpg  \n",
            "  inflating: DeepDRiD/train/291/291_r1.jpg  \n",
            "  inflating: DeepDRiD/train/291/291_r2.jpg  \n",
            "   creating: DeepDRiD/train/292/\n",
            "  inflating: DeepDRiD/train/292/292_l1.jpg  \n",
            "  inflating: DeepDRiD/train/292/292_l2.jpg  \n",
            "  inflating: DeepDRiD/train/292/292_r1.jpg  \n",
            "  inflating: DeepDRiD/train/292/292_r2.jpg  \n",
            "   creating: DeepDRiD/train/293/\n",
            "  inflating: DeepDRiD/train/293/293_l1.jpg  \n",
            "  inflating: DeepDRiD/train/293/293_l2.jpg  \n",
            "  inflating: DeepDRiD/train/293/293_r1.jpg  \n",
            "  inflating: DeepDRiD/train/293/293_r2.jpg  \n",
            "   creating: DeepDRiD/train/295/\n",
            "  inflating: DeepDRiD/train/295/295_l1.jpg  \n",
            "  inflating: DeepDRiD/train/295/295_l2.jpg  \n",
            "  inflating: DeepDRiD/train/295/295_r1.jpg  \n",
            "  inflating: DeepDRiD/train/295/295_r2.jpg  \n",
            "   creating: DeepDRiD/train/297/\n",
            "  inflating: DeepDRiD/train/297/297_l1.jpg  \n",
            "  inflating: DeepDRiD/train/297/297_l2.jpg  \n",
            "  inflating: DeepDRiD/train/297/297_r1.jpg  \n",
            "  inflating: DeepDRiD/train/297/297_r2.jpg  \n",
            "   creating: DeepDRiD/train/3/\n",
            "  inflating: DeepDRiD/train/3/3_l1.jpg  \n",
            "  inflating: DeepDRiD/train/3/3_l2.jpg  \n",
            "  inflating: DeepDRiD/train/3/3_r1.jpg  \n",
            "  inflating: DeepDRiD/train/3/3_r2.jpg  \n",
            "   creating: DeepDRiD/train/30/\n",
            "  inflating: DeepDRiD/train/30/30_l1.jpg  \n",
            "  inflating: DeepDRiD/train/30/30_l2.jpg  \n",
            "  inflating: DeepDRiD/train/30/30_r1.jpg  \n",
            "  inflating: DeepDRiD/train/30/30_r2.jpg  \n",
            "   creating: DeepDRiD/train/302/\n",
            "  inflating: DeepDRiD/train/302/302_l1.jpg  \n",
            "  inflating: DeepDRiD/train/302/302_l2.jpg  \n",
            "  inflating: DeepDRiD/train/302/302_r1.jpg  \n",
            "  inflating: DeepDRiD/train/302/302_r2.jpg  \n",
            "   creating: DeepDRiD/train/308/\n",
            "  inflating: DeepDRiD/train/308/308_l1.jpg  \n",
            "  inflating: DeepDRiD/train/308/308_l2.jpg  \n",
            "  inflating: DeepDRiD/train/308/308_r1.jpg  \n",
            "  inflating: DeepDRiD/train/308/308_r2.jpg  \n",
            "   creating: DeepDRiD/train/31/\n",
            "  inflating: DeepDRiD/train/31/31_l1.jpg  \n",
            "  inflating: DeepDRiD/train/31/31_l2.jpg  \n",
            "  inflating: DeepDRiD/train/31/31_r1.jpg  \n",
            "  inflating: DeepDRiD/train/31/31_r2.jpg  \n",
            "   creating: DeepDRiD/train/312/\n",
            "  inflating: DeepDRiD/train/312/312_l1.jpg  \n",
            "  inflating: DeepDRiD/train/312/312_l2.jpg  \n",
            "  inflating: DeepDRiD/train/312/312_r1.jpg  \n",
            "  inflating: DeepDRiD/train/312/312_r2.jpg  \n",
            "   creating: DeepDRiD/train/313/\n",
            "  inflating: DeepDRiD/train/313/313_l1.jpg  \n",
            "  inflating: DeepDRiD/train/313/313_l2.jpg  \n",
            "  inflating: DeepDRiD/train/313/313_r1.jpg  \n",
            "  inflating: DeepDRiD/train/313/313_r2.jpg  \n",
            "   creating: DeepDRiD/train/32/\n",
            "  inflating: DeepDRiD/train/32/32_l1.jpg  \n",
            "  inflating: DeepDRiD/train/32/32_l2.jpg  \n",
            "  inflating: DeepDRiD/train/32/32_r1.jpg  \n",
            "  inflating: DeepDRiD/train/32/32_r2.jpg  \n",
            "   creating: DeepDRiD/train/324/\n",
            "  inflating: DeepDRiD/train/324/324_l1.jpg  \n",
            "  inflating: DeepDRiD/train/324/324_l2.jpg  \n",
            "  inflating: DeepDRiD/train/324/324_r1.jpg  \n",
            "  inflating: DeepDRiD/train/324/324_r2.jpg  \n",
            "   creating: DeepDRiD/train/327/\n",
            "  inflating: DeepDRiD/train/327/327_l1.jpg  \n",
            "  inflating: DeepDRiD/train/327/327_l2.jpg  \n",
            "  inflating: DeepDRiD/train/327/327_r1.jpg  \n",
            "  inflating: DeepDRiD/train/327/327_r2.jpg  \n",
            "   creating: DeepDRiD/train/329/\n",
            "  inflating: DeepDRiD/train/329/329_l1.jpg  \n",
            "  inflating: DeepDRiD/train/329/329_l2.jpg  \n",
            "  inflating: DeepDRiD/train/329/329_r1.jpg  \n",
            "  inflating: DeepDRiD/train/329/329_r2.jpg  \n",
            "   creating: DeepDRiD/train/33/\n",
            "  inflating: DeepDRiD/train/33/33_l1.jpg  \n",
            "  inflating: DeepDRiD/train/33/33_l2.jpg  \n",
            "  inflating: DeepDRiD/train/33/33_r1.jpg  \n",
            "  inflating: DeepDRiD/train/33/33_r2.jpg  \n",
            "   creating: DeepDRiD/train/330/\n",
            "  inflating: DeepDRiD/train/330/330_l1.jpg  \n",
            "  inflating: DeepDRiD/train/330/330_l2.jpg  \n",
            "  inflating: DeepDRiD/train/330/330_r1.jpg  \n",
            "  inflating: DeepDRiD/train/330/330_r2.jpg  \n",
            "   creating: DeepDRiD/train/34/\n",
            "  inflating: DeepDRiD/train/34/34_l1.jpg  \n",
            "  inflating: DeepDRiD/train/34/34_l2.jpg  \n",
            "  inflating: DeepDRiD/train/34/34_r1.jpg  \n",
            "  inflating: DeepDRiD/train/34/34_r2.jpg  \n",
            "   creating: DeepDRiD/train/35/\n",
            "  inflating: DeepDRiD/train/35/35_l1.jpg  \n",
            "  inflating: DeepDRiD/train/35/35_l2.jpg  \n",
            "  inflating: DeepDRiD/train/35/35_r1.jpg  \n",
            "  inflating: DeepDRiD/train/35/35_r2.jpg  \n",
            "   creating: DeepDRiD/train/36/\n",
            "  inflating: DeepDRiD/train/36/36_l1.jpg  \n",
            "  inflating: DeepDRiD/train/36/36_l2.jpg  \n",
            "  inflating: DeepDRiD/train/36/36_r1.jpg  \n",
            "  inflating: DeepDRiD/train/36/36_r2.jpg  \n",
            "   creating: DeepDRiD/train/37/\n",
            "  inflating: DeepDRiD/train/37/37_l1.jpg  \n",
            "  inflating: DeepDRiD/train/37/37_l2.jpg  \n",
            "  inflating: DeepDRiD/train/37/37_r1.jpg  \n",
            "  inflating: DeepDRiD/train/37/37_r2.jpg  \n",
            "   creating: DeepDRiD/train/38/\n",
            "  inflating: DeepDRiD/train/38/38_l1.jpg  \n",
            "  inflating: DeepDRiD/train/38/38_l2.jpg  \n",
            "  inflating: DeepDRiD/train/38/38_r1.jpg  \n",
            "  inflating: DeepDRiD/train/38/38_r2.jpg  \n",
            "   creating: DeepDRiD/train/39/\n",
            "  inflating: DeepDRiD/train/39/39_l1.jpg  \n",
            "  inflating: DeepDRiD/train/39/39_l2.jpg  \n",
            "  inflating: DeepDRiD/train/39/39_r1.jpg  \n",
            "  inflating: DeepDRiD/train/39/39_r2.jpg  \n",
            "   creating: DeepDRiD/train/4/\n",
            "  inflating: DeepDRiD/train/4/4_l1.jpg  \n",
            "  inflating: DeepDRiD/train/4/4_l2.jpg  \n",
            "  inflating: DeepDRiD/train/4/4_r1.jpg  \n",
            "  inflating: DeepDRiD/train/4/4_r2.jpg  \n",
            "   creating: DeepDRiD/train/40/\n",
            "  inflating: DeepDRiD/train/40/40_l1.jpg  \n",
            "  inflating: DeepDRiD/train/40/40_l2.jpg  \n",
            "  inflating: DeepDRiD/train/40/40_r1.jpg  \n",
            "  inflating: DeepDRiD/train/40/40_r2.jpg  \n",
            "   creating: DeepDRiD/train/41/\n",
            "  inflating: DeepDRiD/train/41/41_l1.jpg  \n",
            "  inflating: DeepDRiD/train/41/41_l2.jpg  \n",
            "  inflating: DeepDRiD/train/41/41_r1.jpg  \n",
            "  inflating: DeepDRiD/train/41/41_r2.jpg  \n",
            "   creating: DeepDRiD/train/42/\n",
            "  inflating: DeepDRiD/train/42/42_l1.jpg  \n",
            "  inflating: DeepDRiD/train/42/42_l2.jpg  \n",
            "  inflating: DeepDRiD/train/42/42_r1.jpg  \n",
            "  inflating: DeepDRiD/train/42/42_r2.jpg  \n",
            "   creating: DeepDRiD/train/43/\n",
            "  inflating: DeepDRiD/train/43/43_l1.jpg  \n",
            "  inflating: DeepDRiD/train/43/43_l2.jpg  \n",
            "  inflating: DeepDRiD/train/43/43_r1.jpg  \n",
            "  inflating: DeepDRiD/train/43/43_r2.jpg  \n",
            "   creating: DeepDRiD/train/44/\n",
            "  inflating: DeepDRiD/train/44/44_l1.jpg  \n",
            "  inflating: DeepDRiD/train/44/44_l2.jpg  \n",
            "  inflating: DeepDRiD/train/44/44_r1.jpg  \n",
            "  inflating: DeepDRiD/train/44/44_r2.jpg  \n",
            "   creating: DeepDRiD/train/45/\n",
            "  inflating: DeepDRiD/train/45/45_l1.jpg  \n",
            "  inflating: DeepDRiD/train/45/45_l2.jpg  \n",
            "  inflating: DeepDRiD/train/45/45_r1.jpg  \n",
            "  inflating: DeepDRiD/train/45/45_r2.jpg  \n",
            "   creating: DeepDRiD/train/46/\n",
            "  inflating: DeepDRiD/train/46/46_l1.jpg  \n",
            "  inflating: DeepDRiD/train/46/46_l2.jpg  \n",
            "  inflating: DeepDRiD/train/46/46_r1.jpg  \n",
            "  inflating: DeepDRiD/train/46/46_r2.jpg  \n",
            "   creating: DeepDRiD/train/47/\n",
            "  inflating: DeepDRiD/train/47/47_l1.jpg  \n",
            "  inflating: DeepDRiD/train/47/47_l2.jpg  \n",
            "  inflating: DeepDRiD/train/47/47_r1.jpg  \n",
            "  inflating: DeepDRiD/train/47/47_r2.jpg  \n",
            "   creating: DeepDRiD/train/48/\n",
            "  inflating: DeepDRiD/train/48/48_l1.jpg  \n",
            "  inflating: DeepDRiD/train/48/48_l2.jpg  \n",
            "  inflating: DeepDRiD/train/48/48_r1.jpg  \n",
            "  inflating: DeepDRiD/train/48/48_r2.jpg  \n",
            "   creating: DeepDRiD/train/49/\n",
            "  inflating: DeepDRiD/train/49/49_l1.jpg  \n",
            "  inflating: DeepDRiD/train/49/49_l2.jpg  \n",
            "  inflating: DeepDRiD/train/49/49_r1.jpg  \n",
            "  inflating: DeepDRiD/train/49/49_r2.jpg  \n",
            "   creating: DeepDRiD/train/5/\n",
            "  inflating: DeepDRiD/train/5/5_l1.jpg  \n",
            "  inflating: DeepDRiD/train/5/5_l2.jpg  \n",
            "  inflating: DeepDRiD/train/5/5_r1.jpg  \n",
            "  inflating: DeepDRiD/train/5/5_r2.jpg  \n",
            "   creating: DeepDRiD/train/50/\n",
            "  inflating: DeepDRiD/train/50/50_l1.jpg  \n",
            "  inflating: DeepDRiD/train/50/50_l2.jpg  \n",
            "  inflating: DeepDRiD/train/50/50_r1.jpg  \n",
            "  inflating: DeepDRiD/train/50/50_r2.jpg  \n",
            "   creating: DeepDRiD/train/51/\n",
            "  inflating: DeepDRiD/train/51/51_l1.jpg  \n",
            "  inflating: DeepDRiD/train/51/51_l2.jpg  \n",
            "  inflating: DeepDRiD/train/51/51_r1.jpg  \n",
            "  inflating: DeepDRiD/train/51/51_r2.jpg  \n",
            "   creating: DeepDRiD/train/52/\n",
            "  inflating: DeepDRiD/train/52/52_l1.jpg  \n",
            "  inflating: DeepDRiD/train/52/52_l2.jpg  \n",
            "  inflating: DeepDRiD/train/52/52_r1.jpg  \n",
            "  inflating: DeepDRiD/train/52/52_r2.jpg  \n",
            "   creating: DeepDRiD/train/53/\n",
            "  inflating: DeepDRiD/train/53/53_l1.jpg  \n",
            "  inflating: DeepDRiD/train/53/53_l2.jpg  \n",
            "  inflating: DeepDRiD/train/53/53_r1.jpg  \n",
            "  inflating: DeepDRiD/train/53/53_r2.jpg  \n",
            "   creating: DeepDRiD/train/54/\n",
            "  inflating: DeepDRiD/train/54/54_l1.jpg  \n",
            "  inflating: DeepDRiD/train/54/54_l2.jpg  \n",
            "  inflating: DeepDRiD/train/54/54_r1.jpg  \n",
            "  inflating: DeepDRiD/train/54/54_r2.jpg  \n",
            "   creating: DeepDRiD/train/55/\n",
            "  inflating: DeepDRiD/train/55/55_l1.jpg  \n",
            "  inflating: DeepDRiD/train/55/55_l2.jpg  \n",
            "  inflating: DeepDRiD/train/55/55_r1.jpg  \n",
            "  inflating: DeepDRiD/train/55/55_r2.jpg  \n",
            "   creating: DeepDRiD/train/56/\n",
            "  inflating: DeepDRiD/train/56/56_l1.jpg  \n",
            "  inflating: DeepDRiD/train/56/56_l3.jpg  \n",
            "  inflating: DeepDRiD/train/56/56_r2.jpg  \n",
            "  inflating: DeepDRiD/train/56/56_r3.jpg  \n",
            "   creating: DeepDRiD/train/57/\n",
            "  inflating: DeepDRiD/train/57/57_l1.jpg  \n",
            "  inflating: DeepDRiD/train/57/57_l2.jpg  \n",
            "  inflating: DeepDRiD/train/57/57_r1.jpg  \n",
            "  inflating: DeepDRiD/train/57/57_r2.jpg  \n",
            "   creating: DeepDRiD/train/58/\n",
            "  inflating: DeepDRiD/train/58/58_l1.jpg  \n",
            "  inflating: DeepDRiD/train/58/58_l2.jpg  \n",
            "  inflating: DeepDRiD/train/58/58_r1.jpg  \n",
            "  inflating: DeepDRiD/train/58/58_r2.jpg  \n",
            "   creating: DeepDRiD/train/59/\n",
            "  inflating: DeepDRiD/train/59/59_l1.jpg  \n",
            "  inflating: DeepDRiD/train/59/59_l2.jpg  \n",
            "  inflating: DeepDRiD/train/59/59_r1.jpg  \n",
            "  inflating: DeepDRiD/train/59/59_r2.jpg  \n",
            "   creating: DeepDRiD/train/6/\n",
            "  inflating: DeepDRiD/train/6/6_l1.jpg  \n",
            "  inflating: DeepDRiD/train/6/6_l2.jpg  \n",
            "  inflating: DeepDRiD/train/6/6_r1.jpg  \n",
            "  inflating: DeepDRiD/train/6/6_r2.jpg  \n",
            "   creating: DeepDRiD/train/60/\n",
            "  inflating: DeepDRiD/train/60/60_l1.jpg  \n",
            "  inflating: DeepDRiD/train/60/60_l2.jpg  \n",
            "  inflating: DeepDRiD/train/60/60_r1.jpg  \n",
            "  inflating: DeepDRiD/train/60/60_r2.jpg  \n",
            "   creating: DeepDRiD/train/61/\n",
            "  inflating: DeepDRiD/train/61/61_l1.jpg  \n",
            "  inflating: DeepDRiD/train/61/61_l2.jpg  \n",
            "  inflating: DeepDRiD/train/61/61_r1.jpg  \n",
            "  inflating: DeepDRiD/train/61/61_r2.jpg  \n",
            "   creating: DeepDRiD/train/62/\n",
            "  inflating: DeepDRiD/train/62/62_l1.jpg  \n",
            "  inflating: DeepDRiD/train/62/62_l2.jpg  \n",
            "  inflating: DeepDRiD/train/62/62_r1.jpg  \n",
            "  inflating: DeepDRiD/train/62/62_r2.jpg  \n",
            "   creating: DeepDRiD/train/63/\n",
            "  inflating: DeepDRiD/train/63/63_l1.jpg  \n",
            "  inflating: DeepDRiD/train/63/63_l2.jpg  \n",
            "  inflating: DeepDRiD/train/63/63_r1.jpg  \n",
            "  inflating: DeepDRiD/train/63/63_r2.jpg  \n",
            "   creating: DeepDRiD/train/64/\n",
            "  inflating: DeepDRiD/train/64/64_l1.jpg  \n",
            "  inflating: DeepDRiD/train/64/64_l2.jpg  \n",
            "  inflating: DeepDRiD/train/64/64_r1.jpg  \n",
            "  inflating: DeepDRiD/train/64/64_r2.jpg  \n",
            "   creating: DeepDRiD/train/65/\n",
            "  inflating: DeepDRiD/train/65/65_l1.jpg  \n",
            "  inflating: DeepDRiD/train/65/65_l2.jpg  \n",
            "  inflating: DeepDRiD/train/65/65_r1.jpg  \n",
            "  inflating: DeepDRiD/train/65/65_r2.jpg  \n",
            "   creating: DeepDRiD/train/66/\n",
            "  inflating: DeepDRiD/train/66/66_l1.jpg  \n",
            "  inflating: DeepDRiD/train/66/66_l2.jpg  \n",
            "  inflating: DeepDRiD/train/66/66_r1.jpg  \n",
            "  inflating: DeepDRiD/train/66/66_r2.jpg  \n",
            "   creating: DeepDRiD/train/67/\n",
            "  inflating: DeepDRiD/train/67/67_l1.jpg  \n",
            "  inflating: DeepDRiD/train/67/67_l2.jpg  \n",
            "  inflating: DeepDRiD/train/67/67_r1.jpg  \n",
            "  inflating: DeepDRiD/train/67/67_r2.jpg  \n",
            "   creating: DeepDRiD/train/68/\n",
            "  inflating: DeepDRiD/train/68/68_l1.jpg  \n",
            "  inflating: DeepDRiD/train/68/68_l2.jpg  \n",
            "  inflating: DeepDRiD/train/68/68_r1.jpg  \n",
            "  inflating: DeepDRiD/train/68/68_r2.jpg  \n",
            "   creating: DeepDRiD/train/69/\n",
            "  inflating: DeepDRiD/train/69/69_l1.jpg  \n",
            "  inflating: DeepDRiD/train/69/69_l2.jpg  \n",
            "  inflating: DeepDRiD/train/69/69_r1.jpg  \n",
            "  inflating: DeepDRiD/train/69/69_r2.jpg  \n",
            "   creating: DeepDRiD/train/7/\n",
            "  inflating: DeepDRiD/train/7/7_l1.jpg  \n",
            "  inflating: DeepDRiD/train/7/7_l2.jpg  \n",
            "  inflating: DeepDRiD/train/7/7_r1.jpg  \n",
            "  inflating: DeepDRiD/train/7/7_r2.jpg  \n",
            "   creating: DeepDRiD/train/70/\n",
            "  inflating: DeepDRiD/train/70/70_l1.jpg  \n",
            "  inflating: DeepDRiD/train/70/70_l2.jpg  \n",
            "  inflating: DeepDRiD/train/70/70_r1.jpg  \n",
            "  inflating: DeepDRiD/train/70/70_r2.jpg  \n",
            "   creating: DeepDRiD/train/71/\n",
            "  inflating: DeepDRiD/train/71/71_l1.jpg  \n",
            "  inflating: DeepDRiD/train/71/71_l2.jpg  \n",
            "  inflating: DeepDRiD/train/71/71_r1.jpg  \n",
            "  inflating: DeepDRiD/train/71/71_r2.jpg  \n",
            "   creating: DeepDRiD/train/72/\n",
            "  inflating: DeepDRiD/train/72/72_l1.jpg  \n",
            "  inflating: DeepDRiD/train/72/72_l2.jpg  \n",
            "  inflating: DeepDRiD/train/72/72_r1.jpg  \n",
            "  inflating: DeepDRiD/train/72/72_r2.jpg  \n",
            "   creating: DeepDRiD/train/73/\n",
            "  inflating: DeepDRiD/train/73/73_l1.jpg  \n",
            "  inflating: DeepDRiD/train/73/73_l2.jpg  \n",
            "  inflating: DeepDRiD/train/73/73_r1.jpg  \n",
            "  inflating: DeepDRiD/train/73/73_r2.jpg  \n",
            "   creating: DeepDRiD/train/74/\n",
            "  inflating: DeepDRiD/train/74/74_l1.jpg  \n",
            "  inflating: DeepDRiD/train/74/74_l2.jpg  \n",
            "  inflating: DeepDRiD/train/74/74_r1.jpg  \n",
            "  inflating: DeepDRiD/train/74/74_r2.jpg  \n",
            "   creating: DeepDRiD/train/75/\n",
            "  inflating: DeepDRiD/train/75/75_l1.jpg  \n",
            "  inflating: DeepDRiD/train/75/75_l2.jpg  \n",
            "  inflating: DeepDRiD/train/75/75_r1.jpg  \n",
            "  inflating: DeepDRiD/train/75/75_r2.jpg  \n",
            "   creating: DeepDRiD/train/76/\n",
            "  inflating: DeepDRiD/train/76/76_l1.jpg  \n",
            "  inflating: DeepDRiD/train/76/76_l2.jpg  \n",
            "  inflating: DeepDRiD/train/76/76_r1.jpg  \n",
            "  inflating: DeepDRiD/train/76/76_r2.jpg  \n",
            "   creating: DeepDRiD/train/77/\n",
            "  inflating: DeepDRiD/train/77/77_l3.jpg  \n",
            "  inflating: DeepDRiD/train/77/77_l4.jpg  \n",
            "  inflating: DeepDRiD/train/77/77_r3.jpg  \n",
            "  inflating: DeepDRiD/train/77/77_r4.jpg  \n",
            "   creating: DeepDRiD/train/78/\n",
            "  inflating: DeepDRiD/train/78/78_l1.jpg  \n",
            "  inflating: DeepDRiD/train/78/78_l2.jpg  \n",
            "  inflating: DeepDRiD/train/78/78_r1.jpg  \n",
            "  inflating: DeepDRiD/train/78/78_r2.jpg  \n",
            "   creating: DeepDRiD/train/79/\n",
            "  inflating: DeepDRiD/train/79/79_l1.jpg  \n",
            "  inflating: DeepDRiD/train/79/79_l2.jpg  \n",
            "  inflating: DeepDRiD/train/79/79_r1.jpg  \n",
            "  inflating: DeepDRiD/train/79/79_r2.jpg  \n",
            "   creating: DeepDRiD/train/8/\n",
            "  inflating: DeepDRiD/train/8/8_l1.jpg  \n",
            "  inflating: DeepDRiD/train/8/8_l2.jpg  \n",
            "  inflating: DeepDRiD/train/8/8_r1.jpg  \n",
            "  inflating: DeepDRiD/train/8/8_r2.jpg  \n",
            "   creating: DeepDRiD/train/80/\n",
            "  inflating: DeepDRiD/train/80/80_l1.jpg  \n",
            "  inflating: DeepDRiD/train/80/80_l2.jpg  \n",
            "  inflating: DeepDRiD/train/80/80_r1.jpg  \n",
            "  inflating: DeepDRiD/train/80/80_r2.jpg  \n",
            "   creating: DeepDRiD/train/81/\n",
            "  inflating: DeepDRiD/train/81/81_l1.jpg  \n",
            "  inflating: DeepDRiD/train/81/81_l2.jpg  \n",
            "  inflating: DeepDRiD/train/81/81_r1.jpg  \n",
            "  inflating: DeepDRiD/train/81/81_r2.jpg  \n",
            "   creating: DeepDRiD/train/82/\n",
            "  inflating: DeepDRiD/train/82/82_l1.jpg  \n",
            "  inflating: DeepDRiD/train/82/82_l2.jpg  \n",
            "  inflating: DeepDRiD/train/82/82_r1.jpg  \n",
            "  inflating: DeepDRiD/train/82/82_r2.jpg  \n",
            "   creating: DeepDRiD/train/83/\n",
            "  inflating: DeepDRiD/train/83/83_l1.jpg  \n",
            "  inflating: DeepDRiD/train/83/83_l2.jpg  \n",
            "  inflating: DeepDRiD/train/83/83_r1.jpg  \n",
            "  inflating: DeepDRiD/train/83/83_r2.jpg  \n",
            "   creating: DeepDRiD/train/84/\n",
            "  inflating: DeepDRiD/train/84/84_l1.jpg  \n",
            "  inflating: DeepDRiD/train/84/84_l2.jpg  \n",
            "  inflating: DeepDRiD/train/84/84_r1.jpg  \n",
            "  inflating: DeepDRiD/train/84/84_r2.jpg  \n",
            "   creating: DeepDRiD/train/85/\n",
            "  inflating: DeepDRiD/train/85/85_l1.jpg  \n",
            "  inflating: DeepDRiD/train/85/85_l2.jpg  \n",
            "  inflating: DeepDRiD/train/85/85_r1.jpg  \n",
            "  inflating: DeepDRiD/train/85/85_r2.jpg  \n",
            "   creating: DeepDRiD/train/86/\n",
            "  inflating: DeepDRiD/train/86/86_l1.jpg  \n",
            "  inflating: DeepDRiD/train/86/86_l2.jpg  \n",
            "  inflating: DeepDRiD/train/86/86_r1.jpg  \n",
            "  inflating: DeepDRiD/train/86/86_r2.jpg  \n",
            "   creating: DeepDRiD/train/87/\n",
            "  inflating: DeepDRiD/train/87/87_l1.jpg  \n",
            "  inflating: DeepDRiD/train/87/87_l2.jpg  \n",
            "  inflating: DeepDRiD/train/87/87_r1.jpg  \n",
            "  inflating: DeepDRiD/train/87/87_r2.jpg  \n",
            "   creating: DeepDRiD/train/88/\n",
            "  inflating: DeepDRiD/train/88/88_l1.jpg  \n",
            "  inflating: DeepDRiD/train/88/88_l2.jpg  \n",
            "  inflating: DeepDRiD/train/88/88_r1.jpg  \n",
            "  inflating: DeepDRiD/train/88/88_r2.jpg  \n",
            "   creating: DeepDRiD/train/89/\n",
            "  inflating: DeepDRiD/train/89/89_l1.jpg  \n",
            "  inflating: DeepDRiD/train/89/89_l2.jpg  \n",
            "  inflating: DeepDRiD/train/89/89_r1.jpg  \n",
            "  inflating: DeepDRiD/train/89/89_r2.jpg  \n",
            "   creating: DeepDRiD/train/9/\n",
            "  inflating: DeepDRiD/train/9/9_l1.jpg  \n",
            "  inflating: DeepDRiD/train/9/9_l2.jpg  \n",
            "  inflating: DeepDRiD/train/9/9_r1.jpg  \n",
            "  inflating: DeepDRiD/train/9/9_r2.jpg  \n",
            "   creating: DeepDRiD/train/90/\n",
            "  inflating: DeepDRiD/train/90/90_l1.jpg  \n",
            "  inflating: DeepDRiD/train/90/90_l2.jpg  \n",
            "  inflating: DeepDRiD/train/90/90_r1.jpg  \n",
            "  inflating: DeepDRiD/train/90/90_r2.jpg  \n",
            "   creating: DeepDRiD/train/91/\n",
            "  inflating: DeepDRiD/train/91/91_l1.jpg  \n",
            "  inflating: DeepDRiD/train/91/91_l2.jpg  \n",
            "  inflating: DeepDRiD/train/91/91_r1.jpg  \n",
            "  inflating: DeepDRiD/train/91/91_r2.jpg  \n",
            "   creating: DeepDRiD/train/92/\n",
            "  inflating: DeepDRiD/train/92/92_l1.jpg  \n",
            "  inflating: DeepDRiD/train/92/92_l2.jpg  \n",
            "  inflating: DeepDRiD/train/92/92_r1.jpg  \n",
            "  inflating: DeepDRiD/train/92/92_r2.jpg  \n",
            "   creating: DeepDRiD/train/93/\n",
            "  inflating: DeepDRiD/train/93/93_l1.jpg  \n",
            "  inflating: DeepDRiD/train/93/93_l2.jpg  \n",
            "  inflating: DeepDRiD/train/93/93_r1.jpg  \n",
            "  inflating: DeepDRiD/train/93/93_r2.jpg  \n",
            "   creating: DeepDRiD/train/94/\n",
            "  inflating: DeepDRiD/train/94/94_l1.jpg  \n",
            "  inflating: DeepDRiD/train/94/94_l2.jpg  \n",
            "  inflating: DeepDRiD/train/94/94_r1.jpg  \n",
            "  inflating: DeepDRiD/train/94/94_r2.jpg  \n",
            "   creating: DeepDRiD/train/95/\n",
            "  inflating: DeepDRiD/train/95/95_l1.jpg  \n",
            "  inflating: DeepDRiD/train/95/95_l2.jpg  \n",
            "  inflating: DeepDRiD/train/95/95_r1.jpg  \n",
            "  inflating: DeepDRiD/train/95/95_r2.jpg  \n",
            "   creating: DeepDRiD/train/96/\n",
            "  inflating: DeepDRiD/train/96/96_l1.jpg  \n",
            "  inflating: DeepDRiD/train/96/96_l2.jpg  \n",
            "  inflating: DeepDRiD/train/96/96_r1.jpg  \n",
            "  inflating: DeepDRiD/train/96/96_r2.jpg  \n",
            "   creating: DeepDRiD/train/97/\n",
            "  inflating: DeepDRiD/train/97/97_l1.jpg  \n",
            "  inflating: DeepDRiD/train/97/97_l2.jpg  \n",
            "  inflating: DeepDRiD/train/97/97_r1.jpg  \n",
            "  inflating: DeepDRiD/train/97/97_r2.jpg  \n",
            "   creating: DeepDRiD/train/98/\n",
            "  inflating: DeepDRiD/train/98/98_l1.jpg  \n",
            "  inflating: DeepDRiD/train/98/98_l2.jpg  \n",
            "  inflating: DeepDRiD/train/98/98_r1.jpg  \n",
            "  inflating: DeepDRiD/train/98/98_r2.jpg  \n",
            "   creating: DeepDRiD/train/99/\n",
            "  inflating: DeepDRiD/train/99/99_l1.jpg  \n",
            "  inflating: DeepDRiD/train/99/99_l2.jpg  \n",
            "  inflating: DeepDRiD/train/99/99_r1.jpg  \n",
            "  inflating: DeepDRiD/train/99/99_r2.jpg  \n",
            "  inflating: DeepDRiD/train.csv      \n",
            "   creating: DeepDRiD/val/\n",
            "   creating: DeepDRiD/val/265/\n",
            "  inflating: DeepDRiD/val/265/265_l1.jpg  \n",
            "  inflating: DeepDRiD/val/265/265_l2.jpg  \n",
            "  inflating: DeepDRiD/val/265/265_r1.jpg  \n",
            "  inflating: DeepDRiD/val/265/265_r2.jpg  \n",
            "   creating: DeepDRiD/val/267/\n",
            "  inflating: DeepDRiD/val/267/267_l1.jpg  \n",
            "  inflating: DeepDRiD/val/267/267_l2.jpg  \n",
            "  inflating: DeepDRiD/val/267/267_r1.jpg  \n",
            "  inflating: DeepDRiD/val/267/267_r2.jpg  \n",
            "   creating: DeepDRiD/val/277/\n",
            "  inflating: DeepDRiD/val/277/277_l1.jpg  \n",
            "  inflating: DeepDRiD/val/277/277_l2.jpg  \n",
            "  inflating: DeepDRiD/val/277/277_r1.jpg  \n",
            "  inflating: DeepDRiD/val/277/277_r2.jpg  \n",
            "   creating: DeepDRiD/val/294/\n",
            "  inflating: DeepDRiD/val/294/294_l1.jpg  \n",
            "  inflating: DeepDRiD/val/294/294_l2.jpg  \n",
            "  inflating: DeepDRiD/val/294/294_r1.jpg  \n",
            "  inflating: DeepDRiD/val/294/294_r2.jpg  \n",
            "   creating: DeepDRiD/val/296/\n",
            "  inflating: DeepDRiD/val/296/296_l1.jpg  \n",
            "  inflating: DeepDRiD/val/296/296_l2.jpg  \n",
            "  inflating: DeepDRiD/val/296/296_r1.jpg  \n",
            "  inflating: DeepDRiD/val/296/296_r2.jpg  \n",
            "   creating: DeepDRiD/val/298/\n",
            "  inflating: DeepDRiD/val/298/298_l1.jpg  \n",
            "  inflating: DeepDRiD/val/298/298_l2.jpg  \n",
            "  inflating: DeepDRiD/val/298/298_r1.jpg  \n",
            "  inflating: DeepDRiD/val/298/298_r2.jpg  \n",
            "   creating: DeepDRiD/val/299/\n",
            "  inflating: DeepDRiD/val/299/299_l1.jpg  \n",
            "  inflating: DeepDRiD/val/299/299_l2.jpg  \n",
            "  inflating: DeepDRiD/val/299/299_r1.jpg  \n",
            "  inflating: DeepDRiD/val/299/299_r2.jpg  \n",
            "   creating: DeepDRiD/val/300/\n",
            "  inflating: DeepDRiD/val/300/300_l1.jpg  \n",
            "  inflating: DeepDRiD/val/300/300_l2.jpg  \n",
            "  inflating: DeepDRiD/val/300/300_r1.jpg  \n",
            "  inflating: DeepDRiD/val/300/300_r2.jpg  \n",
            "   creating: DeepDRiD/val/301/\n",
            "  inflating: DeepDRiD/val/301/301_l1.jpg  \n",
            "  inflating: DeepDRiD/val/301/301_l2.jpg  \n",
            "  inflating: DeepDRiD/val/301/301_r1.jpg  \n",
            "  inflating: DeepDRiD/val/301/301_r2.jpg  \n",
            "   creating: DeepDRiD/val/303/\n",
            "  inflating: DeepDRiD/val/303/303_l1.jpg  \n",
            "  inflating: DeepDRiD/val/303/303_l2.jpg  \n",
            "  inflating: DeepDRiD/val/303/303_r1.jpg  \n",
            "  inflating: DeepDRiD/val/303/303_r2.jpg  \n",
            "   creating: DeepDRiD/val/304/\n",
            "  inflating: DeepDRiD/val/304/304_l1.jpg  \n",
            "  inflating: DeepDRiD/val/304/304_l2.jpg  \n",
            "  inflating: DeepDRiD/val/304/304_r1.jpg  \n",
            "  inflating: DeepDRiD/val/304/304_r2.jpg  \n",
            "   creating: DeepDRiD/val/305/\n",
            "  inflating: DeepDRiD/val/305/305_l1.jpg  \n",
            "  inflating: DeepDRiD/val/305/305_l2.jpg  \n",
            "  inflating: DeepDRiD/val/305/305_r1.jpg  \n",
            "  inflating: DeepDRiD/val/305/305_r2.jpg  \n",
            "   creating: DeepDRiD/val/306/\n",
            "  inflating: DeepDRiD/val/306/306_l1.jpg  \n",
            "  inflating: DeepDRiD/val/306/306_l2.jpg  \n",
            "  inflating: DeepDRiD/val/306/306_r1.jpg  \n",
            "  inflating: DeepDRiD/val/306/306_r2.jpg  \n",
            "   creating: DeepDRiD/val/307/\n",
            "  inflating: DeepDRiD/val/307/307_l1.jpg  \n",
            "  inflating: DeepDRiD/val/307/307_l2.jpg  \n",
            "  inflating: DeepDRiD/val/307/307_r1.jpg  \n",
            "  inflating: DeepDRiD/val/307/307_r2.jpg  \n",
            "   creating: DeepDRiD/val/309/\n",
            "  inflating: DeepDRiD/val/309/309_l1.jpg  \n",
            "  inflating: DeepDRiD/val/309/309_l2.jpg  \n",
            "  inflating: DeepDRiD/val/309/309_r1.jpg  \n",
            "  inflating: DeepDRiD/val/309/309_r2.jpg  \n",
            "   creating: DeepDRiD/val/310/\n",
            "  inflating: DeepDRiD/val/310/310_l1.jpg  \n",
            "  inflating: DeepDRiD/val/310/310_l2.jpg  \n",
            "  inflating: DeepDRiD/val/310/310_r1.jpg  \n",
            "  inflating: DeepDRiD/val/310/310_r2.jpg  \n",
            "   creating: DeepDRiD/val/311/\n",
            "  inflating: DeepDRiD/val/311/311_l1.jpg  \n",
            "  inflating: DeepDRiD/val/311/311_l2.jpg  \n",
            "  inflating: DeepDRiD/val/311/311_r1.jpg  \n",
            "  inflating: DeepDRiD/val/311/311_r2.jpg  \n",
            "   creating: DeepDRiD/val/314/\n",
            "  inflating: DeepDRiD/val/314/314_l1.jpg  \n",
            "  inflating: DeepDRiD/val/314/314_l2.jpg  \n",
            "  inflating: DeepDRiD/val/314/314_r1.jpg  \n",
            "  inflating: DeepDRiD/val/314/314_r2.jpg  \n",
            "   creating: DeepDRiD/val/315/\n",
            "  inflating: DeepDRiD/val/315/315_l1.jpg  \n",
            "  inflating: DeepDRiD/val/315/315_l2.jpg  \n",
            "  inflating: DeepDRiD/val/315/315_r1.jpg  \n",
            "  inflating: DeepDRiD/val/315/315_r2.jpg  \n",
            "   creating: DeepDRiD/val/316/\n",
            "  inflating: DeepDRiD/val/316/316_l1.jpg  \n",
            "  inflating: DeepDRiD/val/316/316_l2.jpg  \n",
            "  inflating: DeepDRiD/val/316/316_r1.jpg  \n",
            "  inflating: DeepDRiD/val/316/316_r2.jpg  \n",
            "   creating: DeepDRiD/val/317/\n",
            "  inflating: DeepDRiD/val/317/317_l1.jpg  \n",
            "  inflating: DeepDRiD/val/317/317_l2.jpg  \n",
            "  inflating: DeepDRiD/val/317/317_r1.jpg  \n",
            "  inflating: DeepDRiD/val/317/317_r2.jpg  \n",
            "   creating: DeepDRiD/val/318/\n",
            "  inflating: DeepDRiD/val/318/318_l1.jpg  \n",
            "  inflating: DeepDRiD/val/318/318_l2.jpg  \n",
            "  inflating: DeepDRiD/val/318/318_r1.jpg  \n",
            "  inflating: DeepDRiD/val/318/318_r2.jpg  \n",
            "   creating: DeepDRiD/val/319/\n",
            "  inflating: DeepDRiD/val/319/319_l1.jpg  \n",
            "  inflating: DeepDRiD/val/319/319_l2.jpg  \n",
            "  inflating: DeepDRiD/val/319/319_r1.jpg  \n",
            "  inflating: DeepDRiD/val/319/319_r2.jpg  \n",
            "   creating: DeepDRiD/val/320/\n",
            "  inflating: DeepDRiD/val/320/320_l1.jpg  \n",
            "  inflating: DeepDRiD/val/320/320_l2.jpg  \n",
            "  inflating: DeepDRiD/val/320/320_r1.jpg  \n",
            "  inflating: DeepDRiD/val/320/320_r2.jpg  \n",
            "   creating: DeepDRiD/val/321/\n",
            "  inflating: DeepDRiD/val/321/321_l1.jpg  \n",
            "  inflating: DeepDRiD/val/321/321_l2.jpg  \n",
            "  inflating: DeepDRiD/val/321/321_r1.jpg  \n",
            "  inflating: DeepDRiD/val/321/321_r2.jpg  \n",
            "   creating: DeepDRiD/val/322/\n",
            "  inflating: DeepDRiD/val/322/322_l1.jpg  \n",
            "  inflating: DeepDRiD/val/322/322_l2.jpg  \n",
            "  inflating: DeepDRiD/val/322/322_r1.jpg  \n",
            "  inflating: DeepDRiD/val/322/322_r2.jpg  \n",
            "   creating: DeepDRiD/val/323/\n",
            "  inflating: DeepDRiD/val/323/323_l1.jpg  \n",
            "  inflating: DeepDRiD/val/323/323_l2.jpg  \n",
            "  inflating: DeepDRiD/val/323/323_r1.jpg  \n",
            "  inflating: DeepDRiD/val/323/323_r2.jpg  \n",
            "   creating: DeepDRiD/val/325/\n",
            "  inflating: DeepDRiD/val/325/325_l1.jpg  \n",
            "  inflating: DeepDRiD/val/325/325_l2.jpg  \n",
            "  inflating: DeepDRiD/val/325/325_r1.jpg  \n",
            "  inflating: DeepDRiD/val/325/325_r2.jpg  \n",
            "   creating: DeepDRiD/val/326/\n",
            "  inflating: DeepDRiD/val/326/326_l1.jpg  \n",
            "  inflating: DeepDRiD/val/326/326_l2.jpg  \n",
            "  inflating: DeepDRiD/val/326/326_r1.jpg  \n",
            "  inflating: DeepDRiD/val/326/326_r2.jpg  \n",
            "   creating: DeepDRiD/val/328/\n",
            "  inflating: DeepDRiD/val/328/328_l1.jpg  \n",
            "  inflating: DeepDRiD/val/328/328_l2.jpg  \n",
            "  inflating: DeepDRiD/val/328/328_r1.jpg  \n",
            "  inflating: DeepDRiD/val/328/328_r2.jpg  \n",
            "   creating: DeepDRiD/val/331/\n",
            "  inflating: DeepDRiD/val/331/331_l1.jpg  \n",
            "  inflating: DeepDRiD/val/331/331_l2.jpg  \n",
            "  inflating: DeepDRiD/val/331/331_r1.jpg  \n",
            "  inflating: DeepDRiD/val/331/331_r2.jpg  \n",
            "   creating: DeepDRiD/val/332/\n",
            "  inflating: DeepDRiD/val/332/332_l1.jpg  \n",
            "  inflating: DeepDRiD/val/332/332_l2.jpg  \n",
            "  inflating: DeepDRiD/val/332/332_r1.jpg  \n",
            "  inflating: DeepDRiD/val/332/332_r2.jpg  \n",
            "   creating: DeepDRiD/val/333/\n",
            "  inflating: DeepDRiD/val/333/333_l1.jpg  \n",
            "  inflating: DeepDRiD/val/333/333_l2.jpg  \n",
            "  inflating: DeepDRiD/val/333/333_r1.jpg  \n",
            "  inflating: DeepDRiD/val/333/333_r2.jpg  \n",
            "   creating: DeepDRiD/val/334/\n",
            "  inflating: DeepDRiD/val/334/334_l1.jpg  \n",
            "  inflating: DeepDRiD/val/334/334_l2.jpg  \n",
            "  inflating: DeepDRiD/val/334/334_r1.jpg  \n",
            "  inflating: DeepDRiD/val/334/334_r2.jpg  \n",
            "   creating: DeepDRiD/val/335/\n",
            "  inflating: DeepDRiD/val/335/335_l1.jpg  \n",
            "  inflating: DeepDRiD/val/335/335_l2.jpg  \n",
            "  inflating: DeepDRiD/val/335/335_r1.jpg  \n",
            "  inflating: DeepDRiD/val/335/335_r2.jpg  \n",
            "   creating: DeepDRiD/val/336/\n",
            "  inflating: DeepDRiD/val/336/336_l1.jpg  \n",
            "  inflating: DeepDRiD/val/336/336_l2.jpg  \n",
            "  inflating: DeepDRiD/val/336/336_r1.jpg  \n",
            "  inflating: DeepDRiD/val/336/336_r2.jpg  \n",
            "   creating: DeepDRiD/val/337/\n",
            "  inflating: DeepDRiD/val/337/337_l1.jpg  \n",
            "  inflating: DeepDRiD/val/337/337_l2.jpg  \n",
            "  inflating: DeepDRiD/val/337/337_r1.jpg  \n",
            "  inflating: DeepDRiD/val/337/337_r2.jpg  \n",
            "   creating: DeepDRiD/val/338/\n",
            "  inflating: DeepDRiD/val/338/338_l1.jpg  \n",
            "  inflating: DeepDRiD/val/338/338_l2.jpg  \n",
            "  inflating: DeepDRiD/val/338/338_r1.jpg  \n",
            "  inflating: DeepDRiD/val/338/338_r2.jpg  \n",
            "   creating: DeepDRiD/val/339/\n",
            "  inflating: DeepDRiD/val/339/339_l1.jpg  \n",
            "  inflating: DeepDRiD/val/339/339_l2.jpg  \n",
            "  inflating: DeepDRiD/val/339/339_r1.jpg  \n",
            "  inflating: DeepDRiD/val/339/339_r2.jpg  \n",
            "   creating: DeepDRiD/val/340/\n",
            "  inflating: DeepDRiD/val/340/340_l1.jpg  \n",
            "  inflating: DeepDRiD/val/340/340_l2.jpg  \n",
            "  inflating: DeepDRiD/val/340/340_r1.jpg  \n",
            "  inflating: DeepDRiD/val/340/340_r2.jpg  \n",
            "   creating: DeepDRiD/val/341/\n",
            "  inflating: DeepDRiD/val/341/341_l1.jpg  \n",
            "  inflating: DeepDRiD/val/341/341_l2.jpg  \n",
            "  inflating: DeepDRiD/val/341/341_r1.jpg  \n",
            "  inflating: DeepDRiD/val/341/341_r2.jpg  \n",
            "   creating: DeepDRiD/val/342/\n",
            "  inflating: DeepDRiD/val/342/342_l1.jpg  \n",
            "  inflating: DeepDRiD/val/342/342_l2.jpg  \n",
            "  inflating: DeepDRiD/val/342/342_r1.jpg  \n",
            "  inflating: DeepDRiD/val/342/342_r2.jpg  \n",
            "   creating: DeepDRiD/val/343/\n",
            "  inflating: DeepDRiD/val/343/343_l1.jpg  \n",
            "  inflating: DeepDRiD/val/343/343_l2.jpg  \n",
            "  inflating: DeepDRiD/val/343/343_r1.jpg  \n",
            "  inflating: DeepDRiD/val/343/343_r2.jpg  \n",
            "   creating: DeepDRiD/val/344/\n",
            "  inflating: DeepDRiD/val/344/344_l1.jpg  \n",
            "  inflating: DeepDRiD/val/344/344_l2.jpg  \n",
            "  inflating: DeepDRiD/val/344/344_r1.jpg  \n",
            "  inflating: DeepDRiD/val/344/344_r2.jpg  \n",
            "   creating: DeepDRiD/val/345/\n",
            "  inflating: DeepDRiD/val/345/345_l1.jpg  \n",
            "  inflating: DeepDRiD/val/345/345_l2.jpg  \n",
            "  inflating: DeepDRiD/val/345/345_r1.jpg  \n",
            "  inflating: DeepDRiD/val/345/345_r2.jpg  \n",
            "   creating: DeepDRiD/val/346/\n",
            "  inflating: DeepDRiD/val/346/346_l1.jpg  \n",
            "  inflating: DeepDRiD/val/346/346_l2.jpg  \n",
            "  inflating: DeepDRiD/val/346/346_r1.jpg  \n",
            "  inflating: DeepDRiD/val/346/346_r2.jpg  \n",
            "   creating: DeepDRiD/val/348/\n",
            "  inflating: DeepDRiD/val/348/348_l1.jpg  \n",
            "  inflating: DeepDRiD/val/348/348_l2.jpg  \n",
            "  inflating: DeepDRiD/val/348/348_r1.jpg  \n",
            "  inflating: DeepDRiD/val/348/348_r2.jpg  \n",
            "   creating: DeepDRiD/val/349/\n",
            "  inflating: DeepDRiD/val/349/349_l1.jpg  \n",
            "  inflating: DeepDRiD/val/349/349_l2.jpg  \n",
            "  inflating: DeepDRiD/val/349/349_r1.jpg  \n",
            "  inflating: DeepDRiD/val/349/349_r2.jpg  \n",
            "   creating: DeepDRiD/val/350/\n",
            "  inflating: DeepDRiD/val/350/350_l1.jpg  \n",
            "  inflating: DeepDRiD/val/350/350_l2.jpg  \n",
            "  inflating: DeepDRiD/val/350/350_r1.jpg  \n",
            "  inflating: DeepDRiD/val/350/350_r2.jpg  \n",
            "   creating: DeepDRiD/val/351/\n",
            "  inflating: DeepDRiD/val/351/351_l1.jpg  \n",
            "  inflating: DeepDRiD/val/351/351_l2.jpg  \n",
            "  inflating: DeepDRiD/val/351/351_r1.jpg  \n",
            "  inflating: DeepDRiD/val/351/351_r2.jpg  \n",
            "   creating: DeepDRiD/val/352/\n",
            "  inflating: DeepDRiD/val/352/352_l1.jpg  \n",
            "  inflating: DeepDRiD/val/352/352_l2.jpg  \n",
            "  inflating: DeepDRiD/val/352/352_r1.jpg  \n",
            "  inflating: DeepDRiD/val/352/352_r2.jpg  \n",
            "   creating: DeepDRiD/val/355/\n",
            "  inflating: DeepDRiD/val/355/355_l1.jpg  \n",
            "  inflating: DeepDRiD/val/355/355_l2.jpg  \n",
            "  inflating: DeepDRiD/val/355/355_r1.jpg  \n",
            "  inflating: DeepDRiD/val/355/355_r2.jpg  \n",
            "   creating: DeepDRiD/val/356/\n",
            "  inflating: DeepDRiD/val/356/356_l1.jpg  \n",
            "  inflating: DeepDRiD/val/356/356_l2.jpg  \n",
            "  inflating: DeepDRiD/val/356/356_r1.jpg  \n",
            "  inflating: DeepDRiD/val/356/356_r2.jpg  \n",
            "   creating: DeepDRiD/val/357/\n",
            "  inflating: DeepDRiD/val/357/357_l1.jpg  \n",
            "  inflating: DeepDRiD/val/357/357_l2.jpg  \n",
            "  inflating: DeepDRiD/val/357/357_r1.jpg  \n",
            "  inflating: DeepDRiD/val/357/357_r2.jpg  \n",
            "   creating: DeepDRiD/val/358/\n",
            "  inflating: DeepDRiD/val/358/358_l1.jpg  \n",
            "  inflating: DeepDRiD/val/358/358_l2.jpg  \n",
            "  inflating: DeepDRiD/val/358/358_r1.jpg  \n",
            "  inflating: DeepDRiD/val/358/358_r2.jpg  \n",
            "   creating: DeepDRiD/val/359/\n",
            "  inflating: DeepDRiD/val/359/359_l1.jpg  \n",
            "  inflating: DeepDRiD/val/359/359_l2.jpg  \n",
            "  inflating: DeepDRiD/val/359/359_r1.jpg  \n",
            "  inflating: DeepDRiD/val/359/359_r2.jpg  \n",
            "   creating: DeepDRiD/val/360/\n",
            "  inflating: DeepDRiD/val/360/360_l1.jpg  \n",
            "  inflating: DeepDRiD/val/360/360_l2.jpg  \n",
            "  inflating: DeepDRiD/val/360/360_r1.jpg  \n",
            "  inflating: DeepDRiD/val/360/360_r2.jpg  \n",
            "   creating: DeepDRiD/val/361/\n",
            "  inflating: DeepDRiD/val/361/361_l1.jpg  \n",
            "  inflating: DeepDRiD/val/361/361_l2.jpg  \n",
            "  inflating: DeepDRiD/val/361/361_r1.jpg  \n",
            "  inflating: DeepDRiD/val/361/361_r2.jpg  \n",
            "   creating: DeepDRiD/val/362/\n",
            "  inflating: DeepDRiD/val/362/362_l1.jpg  \n",
            "  inflating: DeepDRiD/val/362/362_l2.jpg  \n",
            "  inflating: DeepDRiD/val/362/362_r1.jpg  \n",
            "  inflating: DeepDRiD/val/362/362_r2.jpg  \n",
            "   creating: DeepDRiD/val/363/\n",
            "  inflating: DeepDRiD/val/363/363_l1.jpg  \n",
            "  inflating: DeepDRiD/val/363/363_l2.jpg  \n",
            "  inflating: DeepDRiD/val/363/363_r1.jpg  \n",
            "  inflating: DeepDRiD/val/363/363_r2.jpg  \n",
            "   creating: DeepDRiD/val/364/\n",
            "  inflating: DeepDRiD/val/364/364_l1.jpg  \n",
            "  inflating: DeepDRiD/val/364/364_l2.jpg  \n",
            "  inflating: DeepDRiD/val/364/364_r1.jpg  \n",
            "  inflating: DeepDRiD/val/364/364_r2.jpg  \n",
            "   creating: DeepDRiD/val/365/\n",
            "  inflating: DeepDRiD/val/365/365_l1.jpg  \n",
            "  inflating: DeepDRiD/val/365/365_l2.jpg  \n",
            "  inflating: DeepDRiD/val/365/365_r1.jpg  \n",
            "  inflating: DeepDRiD/val/365/365_r2.jpg  \n",
            "   creating: DeepDRiD/val/367/\n",
            "  inflating: DeepDRiD/val/367/367_l1.jpg  \n",
            "  inflating: DeepDRiD/val/367/367_l2.jpg  \n",
            "  inflating: DeepDRiD/val/367/367_r1.jpg  \n",
            "  inflating: DeepDRiD/val/367/367_r2.jpg  \n",
            "   creating: DeepDRiD/val/369/\n",
            "  inflating: DeepDRiD/val/369/369_l1.jpg  \n",
            "  inflating: DeepDRiD/val/369/369_l2.jpg  \n",
            "  inflating: DeepDRiD/val/369/369_r1.jpg  \n",
            "  inflating: DeepDRiD/val/369/369_r2.jpg  \n",
            "   creating: DeepDRiD/val/370/\n",
            "  inflating: DeepDRiD/val/370/370_l1.jpg  \n",
            "  inflating: DeepDRiD/val/370/370_l2.jpg  \n",
            "  inflating: DeepDRiD/val/370/370_r1.jpg  \n",
            "  inflating: DeepDRiD/val/370/370_r2.jpg  \n",
            "   creating: DeepDRiD/val/372/\n",
            "  inflating: DeepDRiD/val/372/372_l1.jpg  \n",
            "  inflating: DeepDRiD/val/372/372_l2.jpg  \n",
            "  inflating: DeepDRiD/val/372/372_r1.jpg  \n",
            "  inflating: DeepDRiD/val/372/372_r2.jpg  \n",
            "   creating: DeepDRiD/val/373/\n",
            "  inflating: DeepDRiD/val/373/373_l1.jpg  \n",
            "  inflating: DeepDRiD/val/373/373_l2.jpg  \n",
            "  inflating: DeepDRiD/val/373/373_r1.jpg  \n",
            "  inflating: DeepDRiD/val/373/373_r2.jpg  \n",
            "   creating: DeepDRiD/val/374/\n",
            "  inflating: DeepDRiD/val/374/374_l1.jpg  \n",
            "  inflating: DeepDRiD/val/374/374_l2.jpg  \n",
            "  inflating: DeepDRiD/val/374/374_r1.jpg  \n",
            "  inflating: DeepDRiD/val/374/374_r2.jpg  \n",
            "   creating: DeepDRiD/val/375/\n",
            "  inflating: DeepDRiD/val/375/375_l1.jpg  \n",
            "  inflating: DeepDRiD/val/375/375_l2.jpg  \n",
            "  inflating: DeepDRiD/val/375/375_r1.jpg  \n",
            "  inflating: DeepDRiD/val/375/375_r2.jpg  \n",
            "   creating: DeepDRiD/val/376/\n",
            "  inflating: DeepDRiD/val/376/376_l1.jpg  \n",
            "  inflating: DeepDRiD/val/376/376_l2.jpg  \n",
            "  inflating: DeepDRiD/val/376/376_r1.jpg  \n",
            "  inflating: DeepDRiD/val/376/376_r2.jpg  \n",
            "   creating: DeepDRiD/val/378/\n",
            "  inflating: DeepDRiD/val/378/378_l1.jpg  \n",
            "  inflating: DeepDRiD/val/378/378_l2.jpg  \n",
            "  inflating: DeepDRiD/val/378/378_r1.jpg  \n",
            "  inflating: DeepDRiD/val/378/378_r2.jpg  \n",
            "   creating: DeepDRiD/val/379/\n",
            "  inflating: DeepDRiD/val/379/379_l1.jpg  \n",
            "  inflating: DeepDRiD/val/379/379_l2.jpg  \n",
            "  inflating: DeepDRiD/val/379/379_r1.jpg  \n",
            "  inflating: DeepDRiD/val/379/379_r2.jpg  \n",
            "   creating: DeepDRiD/val/380/\n",
            "  inflating: DeepDRiD/val/380/380_l1.jpg  \n",
            "  inflating: DeepDRiD/val/380/380_l2.jpg  \n",
            "  inflating: DeepDRiD/val/380/380_r1.jpg  \n",
            "  inflating: DeepDRiD/val/380/380_r2.jpg  \n",
            "   creating: DeepDRiD/val/381/\n",
            "  inflating: DeepDRiD/val/381/381_l1.jpg  \n",
            "  inflating: DeepDRiD/val/381/381_l2.jpg  \n",
            "  inflating: DeepDRiD/val/381/381_r1.jpg  \n",
            "  inflating: DeepDRiD/val/381/381_r2.jpg  \n",
            "   creating: DeepDRiD/val/382/\n",
            "  inflating: DeepDRiD/val/382/382_l1.jpg  \n",
            "  inflating: DeepDRiD/val/382/382_l2.jpg  \n",
            "  inflating: DeepDRiD/val/382/382_r1.jpg  \n",
            "  inflating: DeepDRiD/val/382/382_r2.jpg  \n",
            "   creating: DeepDRiD/val/385/\n",
            "  inflating: DeepDRiD/val/385/385_l1.jpg  \n",
            "  inflating: DeepDRiD/val/385/385_l2.jpg  \n",
            "  inflating: DeepDRiD/val/385/385_r1.jpg  \n",
            "  inflating: DeepDRiD/val/385/385_r2.jpg  \n",
            "   creating: DeepDRiD/val/388/\n",
            "  inflating: DeepDRiD/val/388/388_l1.jpg  \n",
            "  inflating: DeepDRiD/val/388/388_l2.jpg  \n",
            "  inflating: DeepDRiD/val/388/388_r1.jpg  \n",
            "  inflating: DeepDRiD/val/388/388_r2.jpg  \n",
            "   creating: DeepDRiD/val/390/\n",
            "  inflating: DeepDRiD/val/390/390_l1.jpg  \n",
            "  inflating: DeepDRiD/val/390/390_l2.jpg  \n",
            "  inflating: DeepDRiD/val/390/390_r1.jpg  \n",
            "  inflating: DeepDRiD/val/390/390_r2.jpg  \n",
            "   creating: DeepDRiD/val/392/\n",
            "  inflating: DeepDRiD/val/392/392_l1.jpg  \n",
            "  inflating: DeepDRiD/val/392/392_l2.jpg  \n",
            "  inflating: DeepDRiD/val/392/392_r1.jpg  \n",
            "  inflating: DeepDRiD/val/392/392_r2.jpg  \n",
            "   creating: DeepDRiD/val/393/\n",
            "  inflating: DeepDRiD/val/393/393_l1.jpg  \n",
            "  inflating: DeepDRiD/val/393/393_l2.jpg  \n",
            "  inflating: DeepDRiD/val/393/393_r1.jpg  \n",
            "  inflating: DeepDRiD/val/393/393_r2.jpg  \n",
            "   creating: DeepDRiD/val/394/\n",
            "  inflating: DeepDRiD/val/394/394_l1.jpg  \n",
            "  inflating: DeepDRiD/val/394/394_l2.jpg  \n",
            "  inflating: DeepDRiD/val/394/394_r1.jpg  \n",
            "  inflating: DeepDRiD/val/394/394_r2.jpg  \n",
            "   creating: DeepDRiD/val/395/\n",
            "  inflating: DeepDRiD/val/395/395_l1.jpg  \n",
            "  inflating: DeepDRiD/val/395/395_l2.jpg  \n",
            "  inflating: DeepDRiD/val/395/395_r1.jpg  \n",
            "  inflating: DeepDRiD/val/395/395_r2.jpg  \n",
            "   creating: DeepDRiD/val/397/\n",
            "  inflating: DeepDRiD/val/397/397_l1.jpg  \n",
            "  inflating: DeepDRiD/val/397/397_l2.jpg  \n",
            "  inflating: DeepDRiD/val/397/397_r1.jpg  \n",
            "  inflating: DeepDRiD/val/397/397_r2.jpg  \n",
            "   creating: DeepDRiD/val/399/\n",
            "  inflating: DeepDRiD/val/399/399_l1.jpg  \n",
            "  inflating: DeepDRiD/val/399/399_l2.jpg  \n",
            "  inflating: DeepDRiD/val/399/399_r1.jpg  \n",
            "  inflating: DeepDRiD/val/399/399_r2.jpg  \n",
            "   creating: DeepDRiD/val/400/\n",
            "  inflating: DeepDRiD/val/400/400_l1.jpg  \n",
            "  inflating: DeepDRiD/val/400/400_l2.jpg  \n",
            "  inflating: DeepDRiD/val/400/400_r1.jpg  \n",
            "  inflating: DeepDRiD/val/400/400_r2.jpg  \n",
            "   creating: DeepDRiD/val/401/\n",
            "  inflating: DeepDRiD/val/401/401_l1.jpg  \n",
            "  inflating: DeepDRiD/val/401/401_l2.jpg  \n",
            "  inflating: DeepDRiD/val/401/401_r1.jpg  \n",
            "  inflating: DeepDRiD/val/401/401_r2.jpg  \n",
            "   creating: DeepDRiD/val/402/\n",
            "  inflating: DeepDRiD/val/402/402_l1.jpg  \n",
            "  inflating: DeepDRiD/val/402/402_l2.jpg  \n",
            "  inflating: DeepDRiD/val/402/402_r1.jpg  \n",
            "  inflating: DeepDRiD/val/402/402_r2.jpg  \n",
            "   creating: DeepDRiD/val/404/\n",
            "  inflating: DeepDRiD/val/404/404_l1.jpg  \n",
            "  inflating: DeepDRiD/val/404/404_l2.jpg  \n",
            "  inflating: DeepDRiD/val/404/404_r1.jpg  \n",
            "  inflating: DeepDRiD/val/404/404_r2.jpg  \n",
            "   creating: DeepDRiD/val/405/\n",
            "  inflating: DeepDRiD/val/405/405_l1.jpg  \n",
            "  inflating: DeepDRiD/val/405/405_l2.jpg  \n",
            "  inflating: DeepDRiD/val/405/405_r1.jpg  \n",
            "  inflating: DeepDRiD/val/405/405_r2.jpg  \n",
            "   creating: DeepDRiD/val/406/\n",
            "  inflating: DeepDRiD/val/406/406_l1.jpg  \n",
            "  inflating: DeepDRiD/val/406/406_l2.jpg  \n",
            "  inflating: DeepDRiD/val/406/406_r1.jpg  \n",
            "  inflating: DeepDRiD/val/406/406_r2.jpg  \n",
            "   creating: DeepDRiD/val/407/\n",
            "  inflating: DeepDRiD/val/407/407_l1.jpg  \n",
            "  inflating: DeepDRiD/val/407/407_l2.jpg  \n",
            "  inflating: DeepDRiD/val/407/407_r1.jpg  \n",
            "  inflating: DeepDRiD/val/407/407_r2.jpg  \n",
            "   creating: DeepDRiD/val/410/\n",
            "  inflating: DeepDRiD/val/410/410_l1.jpg  \n",
            "  inflating: DeepDRiD/val/410/410_l2.jpg  \n",
            "  inflating: DeepDRiD/val/410/410_r1.jpg  \n",
            "  inflating: DeepDRiD/val/410/410_r2.jpg  \n",
            "   creating: DeepDRiD/val/413/\n",
            "  inflating: DeepDRiD/val/413/413_l1.jpg  \n",
            "  inflating: DeepDRiD/val/413/413_l2.jpg  \n",
            "  inflating: DeepDRiD/val/413/413_r1.jpg  \n",
            "  inflating: DeepDRiD/val/413/413_r2.jpg  \n",
            "   creating: DeepDRiD/val/414/\n",
            "  inflating: DeepDRiD/val/414/414_l1.jpg  \n",
            "  inflating: DeepDRiD/val/414/414_l2.jpg  \n",
            "  inflating: DeepDRiD/val/414/414_r1.jpg  \n",
            "  inflating: DeepDRiD/val/414/414_r2.jpg  \n",
            "   creating: DeepDRiD/val/415/\n",
            "  inflating: DeepDRiD/val/415/415_l1.jpg  \n",
            "  inflating: DeepDRiD/val/415/415_l2.jpg  \n",
            "  inflating: DeepDRiD/val/415/415_r1.jpg  \n",
            "  inflating: DeepDRiD/val/415/415_r2.jpg  \n",
            "   creating: DeepDRiD/val/416/\n",
            "  inflating: DeepDRiD/val/416/416_l1.jpg  \n",
            "  inflating: DeepDRiD/val/416/416_l2.jpg  \n",
            "  inflating: DeepDRiD/val/416/416_r1.jpg  \n",
            "  inflating: DeepDRiD/val/416/416_r2.jpg  \n",
            "   creating: DeepDRiD/val/422/\n",
            "  inflating: DeepDRiD/val/422/422_l1.jpg  \n",
            "  inflating: DeepDRiD/val/422/422_l2.jpg  \n",
            "  inflating: DeepDRiD/val/422/422_r1.jpg  \n",
            "  inflating: DeepDRiD/val/422/422_r2.jpg  \n",
            "   creating: DeepDRiD/val/430/\n",
            "  inflating: DeepDRiD/val/430/430_l1.jpg  \n",
            "  inflating: DeepDRiD/val/430/430_l2.jpg  \n",
            "  inflating: DeepDRiD/val/430/430_r1.jpg  \n",
            "  inflating: DeepDRiD/val/430/430_r2.jpg  \n",
            "   creating: DeepDRiD/val/431/\n",
            "  inflating: DeepDRiD/val/431/431_l1.jpg  \n",
            "  inflating: DeepDRiD/val/431/431_l2.jpg  \n",
            "  inflating: DeepDRiD/val/431/431_r1.jpg  \n",
            "  inflating: DeepDRiD/val/431/431_r2.jpg  \n",
            "   creating: DeepDRiD/val/433/\n",
            "  inflating: DeepDRiD/val/433/433_l1.jpg  \n",
            "  inflating: DeepDRiD/val/433/433_l2.jpg  \n",
            "  inflating: DeepDRiD/val/433/433_r1.jpg  \n",
            "  inflating: DeepDRiD/val/433/433_r2.jpg  \n",
            "  inflating: DeepDRiD/val.csv        \n"
          ]
        }
      ],
      "source": [
        "!unzip DeepDRiD.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d-YN9xoRVLF"
      },
      "outputs": [],
      "source": [
        "# Hyper Parameters\n",
        "batch_size = 24\n",
        "num_classes = 5  # 5 DR levels\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEIG9Y5pRVLG"
      },
      "outputs": [],
      "source": [
        "class RetinopathyDataset(Dataset):\n",
        "    def __init__(self, ann_file, image_dir, transform=None, mode='single', test=False):\n",
        "        self.ann_file = ann_file\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.test = test\n",
        "        self.mode = mode\n",
        "\n",
        "        if self.mode == 'single':\n",
        "            self.data = self.load_data()\n",
        "        else:\n",
        "            self.data = self.load_data_dual()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.mode == 'single':\n",
        "            return self.get_item(index)\n",
        "        else:\n",
        "            return self.get_item_dual(index)\n",
        "\n",
        "    # 1. single image\n",
        "    def load_data(self):\n",
        "        df = pd.read_csv(self.ann_file)\n",
        "\n",
        "        data = []\n",
        "        for _, row in df.iterrows():\n",
        "            file_info = dict()\n",
        "            file_info['img_path'] = os.path.join(self.image_dir, row['img_path'])\n",
        "            if not self.test:\n",
        "                file_info['dr_level'] = int(row['patient_DR_Level'])\n",
        "            data.append(file_info)\n",
        "        return data\n",
        "\n",
        "    def get_item(self, index):\n",
        "        data = self.data[index]\n",
        "        img = Image.open(data['img_path']).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if not self.test:\n",
        "            label = torch.tensor(data['dr_level'], dtype=torch.int64)\n",
        "            return img, label\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    # 2. dual image\n",
        "    def load_data_dual(self):\n",
        "        df = pd.read_csv(self.ann_file)\n",
        "\n",
        "        df['prefix'] = df['image_id'].str.split('_').str[0]  # The patient id of each image\n",
        "        df['suffix'] = df['image_id'].str.split('_').str[1].str[0]  # The left or right eye\n",
        "        grouped = df.groupby(['prefix', 'suffix'])\n",
        "\n",
        "        data = []\n",
        "        for (prefix, suffix), group in grouped:\n",
        "            file_info = dict()\n",
        "            file_info['img_path1'] = os.path.join(self.image_dir, group.iloc[0]['img_path'])\n",
        "            file_info['img_path2'] = os.path.join(self.image_dir, group.iloc[1]['img_path'])\n",
        "            if not self.test:\n",
        "                file_info['dr_level'] = int(group.iloc[0]['patient_DR_Level'])\n",
        "            data.append(file_info)\n",
        "        return data\n",
        "\n",
        "    def get_item_dual(self, index):\n",
        "        data = self.data[index]\n",
        "        img1 = Image.open(data['img_path1']).convert('RGB')\n",
        "        img2 = Image.open(data['img_path2']).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "\n",
        "        if not self.test:\n",
        "            label = torch.tensor(data['dr_level'], dtype=torch.int64)\n",
        "            return [img1, img2], label\n",
        "        else:\n",
        "            return [img1, img2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7S0RC57RVLG"
      },
      "outputs": [],
      "source": [
        "class CutOut(object):\n",
        "    def __init__(self, mask_size, p=0.5):\n",
        "        self.mask_size = mask_size\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() > self.p:\n",
        "            return img\n",
        "\n",
        "        # Ensure the image is a tensor\n",
        "        if not isinstance(img, torch.Tensor):\n",
        "            raise TypeError('Input image must be a torch.Tensor')\n",
        "\n",
        "        # Get height and width of the image\n",
        "        h, w = img.shape[1], img.shape[2]\n",
        "        mask_size_half = self.mask_size // 2\n",
        "        offset = 1 if self.mask_size % 2 == 0 else 0\n",
        "\n",
        "        cx = np.random.randint(mask_size_half, w + offset - mask_size_half)\n",
        "        cy = np.random.randint(mask_size_half, h + offset - mask_size_half)\n",
        "\n",
        "        xmin, xmax = cx - mask_size_half, cx + mask_size_half + offset\n",
        "        ymin, ymax = cy - mask_size_half, cy + mask_size_half + offset\n",
        "        xmin, xmax = max(0, xmin), min(w, xmax)\n",
        "        ymin, ymax = max(0, ymin), min(h, ymax)\n",
        "\n",
        "        img[:, ymin:ymax, xmin:xmax] = 0\n",
        "        return img\n",
        "\n",
        "\n",
        "class SLORandomPad:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        pad_width = max(0, self.size[0] - img.width)\n",
        "        pad_height = max(0, self.size[1] - img.height)\n",
        "        pad_left = random.randint(0, pad_width)\n",
        "        pad_top = random.randint(0, pad_height)\n",
        "        pad_right = pad_width - pad_left\n",
        "        pad_bottom = pad_height - pad_top\n",
        "        return transforms.functional.pad(img, (pad_left, pad_top, pad_right, pad_bottom))\n",
        "\n",
        "\n",
        "class FundRandomRotate:\n",
        "    def __init__(self, prob, degree):\n",
        "        self.prob = prob\n",
        "        self.degree = degree\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.prob:\n",
        "            angle = random.uniform(-self.degree, self.degree)\n",
        "            return transforms.functional.rotate(img, angle)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pFUt6z8RVLG"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop((210, 210)),\n",
        "    SLORandomPad((224, 224)),\n",
        "    FundRandomRotate(prob=0.5, degree=30),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=25,\n",
        "                checkpoint_path='model.pth'):\n",
        "    best_model = model.state_dict()\n",
        "    best_epoch = None\n",
        "    best_val_kappa = -1.0  # Initialize the best kappa score\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f'\\nEpoch {epoch}/{num_epochs}')\n",
        "        running_loss = []\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        with tqdm(total=len(train_loader), desc=f'Training', unit=' batch', file=sys.stdout) as pbar:\n",
        "            for images, labels in train_loader:\n",
        "                if not isinstance(images, list):\n",
        "                    images = images.to(device)  # single image case\n",
        "                else:\n",
        "                    images = [x.to(device) for x in images]  # dual images case\n",
        "\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                preds = torch.argmax(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "                pbar.set_postfix({'lr': f'{optimizer.param_groups[0][\"lr\"]:.1e}', 'Loss': f'{loss.item():.4f}'})\n",
        "                pbar.update(1)\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        epoch_loss = sum(running_loss) / len(running_loss)\n",
        "\n",
        "        train_metrics = compute_metrics(all_preds, all_labels, per_class=True)\n",
        "        kappa, accuracy, precision, recall = train_metrics[:4]\n",
        "\n",
        "        print(f'[Train] Kappa: {kappa:.4f} Accuracy: {accuracy:.4f} '\n",
        "              f'Precision: {precision:.4f} Recall: {recall:.4f} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        if len(train_metrics) > 4:\n",
        "            precision_per_class, recall_per_class = train_metrics[4:]\n",
        "            for i, (precision, recall) in enumerate(zip(precision_per_class, recall_per_class)):\n",
        "                print(f'[Train] Class {i}: Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
        "\n",
        "        # Evaluation on the validation set at the end of each epoch\n",
        "        val_metrics = evaluate_model(model, val_loader, device)\n",
        "        val_kappa, val_accuracy, val_precision, val_recall = val_metrics[:4]\n",
        "        print(f'[Val] Kappa: {val_kappa:.4f} Accuracy: {val_accuracy:.4f} '\n",
        "              f'Precision: {val_precision:.4f} Recall: {val_recall:.4f}')\n",
        "\n",
        "        if val_kappa > best_val_kappa:\n",
        "            best_val_kappa = val_kappa\n",
        "            best_epoch = epoch\n",
        "            best_model = model.state_dict()\n",
        "            torch.save(best_model, checkpoint_path)\n",
        "\n",
        "    print(f'[Val] Best kappa: {best_val_kappa:.4f}, Epoch {best_epoch}')\n",
        "\n",
        "    return model, best_val_kappa\n",
        "\n",
        "def evaluate_model(model, test_loader, device, test_only=False, prediction_path='./test_predictions.csv'):\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_image_ids = []\n",
        "\n",
        "    with tqdm(total=len(test_loader), desc=f'Evaluating', unit=' batch', file=sys.stdout) as pbar:\n",
        "        for i, data in enumerate(test_loader):\n",
        "\n",
        "            if test_only:\n",
        "                images = data\n",
        "            else:\n",
        "                images, labels = data\n",
        "\n",
        "            if not isinstance(images, list):\n",
        "                images = images.to(device)  # single image case\n",
        "            else:\n",
        "                images = [x.to(device) for x in images]  # dual images case\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(images)\n",
        "                preds = torch.argmax(outputs, 1)\n",
        "\n",
        "            if not isinstance(images, list):\n",
        "                # single image case\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                image_ids = [\n",
        "                    os.path.basename(test_loader.dataset.data[idx]['img_path']) for idx in\n",
        "                    range(i * test_loader.batch_size, i * test_loader.batch_size + len(images))\n",
        "                ]\n",
        "                all_image_ids.extend(image_ids)\n",
        "                if not test_only:\n",
        "                    all_labels.extend(labels.numpy())\n",
        "            else:\n",
        "                # dual images case\n",
        "                for k in range(2):\n",
        "                    all_preds.extend(preds.cpu().numpy())\n",
        "                    image_ids = [\n",
        "                        os.path.basename(test_loader.dataset.data[idx][f'img_path{k + 1}']) for idx in\n",
        "                        range(i * test_loader.batch_size, i * test_loader.batch_size + len(images[k]))\n",
        "                    ]\n",
        "                    all_image_ids.extend(image_ids)\n",
        "                    if not test_only:\n",
        "                        all_labels.extend(labels.numpy())\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    # Save predictions to csv file for Kaggle online evaluation\n",
        "    if test_only:\n",
        "        df = pd.DataFrame({\n",
        "            'ID': all_image_ids,\n",
        "            'TARGET': all_preds\n",
        "        })\n",
        "        df.to_csv(prediction_path, index=False)\n",
        "        print(f'[Test] Save predictions to {os.path.abspath(prediction_path)}')\n",
        "    else:\n",
        "        metrics = compute_metrics(all_preds, all_labels)\n",
        "        return metrics\n",
        "\n",
        "\n",
        "def compute_metrics(preds, labels, per_class=False):\n",
        "    kappa = cohen_kappa_score(labels, preds, weights='quadratic')\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
        "\n",
        "    # Calculate and print precision and recall for each class\n",
        "    if per_class:\n",
        "        precision_per_class = precision_score(labels, preds, average=None, zero_division=0)\n",
        "        recall_per_class = recall_score(labels, preds, average=None, zero_division=0)\n",
        "        return kappa, accuracy, precision, recall, precision_per_class, recall_per_class\n",
        "\n",
        "    return kappa, accuracy, precision, recall\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG2HGCrHRVLG"
      },
      "outputs": [],
      "source": [
        "# Define model classes as given in the base code\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, backbone, num_classes=5, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = backbone\n",
        "        self.backbone.fc = nn.Identity()  # Remove original classification layer\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class VGG16Model(nn.Module):\n",
        "    def __init__(self, backbone, num_classes=5, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        # Use the VGG16 feature extractor\n",
        "        self.backbone = backbone\n",
        "        self.backbone.classifier = nn.Identity()  # Remove VGG16's original classifier\n",
        "\n",
        "        # Add adaptive pooling to ensure consistent output\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "\n",
        "        # Define the custom classifier\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 256),  # Output size of adaptive pooling is (512, 7, 7)\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone.features(x)  # Extract features\n",
        "        x = self.adaptive_pool(x)      # Apply adaptive pooling\n",
        "        x = torch.flatten(x, 1)        # Flatten the output\n",
        "        x = self.fc(x)                 # Pass through the custom classifier\n",
        "        return x\n",
        "\n",
        "class MyDualModel(nn.Module):\n",
        "    def __init__(self, backbone, num_classes=5, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        backbone.fc = nn.Identity()\n",
        "\n",
        "        self.backbone1 = copy.deepcopy(backbone)\n",
        "        self.backbone2 = copy.deepcopy(backbone)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512 * 2, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images):\n",
        "        image1, image2 = images\n",
        "\n",
        "        x1 = self.backbone1(image1)\n",
        "        x2 = self.backbone2(image2)\n",
        "\n",
        "        x = torch.cat((x1, x2), dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnyrr2B8I6SW"
      },
      "outputs": [],
      "source": [
        "def get_model(backbone_name, mode, num_classes=5):\n",
        "    if backbone_name == 'vgg16':\n",
        "        backbone = models.vgg16(pretrained=True)\n",
        "        if mode == 'single':\n",
        "            return VGG16Model(backbone, num_classes=num_classes)\n",
        "        elif mode == 'dual':\n",
        "            return MyDualModel(backbone, num_classes=num_classes)  # Modify if you need a dual-mode VGG16\n",
        "    else:\n",
        "        backbone = getattr(models, backbone_name)(pretrained=True)\n",
        "        if mode == 'single':\n",
        "            return MyModel(backbone, num_classes=num_classes)\n",
        "        elif mode == 'dual':\n",
        "            return MyDualModel(backbone, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IAGE0uLKJJKr",
        "outputId": "8c548865-27ff-4000-8186-8f685c6592b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 104MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet18 in single mode with Default augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 38/38 [00:23<00:00,  1.65 batch/s, lr=1.0e-03, Loss=1.0995]\n",
            "[Train] Kappa: 0.4783 Accuracy: 0.4250 Precision: 0.3882 Recall: 0.4250 Loss: 1.3730\n",
            "[Train] Class 0: Precision: 0.6599, Recall: 0.8083\n",
            "[Train] Class 1: Precision: 0.3250, Recall: 0.2708\n",
            "[Train] Class 2: Precision: 0.2802, Recall: 0.2417\n",
            "[Train] Class 3: Precision: 0.2921, Recall: 0.3833\n",
            "[Train] Class 4: Precision: 0.1081, Recall: 0.0333\n",
            "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.57 batch/s]\n",
            "[Val] Kappa: 0.5564 Accuracy: 0.5025 Precision: 0.3436 Recall: 0.5025\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.60 batch/s, lr=1.0e-03, Loss=1.1421]\n",
            "[Train] Kappa: 0.5727 Accuracy: 0.4658 Precision: 0.4256 Recall: 0.4658 Loss: 1.2589\n",
            "[Train] Class 0: Precision: 0.7483, Recall: 0.9083\n",
            "[Train] Class 1: Precision: 0.3398, Recall: 0.3625\n",
            "[Train] Class 2: Precision: 0.2278, Recall: 0.1708\n",
            "[Train] Class 3: Precision: 0.3288, Recall: 0.4042\n",
            "[Train] Class 4: Precision: 0.2188, Recall: 0.0583\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.97 batch/s]\n",
            "[Val] Kappa: 0.5127 Accuracy: 0.3850 Precision: 0.3237 Recall: 0.3850\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.60 batch/s, lr=1.0e-03, Loss=1.1539]\n",
            "[Train] Kappa: 0.5866 Accuracy: 0.4917 Precision: 0.4578 Recall: 0.4917 Loss: 1.1968\n",
            "[Train] Class 0: Precision: 0.7651, Recall: 0.9500\n",
            "[Train] Class 1: Precision: 0.3420, Recall: 0.4375\n",
            "[Train] Class 2: Precision: 0.3039, Recall: 0.2292\n",
            "[Train] Class 3: Precision: 0.3366, Recall: 0.2833\n",
            "[Train] Class 4: Precision: 0.3175, Recall: 0.1667\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.90 batch/s]\n",
            "[Val] Kappa: 0.6572 Accuracy: 0.4650 Precision: 0.3688 Recall: 0.4650\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.57 batch/s, lr=1.0e-03, Loss=0.9736]\n",
            "[Train] Kappa: 0.6640 Accuracy: 0.5100 Precision: 0.4715 Recall: 0.5100 Loss: 1.1522\n",
            "[Train] Class 0: Precision: 0.7616, Recall: 0.9583\n",
            "[Train] Class 1: Precision: 0.3766, Recall: 0.2417\n",
            "[Train] Class 2: Precision: 0.3027, Recall: 0.3292\n",
            "[Train] Class 3: Precision: 0.4110, Recall: 0.5000\n",
            "[Train] Class 4: Precision: 0.2500, Recall: 0.0833\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.87 batch/s]\n",
            "[Val] Kappa: 0.6494 Accuracy: 0.5075 Precision: 0.4962 Recall: 0.5075\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.57 batch/s, lr=1.0e-03, Loss=1.2027]\n",
            "[Train] Kappa: 0.6642 Accuracy: 0.5217 Precision: 0.4721 Recall: 0.5217 Loss: 1.1376\n",
            "[Train] Class 0: Precision: 0.7655, Recall: 0.9611\n",
            "[Train] Class 1: Precision: 0.4152, Recall: 0.4792\n",
            "[Train] Class 2: Precision: 0.2653, Recall: 0.1625\n",
            "[Train] Class 3: Precision: 0.4027, Recall: 0.4917\n",
            "[Train] Class 4: Precision: 0.2581, Recall: 0.0667\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.91 batch/s]\n",
            "[Val] Kappa: 0.5034 Accuracy: 0.4000 Precision: 0.4423 Recall: 0.4000\n",
            "[Val] Best kappa: 0.6572, Epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet18 in single mode with With CutOut augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 38/38 [00:12<00:00,  3.05 batch/s, lr=1.0e-03, Loss=1.2961]\n",
            "[Train] Kappa: 0.4375 Accuracy: 0.3992 Precision: 0.3767 Recall: 0.3992 Loss: 1.4132\n",
            "[Train] Class 0: Precision: 0.6499, Recall: 0.7889\n",
            "[Train] Class 1: Precision: 0.2329, Recall: 0.2125\n",
            "[Train] Class 2: Precision: 0.2600, Recall: 0.1625\n",
            "[Train] Class 3: Precision: 0.2656, Recall: 0.4250\n",
            "[Train] Class 4: Precision: 0.3000, Recall: 0.0250\n",
            "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.45 batch/s]\n",
            "[Val] Kappa: 0.3607 Accuracy: 0.4150 Precision: 0.4056 Recall: 0.4150\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.78 batch/s, lr=1.0e-03, Loss=1.2581]\n",
            "[Train] Kappa: 0.5054 Accuracy: 0.4333 Precision: 0.3709 Recall: 0.4333 Loss: 1.3162\n",
            "[Train] Class 0: Precision: 0.7152, Recall: 0.9000\n",
            "[Train] Class 1: Precision: 0.2412, Recall: 0.2000\n",
            "[Train] Class 2: Precision: 0.2511, Recall: 0.2417\n",
            "[Train] Class 3: Precision: 0.2894, Recall: 0.3750\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.42 batch/s]\n",
            "[Val] Kappa: 0.6248 Accuracy: 0.4850 Precision: 0.3195 Recall: 0.4850\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.67 batch/s, lr=1.0e-03, Loss=1.0391]\n",
            "[Train] Kappa: 0.5574 Accuracy: 0.4508 Precision: 0.4150 Recall: 0.4508 Loss: 1.2619\n",
            "[Train] Class 0: Precision: 0.7261, Recall: 0.9056\n",
            "[Train] Class 1: Precision: 0.3258, Recall: 0.1792\n",
            "[Train] Class 2: Precision: 0.2353, Recall: 0.2000\n",
            "[Train] Class 3: Precision: 0.2998, Recall: 0.5083\n",
            "[Train] Class 4: Precision: 0.2500, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.63 batch/s]\n",
            "[Val] Kappa: 0.6122 Accuracy: 0.3850 Precision: 0.3088 Recall: 0.3850\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.73 batch/s, lr=1.0e-03, Loss=1.3365]\n",
            "[Train] Kappa: 0.5304 Accuracy: 0.4750 Precision: 0.4463 Recall: 0.4750 Loss: 1.2360\n",
            "[Train] Class 0: Precision: 0.7246, Recall: 0.9500\n",
            "[Train] Class 1: Precision: 0.3074, Recall: 0.3125\n",
            "[Train] Class 2: Precision: 0.2994, Recall: 0.2208\n",
            "[Train] Class 3: Precision: 0.3233, Recall: 0.4042\n",
            "[Train] Class 4: Precision: 0.4286, Recall: 0.0250\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.74 batch/s]\n",
            "[Val] Kappa: 0.6424 Accuracy: 0.5150 Precision: 0.4027 Recall: 0.5150\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.75 batch/s, lr=1.0e-03, Loss=1.1193]\n",
            "[Train] Kappa: 0.5674 Accuracy: 0.4692 Precision: 0.4180 Recall: 0.4692 Loss: 1.2544\n",
            "[Train] Class 0: Precision: 0.7289, Recall: 0.9333\n",
            "[Train] Class 1: Precision: 0.2899, Recall: 0.2875\n",
            "[Train] Class 2: Precision: 0.2294, Recall: 0.1042\n",
            "[Train] Class 3: Precision: 0.3412, Recall: 0.5417\n",
            "[Train] Class 4: Precision: 0.2727, Recall: 0.0250\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.75 batch/s]\n",
            "[Val] Kappa: 0.4972 Accuracy: 0.4625 Precision: 0.3494 Recall: 0.4625\n",
            "[Val] Best kappa: 0.6424, Epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet18 in single mode with With SLO Padding augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 38/38 [00:11<00:00,  3.19 batch/s, lr=1.0e-03, Loss=1.1157]\n",
            "[Train] Kappa: 0.4167 Accuracy: 0.4275 Precision: 0.3916 Recall: 0.4275 Loss: 1.3477\n",
            "[Train] Class 0: Precision: 0.6892, Recall: 0.8500\n",
            "[Train] Class 1: Precision: 0.2558, Recall: 0.1833\n",
            "[Train] Class 2: Precision: 0.2872, Recall: 0.3375\n",
            "[Train] Class 3: Precision: 0.2770, Recall: 0.3208\n",
            "[Train] Class 4: Precision: 0.2083, Recall: 0.0417\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  6.01 batch/s]\n",
            "[Val] Kappa: 0.5655 Accuracy: 0.3325 Precision: 0.3647 Recall: 0.3325\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 38/38 [00:12<00:00,  3.10 batch/s, lr=1.0e-03, Loss=1.3773]\n",
            "[Train] Kappa: 0.5729 Accuracy: 0.4792 Precision: 0.4289 Recall: 0.4792 Loss: 1.2229\n",
            "[Train] Class 0: Precision: 0.7405, Recall: 0.9750\n",
            "[Train] Class 1: Precision: 0.3865, Recall: 0.3333\n",
            "[Train] Class 2: Precision: 0.2297, Recall: 0.2000\n",
            "[Train] Class 3: Precision: 0.3451, Recall: 0.3667\n",
            "[Train] Class 4: Precision: 0.1455, Recall: 0.0667\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.14 batch/s]\n",
            "[Val] Kappa: 0.6145 Accuracy: 0.5400 Precision: 0.4169 Recall: 0.5400\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 38/38 [00:13<00:00,  2.77 batch/s, lr=1.0e-03, Loss=1.0286]\n",
            "[Train] Kappa: 0.6442 Accuracy: 0.5133 Precision: 0.4759 Recall: 0.5133 Loss: 1.1559\n",
            "[Train] Class 0: Precision: 0.7761, Recall: 0.9722\n",
            "[Train] Class 1: Precision: 0.4118, Recall: 0.3500\n",
            "[Train] Class 2: Precision: 0.2810, Recall: 0.1792\n",
            "[Train] Class 3: Precision: 0.3562, Recall: 0.5417\n",
            "[Train] Class 4: Precision: 0.3333, Recall: 0.0750\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.79 batch/s]\n",
            "[Val] Kappa: 0.6271 Accuracy: 0.5325 Precision: 0.4498 Recall: 0.5325\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 38/38 [00:11<00:00,  3.18 batch/s, lr=1.0e-03, Loss=0.9447]\n",
            "[Train] Kappa: 0.6546 Accuracy: 0.5175 Precision: 0.4760 Recall: 0.5175 Loss: 1.1394\n",
            "[Train] Class 0: Precision: 0.7654, Recall: 0.9694\n",
            "[Train] Class 1: Precision: 0.4036, Recall: 0.3750\n",
            "[Train] Class 2: Precision: 0.2983, Recall: 0.2250\n",
            "[Train] Class 3: Precision: 0.3906, Recall: 0.4833\n",
            "[Train] Class 4: Precision: 0.2791, Recall: 0.1000\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.92 batch/s]\n",
            "[Val] Kappa: 0.6138 Accuracy: 0.4725 Precision: 0.2952 Recall: 0.4725\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 38/38 [00:11<00:00,  3.18 batch/s, lr=1.0e-03, Loss=1.2540]\n",
            "[Train] Kappa: 0.6455 Accuracy: 0.5092 Precision: 0.4702 Recall: 0.5092 Loss: 1.1389\n",
            "[Train] Class 0: Precision: 0.7765, Recall: 0.9556\n",
            "[Train] Class 1: Precision: 0.3974, Recall: 0.3792\n",
            "[Train] Class 2: Precision: 0.2886, Recall: 0.1792\n",
            "[Train] Class 3: Precision: 0.3576, Recall: 0.5125\n",
            "[Train] Class 4: Precision: 0.2857, Recall: 0.0833\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.26 batch/s]\n",
            "[Val] Kappa: 0.7012 Accuracy: 0.5925 Precision: 0.5321 Recall: 0.5925\n",
            "[Val] Best kappa: 0.7012, Epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet18 in single mode with With Fundus Rotation augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.62 batch/s, lr=1.0e-03, Loss=1.2634]\n",
            "[Train] Kappa: 0.4390 Accuracy: 0.4258 Precision: 0.3974 Recall: 0.4258 Loss: 1.3712\n",
            "[Train] Class 0: Precision: 0.6599, Recall: 0.8139\n",
            "[Train] Class 1: Precision: 0.2982, Recall: 0.2708\n",
            "[Train] Class 2: Precision: 0.2696, Recall: 0.2583\n",
            "[Train] Class 3: Precision: 0.3011, Recall: 0.3375\n",
            "[Train] Class 4: Precision: 0.2564, Recall: 0.0833\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.61 batch/s]\n",
            "[Val] Kappa: 0.2777 Accuracy: 0.4450 Precision: 0.2654 Recall: 0.4450\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.68 batch/s, lr=1.0e-03, Loss=1.3714]\n",
            "[Train] Kappa: 0.6162 Accuracy: 0.4983 Precision: 0.4569 Recall: 0.4983 Loss: 1.2073\n",
            "[Train] Class 0: Precision: 0.7164, Recall: 0.9333\n",
            "[Train] Class 1: Precision: 0.3889, Recall: 0.3792\n",
            "[Train] Class 2: Precision: 0.3048, Recall: 0.2375\n",
            "[Train] Class 3: Precision: 0.3810, Recall: 0.4333\n",
            "[Train] Class 4: Precision: 0.2703, Recall: 0.0833\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.49 batch/s]\n",
            "[Val] Kappa: 0.6896 Accuracy: 0.5150 Precision: 0.5077 Recall: 0.5150\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.72 batch/s, lr=1.0e-03, Loss=1.6127]\n",
            "[Train] Kappa: 0.6334 Accuracy: 0.5042 Precision: 0.4734 Recall: 0.5042 Loss: 1.2045\n",
            "[Train] Class 0: Precision: 0.7770, Recall: 0.9194\n",
            "[Train] Class 1: Precision: 0.3837, Recall: 0.3917\n",
            "[Train] Class 2: Precision: 0.2899, Recall: 0.2042\n",
            "[Train] Class 3: Precision: 0.3771, Recall: 0.4667\n",
            "[Train] Class 4: Precision: 0.3016, Recall: 0.1583\n",
            "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  4.29 batch/s]\n",
            "[Val] Kappa: 0.5917 Accuracy: 0.5450 Precision: 0.4652 Recall: 0.5450\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.59 batch/s, lr=1.0e-03, Loss=0.7614]\n",
            "[Train] Kappa: 0.6668 Accuracy: 0.5342 Precision: 0.4962 Recall: 0.5342 Loss: 1.1472\n",
            "[Train] Class 0: Precision: 0.7708, Recall: 0.9528\n",
            "[Train] Class 1: Precision: 0.4377, Recall: 0.4833\n",
            "[Train] Class 2: Precision: 0.2941, Recall: 0.1875\n",
            "[Train] Class 3: Precision: 0.4112, Recall: 0.5208\n",
            "[Train] Class 4: Precision: 0.3636, Recall: 0.1000\n",
            "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  4.29 batch/s]\n",
            "[Val] Kappa: 0.4429 Accuracy: 0.4825 Precision: 0.4090 Recall: 0.4825\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.74 batch/s, lr=1.0e-03, Loss=1.1554]\n",
            "[Train] Kappa: 0.6927 Accuracy: 0.5342 Precision: 0.4912 Recall: 0.5342 Loss: 1.1285\n",
            "[Train] Class 0: Precision: 0.7494, Recall: 0.9389\n",
            "[Train] Class 1: Precision: 0.4419, Recall: 0.4750\n",
            "[Train] Class 2: Precision: 0.3409, Recall: 0.1875\n",
            "[Train] Class 3: Precision: 0.4174, Recall: 0.5583\n",
            "[Train] Class 4: Precision: 0.2632, Recall: 0.0833\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.56 batch/s]\n",
            "[Val] Kappa: 0.7343 Accuracy: 0.5425 Precision: 0.4667 Recall: 0.5425\n",
            "[Val] Best kappa: 0.7343, Epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet18 in single mode with Combined augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.47 batch/s, lr=1.0e-03, Loss=1.7790]\n",
            "[Train] Kappa: 0.3938 Accuracy: 0.3975 Precision: 0.3513 Recall: 0.3975 Loss: 1.4055\n",
            "[Train] Class 0: Precision: 0.5743, Recall: 0.7944\n",
            "[Train] Class 1: Precision: 0.2374, Recall: 0.2542\n",
            "[Train] Class 2: Precision: 0.2915, Recall: 0.2708\n",
            "[Train] Class 3: Precision: 0.3073, Recall: 0.2625\n",
            "[Train] Class 4: Precision: 0.1176, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.66 batch/s]\n",
            "[Val] Kappa: 0.4264 Accuracy: 0.4125 Precision: 0.3430 Recall: 0.4125\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 38/38 [00:11<00:00,  3.43 batch/s, lr=1.0e-03, Loss=1.6788]\n",
            "[Train] Kappa: 0.5324 Accuracy: 0.4700 Precision: 0.4282 Recall: 0.4700 Loss: 1.2899\n",
            "[Train] Class 0: Precision: 0.7438, Recall: 0.9111\n",
            "[Train] Class 1: Precision: 0.3021, Recall: 0.2417\n",
            "[Train] Class 2: Precision: 0.3252, Recall: 0.2792\n",
            "[Train] Class 3: Precision: 0.3149, Recall: 0.4500\n",
            "[Train] Class 4: Precision: 0.1667, Recall: 0.0250\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.86 batch/s]\n",
            "[Val] Kappa: 0.0000 Accuracy: 0.2000 Precision: 0.0400 Recall: 0.2000\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 38/38 [00:11<00:00,  3.45 batch/s, lr=1.0e-03, Loss=1.1590]\n",
            "[Train] Kappa: 0.5069 Accuracy: 0.4408 Precision: 0.3910 Recall: 0.4408 Loss: 1.2827\n",
            "[Train] Class 0: Precision: 0.6976, Recall: 0.8972\n",
            "[Train] Class 1: Precision: 0.2885, Recall: 0.1875\n",
            "[Train] Class 2: Precision: 0.2542, Recall: 0.3125\n",
            "[Train] Class 3: Precision: 0.3158, Recall: 0.3500\n",
            "[Train] Class 4: Precision: 0.1000, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.84 batch/s]\n",
            "[Val] Kappa: 0.5274 Accuracy: 0.4725 Precision: 0.3494 Recall: 0.4725\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 38/38 [00:10<00:00,  3.47 batch/s, lr=1.0e-03, Loss=1.6124]\n",
            "[Train] Kappa: 0.5599 Accuracy: 0.4733 Precision: 0.4241 Recall: 0.4733 Loss: 1.2341\n",
            "[Train] Class 0: Precision: 0.7420, Recall: 0.9667\n",
            "[Train] Class 1: Precision: 0.3246, Recall: 0.2583\n",
            "[Train] Class 2: Precision: 0.2564, Recall: 0.2500\n",
            "[Train] Class 3: Precision: 0.3265, Recall: 0.3958\n",
            "[Train] Class 4: Precision: 0.2000, Recall: 0.0250\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.93 batch/s]\n",
            "[Val] Kappa: 0.6481 Accuracy: 0.5200 Precision: 0.3639 Recall: 0.5200\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 38/38 [00:11<00:00,  3.44 batch/s, lr=1.0e-03, Loss=1.6915]\n",
            "[Train] Kappa: 0.5721 Accuracy: 0.4783 Precision: 0.4273 Recall: 0.4783 Loss: 1.2044\n",
            "[Train] Class 0: Precision: 0.7286, Recall: 0.9694\n",
            "[Train] Class 1: Precision: 0.3655, Recall: 0.3792\n",
            "[Train] Class 2: Precision: 0.2909, Recall: 0.2000\n",
            "[Train] Class 3: Precision: 0.2926, Recall: 0.3292\n",
            "[Train] Class 4: Precision: 0.1892, Recall: 0.0583\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.86 batch/s]\n",
            "[Val] Kappa: 0.2941 Accuracy: 0.4600 Precision: 0.3819 Recall: 0.4600\n",
            "[Val] Best kappa: 0.6481, Epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet18 in dual mode with Default augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.78 batch/s, lr=1.0e-03, Loss=1.3856]\n",
            "[Train] Kappa: 0.3247 Accuracy: 0.3950 Precision: 0.3585 Recall: 0.3950 Loss: 1.4489\n",
            "[Train] Class 0: Precision: 0.5773, Recall: 0.7056\n",
            "[Train] Class 1: Precision: 0.2840, Recall: 0.3833\n",
            "[Train] Class 2: Precision: 0.2778, Recall: 0.1667\n",
            "[Train] Class 3: Precision: 0.3386, Recall: 0.3583\n",
            "[Train] Class 4: Precision: 0.0526, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.25 batch/s]\n",
            "[Val] Kappa: 0.3119 Accuracy: 0.4250 Precision: 0.3519 Recall: 0.4250\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.82 batch/s, lr=1.0e-03, Loss=1.2603]\n",
            "[Train] Kappa: 0.4750 Accuracy: 0.4500 Precision: 0.3988 Recall: 0.4500 Loss: 1.3549\n",
            "[Train] Class 0: Precision: 0.7476, Recall: 0.8556\n",
            "[Train] Class 1: Precision: 0.2587, Recall: 0.3083\n",
            "[Train] Class 2: Precision: 0.2787, Recall: 0.1417\n",
            "[Train] Class 3: Precision: 0.3351, Recall: 0.5167\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.30 batch/s]\n",
            "[Val] Kappa: 0.6087 Accuracy: 0.5050 Precision: 0.4291 Recall: 0.5050\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.80 batch/s, lr=1.0e-03, Loss=1.3727]\n",
            "[Train] Kappa: 0.5194 Accuracy: 0.4500 Precision: 0.4064 Recall: 0.4500 Loss: 1.2842\n",
            "[Train] Class 0: Precision: 0.7674, Recall: 0.9167\n",
            "[Train] Class 1: Precision: 0.3168, Recall: 0.2667\n",
            "[Train] Class 2: Precision: 0.2500, Recall: 0.1500\n",
            "[Train] Class 3: Precision: 0.2687, Recall: 0.4500\n",
            "[Train] Class 4: Precision: 0.0909, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.27 batch/s]\n",
            "[Val] Kappa: 0.4629 Accuracy: 0.4450 Precision: 0.3331 Recall: 0.4450\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.80 batch/s, lr=1.0e-03, Loss=1.3185]\n",
            "[Train] Kappa: 0.5588 Accuracy: 0.4850 Precision: 0.4486 Recall: 0.4850 Loss: 1.2460\n",
            "[Train] Class 0: Precision: 0.7558, Recall: 0.9111\n",
            "[Train] Class 1: Precision: 0.3333, Recall: 0.2917\n",
            "[Train] Class 2: Precision: 0.3040, Recall: 0.3167\n",
            "[Train] Class 3: Precision: 0.3611, Recall: 0.4333\n",
            "[Train] Class 4: Precision: 0.2222, Recall: 0.0333\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.20 batch/s]\n",
            "[Val] Kappa: 0.6035 Accuracy: 0.5000 Precision: 0.4796 Recall: 0.5000\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.81 batch/s, lr=1.0e-03, Loss=0.9890]\n",
            "[Train] Kappa: 0.5670 Accuracy: 0.4800 Precision: 0.4166 Recall: 0.4800 Loss: 1.2113\n",
            "[Train] Class 0: Precision: 0.7758, Recall: 0.9611\n",
            "[Train] Class 1: Precision: 0.3016, Recall: 0.3167\n",
            "[Train] Class 2: Precision: 0.2778, Recall: 0.2083\n",
            "[Train] Class 3: Precision: 0.3399, Recall: 0.4333\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.17 batch/s]\n",
            "[Val] Kappa: 0.6188 Accuracy: 0.5400 Precision: 0.3896 Recall: 0.5400\n",
            "[Val] Best kappa: 0.6188, Epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet18 in dual mode with With CutOut augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.78 batch/s, lr=1.0e-03, Loss=1.5238]\n",
            "[Train] Kappa: 0.3044 Accuracy: 0.3767 Precision: 0.3356 Recall: 0.3767 Loss: 1.4332\n",
            "[Train] Class 0: Precision: 0.5690, Recall: 0.7556\n",
            "[Train] Class 1: Precision: 0.2339, Recall: 0.2417\n",
            "[Train] Class 2: Precision: 0.2846, Recall: 0.2917\n",
            "[Train] Class 3: Precision: 0.2874, Recall: 0.2083\n",
            "[Train] Class 4: Precision: 0.0370, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.19 batch/s]\n",
            "[Val] Kappa: 0.4672 Accuracy: 0.4300 Precision: 0.3144 Recall: 0.4300\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.79 batch/s, lr=1.0e-03, Loss=1.1916]\n",
            "[Train] Kappa: 0.4516 Accuracy: 0.4133 Precision: 0.3757 Recall: 0.4133 Loss: 1.3964\n",
            "[Train] Class 0: Precision: 0.7050, Recall: 0.7833\n",
            "[Train] Class 1: Precision: 0.2653, Recall: 0.2167\n",
            "[Train] Class 2: Precision: 0.2595, Recall: 0.3417\n",
            "[Train] Class 3: Precision: 0.2963, Recall: 0.3333\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.17 batch/s]\n",
            "[Val] Kappa: 0.6413 Accuracy: 0.4850 Precision: 0.2850 Recall: 0.4850\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.80 batch/s, lr=1.0e-03, Loss=1.2994]\n",
            "[Train] Kappa: 0.5110 Accuracy: 0.4500 Precision: 0.4198 Recall: 0.4500 Loss: 1.2997\n",
            "[Train] Class 0: Precision: 0.7678, Recall: 0.9000\n",
            "[Train] Class 1: Precision: 0.2346, Recall: 0.3167\n",
            "[Train] Class 2: Precision: 0.2476, Recall: 0.2167\n",
            "[Train] Class 3: Precision: 0.3818, Recall: 0.3500\n",
            "[Train] Class 4: Precision: 0.1667, Recall: 0.0333\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.15 batch/s]\n",
            "[Val] Kappa: 0.6117 Accuracy: 0.4850 Precision: 0.4133 Recall: 0.4850\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.79 batch/s, lr=1.0e-03, Loss=0.9548]\n",
            "[Train] Kappa: 0.4981 Accuracy: 0.4700 Precision: 0.4286 Recall: 0.4700 Loss: 1.3132\n",
            "[Train] Class 0: Precision: 0.7080, Recall: 0.8889\n",
            "[Train] Class 1: Precision: 0.3077, Recall: 0.4333\n",
            "[Train] Class 2: Precision: 0.3380, Recall: 0.2000\n",
            "[Train] Class 3: Precision: 0.3729, Recall: 0.3667\n",
            "[Train] Class 4: Precision: 0.1250, Recall: 0.0333\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.18 batch/s]\n",
            "[Val] Kappa: 0.6640 Accuracy: 0.5100 Precision: 0.4461 Recall: 0.5100\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.78 batch/s, lr=1.0e-03, Loss=1.1438]\n",
            "[Train] Kappa: 0.6080 Accuracy: 0.4583 Precision: 0.4276 Recall: 0.4583 Loss: 1.2625\n",
            "[Train] Class 0: Precision: 0.7915, Recall: 0.9278\n",
            "[Train] Class 1: Precision: 0.3028, Recall: 0.2750\n",
            "[Train] Class 2: Precision: 0.2065, Recall: 0.1583\n",
            "[Train] Class 3: Precision: 0.2989, Recall: 0.4333\n",
            "[Train] Class 4: Precision: 0.2857, Recall: 0.0667\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.18 batch/s]\n",
            "[Val] Kappa: 0.5390 Accuracy: 0.5150 Precision: 0.3787 Recall: 0.5150\n",
            "[Val] Best kappa: 0.6640, Epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet18 in dual mode with With SLO Padding augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 19/19 [00:11<00:00,  1.59 batch/s, lr=1.0e-03, Loss=1.7952]\n",
            "[Train] Kappa: 0.3595 Accuracy: 0.3633 Precision: 0.3354 Recall: 0.3633 Loss: 1.4877\n",
            "[Train] Class 0: Precision: 0.6055, Recall: 0.7333\n",
            "[Train] Class 1: Precision: 0.2407, Recall: 0.2167\n",
            "[Train] Class 2: Precision: 0.2000, Recall: 0.2667\n",
            "[Train] Class 3: Precision: 0.2653, Recall: 0.2167\n",
            "[Train] Class 4: Precision: 0.1250, Recall: 0.0333\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.79 batch/s]\n",
            "[Val] Kappa: 0.0587 Accuracy: 0.3050 Precision: 0.1165 Recall: 0.3050\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 19/19 [00:11<00:00,  1.64 batch/s, lr=1.0e-03, Loss=1.2298]\n",
            "[Train] Kappa: 0.4197 Accuracy: 0.4267 Precision: 0.3920 Recall: 0.4267 Loss: 1.3435\n",
            "[Train] Class 0: Precision: 0.7500, Recall: 0.8667\n",
            "[Train] Class 1: Precision: 0.2955, Recall: 0.4333\n",
            "[Train] Class 2: Precision: 0.1957, Recall: 0.1500\n",
            "[Train] Class 3: Precision: 0.2437, Recall: 0.2417\n",
            "[Train] Class 4: Precision: 0.2000, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.35 batch/s]\n",
            "[Val] Kappa: 0.2299 Accuracy: 0.3750 Precision: 0.4949 Recall: 0.3750\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 19/19 [00:11<00:00,  1.59 batch/s, lr=1.0e-03, Loss=0.9284]\n",
            "[Train] Kappa: 0.5393 Accuracy: 0.4467 Precision: 0.4261 Recall: 0.4467 Loss: 1.2654\n",
            "[Train] Class 0: Precision: 0.7500, Recall: 0.8667\n",
            "[Train] Class 1: Precision: 0.3636, Recall: 0.2333\n",
            "[Train] Class 2: Precision: 0.2308, Recall: 0.2750\n",
            "[Train] Class 3: Precision: 0.3038, Recall: 0.4000\n",
            "[Train] Class 4: Precision: 0.2143, Recall: 0.0500\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.34 batch/s]\n",
            "[Val] Kappa: -0.0524 Accuracy: 0.2350 Precision: 0.3025 Recall: 0.2350\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 19/19 [00:11<00:00,  1.60 batch/s, lr=1.0e-03, Loss=1.1988]\n",
            "[Train] Kappa: 0.5165 Accuracy: 0.4583 Precision: 0.4007 Recall: 0.4583 Loss: 1.2271\n",
            "[Train] Class 0: Precision: 0.7613, Recall: 0.9389\n",
            "[Train] Class 1: Precision: 0.2891, Recall: 0.3083\n",
            "[Train] Class 2: Precision: 0.2518, Recall: 0.2917\n",
            "[Train] Class 3: Precision: 0.3208, Recall: 0.2833\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.17 batch/s]\n",
            "[Val] Kappa: 0.4782 Accuracy: 0.3100 Precision: 0.4413 Recall: 0.3100\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 19/19 [00:11<00:00,  1.58 batch/s, lr=1.0e-03, Loss=1.2925]\n",
            "[Train] Kappa: 0.5804 Accuracy: 0.4850 Precision: 0.4430 Recall: 0.4850 Loss: 1.2643\n",
            "[Train] Class 0: Precision: 0.7080, Recall: 0.8889\n",
            "[Train] Class 1: Precision: 0.3295, Recall: 0.2417\n",
            "[Train] Class 2: Precision: 0.4306, Recall: 0.2583\n",
            "[Train] Class 3: Precision: 0.3431, Recall: 0.5833\n",
            "[Train] Class 4: Precision: 0.1000, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.19 batch/s]\n",
            "[Val] Kappa: 0.6078 Accuracy: 0.4550 Precision: 0.4743 Recall: 0.4550\n",
            "[Val] Best kappa: 0.6078, Epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet18 in dual mode with With Fundus Rotation augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.77 batch/s, lr=1.0e-03, Loss=1.3232]\n",
            "[Train] Kappa: 0.3790 Accuracy: 0.3967 Precision: 0.3468 Recall: 0.3967 Loss: 1.3888\n",
            "[Train] Class 0: Precision: 0.6025, Recall: 0.8167\n",
            "[Train] Class 1: Precision: 0.2530, Recall: 0.1750\n",
            "[Train] Class 2: Precision: 0.2958, Recall: 0.1750\n",
            "[Train] Class 3: Precision: 0.2816, Recall: 0.4083\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.24 batch/s]\n",
            "[Val] Kappa: 0.6254 Accuracy: 0.4900 Precision: 0.2760 Recall: 0.4900\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.76 batch/s, lr=1.0e-03, Loss=1.6672]\n",
            "[Train] Kappa: 0.4976 Accuracy: 0.4483 Precision: 0.4513 Recall: 0.4483 Loss: 1.3295\n",
            "[Train] Class 0: Precision: 0.7413, Recall: 0.8278\n",
            "[Train] Class 1: Precision: 0.2798, Recall: 0.3917\n",
            "[Train] Class 2: Precision: 0.2889, Recall: 0.2167\n",
            "[Train] Class 3: Precision: 0.3259, Recall: 0.3667\n",
            "[Train] Class 4: Precision: 0.5000, Recall: 0.0500\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.19 batch/s]\n",
            "[Val] Kappa: 0.5839 Accuracy: 0.4750 Precision: 0.3097 Recall: 0.4750\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.77 batch/s, lr=1.0e-03, Loss=1.6343]\n",
            "[Train] Kappa: 0.5365 Accuracy: 0.4350 Precision: 0.3729 Recall: 0.4350 Loss: 1.3406\n",
            "[Train] Class 0: Precision: 0.6364, Recall: 0.9333\n",
            "[Train] Class 1: Precision: 0.2927, Recall: 0.2000\n",
            "[Train] Class 2: Precision: 0.2208, Recall: 0.1417\n",
            "[Train] Class 3: Precision: 0.2965, Recall: 0.4250\n",
            "[Train] Class 4: Precision: 0.2000, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.16 batch/s]\n",
            "[Val] Kappa: 0.6320 Accuracy: 0.4950 Precision: 0.3760 Recall: 0.4950\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.79 batch/s, lr=1.0e-03, Loss=1.3336]\n",
            "[Train] Kappa: 0.5237 Accuracy: 0.4650 Precision: 0.4946 Recall: 0.4650 Loss: 1.2963\n",
            "[Train] Class 0: Precision: 0.7217, Recall: 0.9222\n",
            "[Train] Class 1: Precision: 0.3252, Recall: 0.3333\n",
            "[Train] Class 2: Precision: 0.2373, Recall: 0.2333\n",
            "[Train] Class 3: Precision: 0.3280, Recall: 0.3417\n",
            "[Train] Class 4: Precision: 1.0000, Recall: 0.0667\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.22 batch/s]\n",
            "[Val] Kappa: 0.6452 Accuracy: 0.5000 Precision: 0.5871 Recall: 0.5000\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.77 batch/s, lr=1.0e-03, Loss=1.1046]\n",
            "[Train] Kappa: 0.5316 Accuracy: 0.4683 Precision: 0.4121 Recall: 0.4683 Loss: 1.2417\n",
            "[Train] Class 0: Precision: 0.7350, Recall: 0.9556\n",
            "[Train] Class 1: Precision: 0.2925, Recall: 0.3583\n",
            "[Train] Class 2: Precision: 0.2791, Recall: 0.2000\n",
            "[Train] Class 3: Precision: 0.3306, Recall: 0.3417\n",
            "[Train] Class 4: Precision: 0.1111, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.23 batch/s]\n",
            "[Val] Kappa: 0.6857 Accuracy: 0.5250 Precision: 0.5147 Recall: 0.5250\n",
            "[Val] Best kappa: 0.6857, Epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet18 in dual mode with Combined augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 19/19 [00:11<00:00,  1.72 batch/s, lr=1.0e-03, Loss=1.2841]\n",
            "[Train] Kappa: 0.3073 Accuracy: 0.3583 Precision: 0.3213 Recall: 0.3583 Loss: 1.4380\n",
            "[Train] Class 0: Precision: 0.6150, Recall: 0.7278\n",
            "[Train] Class 1: Precision: 0.2143, Recall: 0.2000\n",
            "[Train] Class 2: Precision: 0.2013, Recall: 0.2583\n",
            "[Train] Class 3: Precision: 0.2685, Recall: 0.2417\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.20 batch/s]\n",
            "[Val] Kappa: 0.5161 Accuracy: 0.4450 Precision: 0.2771 Recall: 0.4450\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 19/19 [00:11<00:00,  1.72 batch/s, lr=1.0e-03, Loss=1.2895]\n",
            "[Train] Kappa: 0.4658 Accuracy: 0.4400 Precision: 0.4166 Recall: 0.4400 Loss: 1.3509\n",
            "[Train] Class 0: Precision: 0.7130, Recall: 0.8833\n",
            "[Train] Class 1: Precision: 0.2328, Recall: 0.2250\n",
            "[Train] Class 2: Precision: 0.2969, Recall: 0.1583\n",
            "[Train] Class 3: Precision: 0.2963, Recall: 0.4667\n",
            "[Train] Class 4: Precision: 0.3750, Recall: 0.0500\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.91 batch/s]\n",
            "[Val] Kappa: 0.1992 Accuracy: 0.3100 Precision: 0.3922 Recall: 0.3100\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.76 batch/s, lr=1.0e-03, Loss=1.2945]\n",
            "[Train] Kappa: 0.4801 Accuracy: 0.4433 Precision: 0.4027 Recall: 0.4433 Loss: 1.3315\n",
            "[Train] Class 0: Precision: 0.7315, Recall: 0.8778\n",
            "[Train] Class 1: Precision: 0.3015, Recall: 0.3417\n",
            "[Train] Class 2: Precision: 0.2644, Recall: 0.1917\n",
            "[Train] Class 3: Precision: 0.2792, Recall: 0.3583\n",
            "[Train] Class 4: Precision: 0.1429, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.64 batch/s]\n",
            "[Val] Kappa: 0.4332 Accuracy: 0.4500 Precision: 0.3403 Recall: 0.4500\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.78 batch/s, lr=1.0e-03, Loss=1.2691]\n",
            "[Train] Kappa: 0.5071 Accuracy: 0.4717 Precision: 0.4174 Recall: 0.4717 Loss: 1.2439\n",
            "[Train] Class 0: Precision: 0.7867, Recall: 0.9222\n",
            "[Train] Class 1: Precision: 0.2963, Recall: 0.4667\n",
            "[Train] Class 2: Precision: 0.2632, Recall: 0.1667\n",
            "[Train] Class 3: Precision: 0.3475, Recall: 0.3417\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.41 batch/s]\n",
            "[Val] Kappa: 0.6392 Accuracy: 0.4650 Precision: 0.3016 Recall: 0.4650\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 19/19 [00:10<00:00,  1.81 batch/s, lr=1.0e-03, Loss=1.3911]\n",
            "[Train] Kappa: 0.6185 Accuracy: 0.4817 Precision: 0.4254 Recall: 0.4817 Loss: 1.2373\n",
            "[Train] Class 0: Precision: 0.7556, Recall: 0.9444\n",
            "[Train] Class 1: Precision: 0.3556, Recall: 0.1333\n",
            "[Train] Class 2: Precision: 0.3133, Recall: 0.4333\n",
            "[Train] Class 3: Precision: 0.3248, Recall: 0.4250\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.45 batch/s]\n",
            "[Val] Kappa: 0.5847 Accuracy: 0.4850 Precision: 0.5304 Recall: 0.4850\n",
            "[Val] Best kappa: 0.6392, Epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 93.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet34 in single mode with Default augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 38/38 [00:12<00:00,  2.93 batch/s, lr=1.0e-03, Loss=1.4693]\n",
            "[Train] Kappa: 0.4413 Accuracy: 0.4142 Precision: 0.3685 Recall: 0.4142 Loss: 1.3831\n",
            "[Train] Class 0: Precision: 0.6818, Recall: 0.8333\n",
            "[Train] Class 1: Precision: 0.2594, Recall: 0.2875\n",
            "[Train] Class 2: Precision: 0.2601, Recall: 0.2417\n",
            "[Train] Class 3: Precision: 0.2805, Recall: 0.2875\n",
            "[Train] Class 4: Precision: 0.0400, Recall: 0.0083\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.40 batch/s]\n",
            "[Val] Kappa: 0.5947 Accuracy: 0.4775 Precision: 0.4659 Recall: 0.4775\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 38/38 [00:12<00:00,  2.95 batch/s, lr=1.0e-03, Loss=1.5012]\n",
            "[Train] Kappa: 0.4922 Accuracy: 0.4350 Precision: 0.3885 Recall: 0.4350 Loss: 1.3193\n",
            "[Train] Class 0: Precision: 0.7332, Recall: 0.9083\n",
            "[Train] Class 1: Precision: 0.2339, Recall: 0.2125\n",
            "[Train] Class 2: Precision: 0.2222, Recall: 0.1667\n",
            "[Train] Class 3: Precision: 0.2957, Recall: 0.4250\n",
            "[Train] Class 4: Precision: 0.1818, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.32 batch/s]\n",
            "[Val] Kappa: 0.4575 Accuracy: 0.4700 Precision: 0.3776 Recall: 0.4700\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 38/38 [00:12<00:00,  2.97 batch/s, lr=1.0e-03, Loss=1.0108]\n",
            "[Train] Kappa: 0.5620 Accuracy: 0.4708 Precision: 0.4207 Recall: 0.4708 Loss: 1.2405\n",
            "[Train] Class 0: Precision: 0.7574, Recall: 0.9194\n",
            "[Train] Class 1: Precision: 0.3061, Recall: 0.1875\n",
            "[Train] Class 2: Precision: 0.2710, Recall: 0.2958\n",
            "[Train] Class 3: Precision: 0.3401, Recall: 0.4875\n",
            "[Train] Class 4: Precision: 0.1000, Recall: 0.0083\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.82 batch/s]\n",
            "[Val] Kappa: 0.4928 Accuracy: 0.4800 Precision: 0.4034 Recall: 0.4800\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 38/38 [00:12<00:00,  2.94 batch/s, lr=1.0e-03, Loss=1.3852]\n",
            "[Train] Kappa: 0.5378 Accuracy: 0.4675 Precision: 0.4317 Recall: 0.4675 Loss: 1.2470\n",
            "[Train] Class 0: Precision: 0.7609, Recall: 0.9194\n",
            "[Train] Class 1: Precision: 0.3084, Recall: 0.3958\n",
            "[Train] Class 2: Precision: 0.2500, Recall: 0.1542\n",
            "[Train] Class 3: Precision: 0.3196, Recall: 0.3875\n",
            "[Train] Class 4: Precision: 0.2778, Recall: 0.0417\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.68 batch/s]\n",
            "[Val] Kappa: 0.5925 Accuracy: 0.5125 Precision: 0.4447 Recall: 0.5125\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 38/38 [00:12<00:00,  2.96 batch/s, lr=1.0e-03, Loss=1.1077]\n",
            "[Train] Kappa: 0.5380 Accuracy: 0.4783 Precision: 0.4228 Recall: 0.4783 Loss: 1.2111\n",
            "[Train] Class 0: Precision: 0.7376, Recall: 0.9528\n",
            "[Train] Class 1: Precision: 0.3289, Recall: 0.4167\n",
            "[Train] Class 2: Precision: 0.2632, Recall: 0.1875\n",
            "[Train] Class 3: Precision: 0.3502, Recall: 0.3458\n",
            "[Train] Class 4: Precision: 0.1304, Recall: 0.0250\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.35 batch/s]\n",
            "[Val] Kappa: 0.5055 Accuracy: 0.4725 Precision: 0.3791 Recall: 0.4725\n",
            "[Val] Best kappa: 0.5947, Epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet34 in single mode with With CutOut augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 38/38 [00:12<00:00,  2.96 batch/s, lr=1.0e-03, Loss=1.3464]\n",
            "[Train] Kappa: 0.3079 Accuracy: 0.3492 Precision: 0.3395 Recall: 0.3492 Loss: 1.4664\n",
            "[Train] Class 0: Precision: 0.6415, Recall: 0.6361\n",
            "[Train] Class 1: Precision: 0.2103, Recall: 0.2208\n",
            "[Train] Class 2: Precision: 0.2137, Recall: 0.2083\n",
            "[Train] Class 3: Precision: 0.2757, Recall: 0.3458\n",
            "[Train] Class 4: Precision: 0.0714, Recall: 0.0333\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.38 batch/s]\n",
            "[Val] Kappa: 0.6648 Accuracy: 0.4850 Precision: 0.3266 Recall: 0.4850\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 38/38 [00:12<00:00,  2.95 batch/s, lr=1.0e-03, Loss=1.1851]\n",
            "[Train] Kappa: 0.4789 Accuracy: 0.4375 Precision: 0.3856 Recall: 0.4375 Loss: 1.3477\n",
            "[Train] Class 0: Precision: 0.6320, Recall: 0.9444\n",
            "[Train] Class 1: Precision: 0.2818, Recall: 0.2125\n",
            "[Train] Class 2: Precision: 0.2400, Recall: 0.1250\n",
            "[Train] Class 3: Precision: 0.2918, Recall: 0.4292\n",
            "[Train] Class 4: Precision: 0.3333, Recall: 0.0083\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.95 batch/s]\n",
            "[Val] Kappa: 0.6444 Accuracy: 0.4775 Precision: 0.3902 Recall: 0.4775\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 38/38 [00:12<00:00,  2.97 batch/s, lr=1.0e-03, Loss=1.0094]\n",
            "[Train] Kappa: 0.5443 Accuracy: 0.4442 Precision: 0.3820 Recall: 0.4442 Loss: 1.2580\n",
            "[Train] Class 0: Precision: 0.7263, Recall: 0.9583\n",
            "[Train] Class 1: Precision: 0.2471, Recall: 0.1792\n",
            "[Train] Class 2: Precision: 0.2260, Recall: 0.1958\n",
            "[Train] Class 3: Precision: 0.2972, Recall: 0.4000\n",
            "[Train] Class 4: Precision: 0.1000, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.59 batch/s]\n",
            "[Val] Kappa: 0.6565 Accuracy: 0.4900 Precision: 0.3985 Recall: 0.4900\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 38/38 [00:12<00:00,  2.96 batch/s, lr=1.0e-03, Loss=1.0939]\n",
            "[Train] Kappa: 0.4749 Accuracy: 0.4558 Precision: 0.4065 Recall: 0.4558 Loss: 1.2684\n",
            "[Train] Class 0: Precision: 0.6939, Recall: 0.9194\n",
            "[Train] Class 1: Precision: 0.2950, Recall: 0.3208\n",
            "[Train] Class 2: Precision: 0.2514, Recall: 0.1875\n",
            "[Train] Class 3: Precision: 0.3383, Recall: 0.3792\n",
            "[Train] Class 4: Precision: 0.2143, Recall: 0.0250\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.43 batch/s]\n",
            "[Val] Kappa: 0.0399 Accuracy: 0.3150 Precision: 0.2013 Recall: 0.3150\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 38/38 [00:12<00:00,  2.93 batch/s, lr=1.0e-03, Loss=1.4498]\n",
            "[Train] Kappa: 0.5637 Accuracy: 0.4742 Precision: 0.4201 Recall: 0.4742 Loss: 1.2687\n",
            "[Train] Class 0: Precision: 0.6528, Recall: 0.9611\n",
            "[Train] Class 1: Precision: 0.2388, Recall: 0.0667\n",
            "[Train] Class 2: Precision: 0.3374, Recall: 0.2292\n",
            "[Train] Class 3: Precision: 0.3448, Recall: 0.6250\n",
            "[Train] Class 4: Precision: 0.4000, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.27 batch/s]\n",
            "[Val] Kappa: 0.6789 Accuracy: 0.4925 Precision: 0.4167 Recall: 0.4925\n",
            "[Val] Best kappa: 0.6789, Epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet34 in single mode with With SLO Padding augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 38/38 [00:14<00:00,  2.58 batch/s, lr=1.0e-03, Loss=1.2173]\n",
            "[Train] Kappa: 0.3980 Accuracy: 0.4017 Precision: 0.3630 Recall: 0.4017 Loss: 1.4043\n",
            "[Train] Class 0: Precision: 0.6750, Recall: 0.7500\n",
            "[Train] Class 1: Precision: 0.2667, Recall: 0.2667\n",
            "[Train] Class 2: Precision: 0.2500, Recall: 0.2917\n",
            "[Train] Class 3: Precision: 0.2857, Recall: 0.3250\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  4.09 batch/s]\n",
            "[Val] Kappa: 0.5604 Accuracy: 0.4000 Precision: 0.3185 Recall: 0.4000\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 38/38 [00:14<00:00,  2.58 batch/s, lr=1.0e-03, Loss=1.1921]\n",
            "[Train] Kappa: 0.4833 Accuracy: 0.4367 Precision: 0.3946 Recall: 0.4367 Loss: 1.3490\n",
            "[Train] Class 0: Precision: 0.6694, Recall: 0.9000\n",
            "[Train] Class 1: Precision: 0.2687, Recall: 0.2250\n",
            "[Train] Class 2: Precision: 0.2682, Recall: 0.2458\n",
            "[Train] Class 3: Precision: 0.2958, Recall: 0.3500\n",
            "[Train] Class 4: Precision: 0.2727, Recall: 0.0250\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.36 batch/s]\n",
            "[Val] Kappa: 0.6453 Accuracy: 0.4625 Precision: 0.2907 Recall: 0.4625\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 38/38 [00:14<00:00,  2.60 batch/s, lr=1.0e-03, Loss=0.8620]\n",
            "[Train] Kappa: 0.5562 Accuracy: 0.4742 Precision: 0.4295 Recall: 0.4742 Loss: 1.2100\n",
            "[Train] Class 0: Precision: 0.7435, Recall: 0.9583\n",
            "[Train] Class 1: Precision: 0.2843, Recall: 0.2417\n",
            "[Train] Class 2: Precision: 0.2917, Recall: 0.2917\n",
            "[Train] Class 3: Precision: 0.3310, Recall: 0.3917\n",
            "[Train] Class 4: Precision: 0.2500, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.28 batch/s]\n",
            "[Val] Kappa: 0.6109 Accuracy: 0.4950 Precision: 0.3493 Recall: 0.4950\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 38/38 [00:15<00:00,  2.52 batch/s, lr=1.0e-03, Loss=0.9647]\n",
            "[Train] Kappa: 0.5656 Accuracy: 0.4683 Precision: 0.4184 Recall: 0.4683 Loss: 1.2388\n",
            "[Train] Class 0: Precision: 0.7398, Recall: 0.9556\n",
            "[Train] Class 1: Precision: 0.3138, Recall: 0.2458\n",
            "[Train] Class 2: Precision: 0.2613, Recall: 0.2167\n",
            "[Train] Class 3: Precision: 0.3133, Recall: 0.4333\n",
            "[Train] Class 4: Precision: 0.1875, Recall: 0.0250\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.99 batch/s]\n",
            "[Val] Kappa: 0.5952 Accuracy: 0.4925 Precision: 0.4196 Recall: 0.4925\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 38/38 [00:14<00:00,  2.60 batch/s, lr=1.0e-03, Loss=1.2151]\n",
            "[Train] Kappa: 0.5660 Accuracy: 0.4608 Precision: 0.4270 Recall: 0.4608 Loss: 1.2132\n",
            "[Train] Class 0: Precision: 0.7694, Recall: 0.9639\n",
            "[Train] Class 1: Precision: 0.3175, Recall: 0.3333\n",
            "[Train] Class 2: Precision: 0.2212, Recall: 0.1917\n",
            "[Train] Class 3: Precision: 0.2756, Recall: 0.3250\n",
            "[Train] Class 4: Precision: 0.3333, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.30 batch/s]\n",
            "[Val] Kappa: 0.4762 Accuracy: 0.4825 Precision: 0.3633 Recall: 0.4825\n",
            "[Val] Best kappa: 0.6453, Epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet34 in single mode with With Fundus Rotation augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 38/38 [00:13<00:00,  2.90 batch/s, lr=1.0e-03, Loss=1.1312]\n",
            "[Train] Kappa: 0.4338 Accuracy: 0.3967 Precision: 0.3718 Recall: 0.3967 Loss: 1.3867\n",
            "[Train] Class 0: Precision: 0.6739, Recall: 0.7806\n",
            "[Train] Class 1: Precision: 0.3118, Recall: 0.2417\n",
            "[Train] Class 2: Precision: 0.1984, Recall: 0.2042\n",
            "[Train] Class 3: Precision: 0.2755, Recall: 0.3375\n",
            "[Train] Class 4: Precision: 0.1250, Recall: 0.0583\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.22 batch/s]\n",
            "[Val] Kappa: 0.5955 Accuracy: 0.4375 Precision: 0.2907 Recall: 0.4375\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 38/38 [00:13<00:00,  2.87 batch/s, lr=1.0e-03, Loss=1.4538]\n",
            "[Train] Kappa: 0.4959 Accuracy: 0.4242 Precision: 0.3699 Recall: 0.4242 Loss: 1.3217\n",
            "[Train] Class 0: Precision: 0.6978, Recall: 0.8917\n",
            "[Train] Class 1: Precision: 0.2500, Recall: 0.2542\n",
            "[Train] Class 2: Precision: 0.2033, Recall: 0.1542\n",
            "[Train] Class 3: Precision: 0.2993, Recall: 0.3667\n",
            "[Train] Class 4: Precision: 0.1000, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.39 batch/s]\n",
            "[Val] Kappa: 0.5992 Accuracy: 0.4175 Precision: 0.3333 Recall: 0.4175\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 38/38 [00:13<00:00,  2.88 batch/s, lr=1.0e-03, Loss=1.4745]\n",
            "[Train] Kappa: 0.4765 Accuracy: 0.4325 Precision: 0.3756 Recall: 0.4325 Loss: 1.3270\n",
            "[Train] Class 0: Precision: 0.6756, Recall: 0.9083\n",
            "[Train] Class 1: Precision: 0.2780, Recall: 0.2792\n",
            "[Train] Class 2: Precision: 0.2299, Recall: 0.1792\n",
            "[Train] Class 3: Precision: 0.2941, Recall: 0.3333\n",
            "[Train] Class 4: Precision: 0.1250, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.34 batch/s]\n",
            "[Val] Kappa: 0.5983 Accuracy: 0.4975 Precision: 0.3448 Recall: 0.4975\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 38/38 [00:13<00:00,  2.92 batch/s, lr=1.0e-03, Loss=1.2535]\n",
            "[Train] Kappa: 0.5432 Accuracy: 0.4575 Precision: 0.3972 Recall: 0.4575 Loss: 1.2633\n",
            "[Train] Class 0: Precision: 0.7348, Recall: 0.9083\n",
            "[Train] Class 1: Precision: 0.3370, Recall: 0.2542\n",
            "[Train] Class 2: Precision: 0.2412, Recall: 0.1708\n",
            "[Train] Class 3: Precision: 0.3053, Recall: 0.5000\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.31 batch/s]\n",
            "[Val] Kappa: 0.6238 Accuracy: 0.5025 Precision: 0.3993 Recall: 0.5025\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 38/38 [00:13<00:00,  2.92 batch/s, lr=1.0e-03, Loss=1.1025]\n",
            "[Train] Kappa: 0.5761 Accuracy: 0.4875 Precision: 0.4445 Recall: 0.4875 Loss: 1.2197\n",
            "[Train] Class 0: Precision: 0.7294, Recall: 0.9583\n",
            "[Train] Class 1: Precision: 0.3453, Recall: 0.4000\n",
            "[Train] Class 2: Precision: 0.3028, Recall: 0.1792\n",
            "[Train] Class 3: Precision: 0.3333, Recall: 0.3792\n",
            "[Train] Class 4: Precision: 0.2941, Recall: 0.0833\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.87 batch/s]\n",
            "[Val] Kappa: 0.5770 Accuracy: 0.5175 Precision: 0.4467 Recall: 0.5175\n",
            "[Val] Best kappa: 0.6238, Epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet34 in single mode with Combined augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 38/38 [00:13<00:00,  2.77 batch/s, lr=1.0e-03, Loss=1.4773]\n",
            "[Train] Kappa: 0.2986 Accuracy: 0.3767 Precision: 0.3368 Recall: 0.3767 Loss: 1.4455\n",
            "[Train] Class 0: Precision: 0.6210, Recall: 0.7056\n",
            "[Train] Class 1: Precision: 0.2607, Recall: 0.2792\n",
            "[Train] Class 2: Precision: 0.2574, Recall: 0.3250\n",
            "[Train] Class 3: Precision: 0.2345, Recall: 0.2208\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.30 batch/s]\n",
            "[Val] Kappa: 0.1382 Accuracy: 0.4025 Precision: 0.2017 Recall: 0.4025\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 38/38 [00:13<00:00,  2.82 batch/s, lr=1.0e-03, Loss=1.7805]\n",
            "[Train] Kappa: 0.4566 Accuracy: 0.4158 Precision: 0.3787 Recall: 0.4158 Loss: 1.3682\n",
            "[Train] Class 0: Precision: 0.7214, Recall: 0.7694\n",
            "[Train] Class 1: Precision: 0.2611, Recall: 0.1958\n",
            "[Train] Class 2: Precision: 0.2716, Recall: 0.2625\n",
            "[Train] Class 3: Precision: 0.2786, Recall: 0.4667\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.29 batch/s]\n",
            "[Val] Kappa: 0.1273 Accuracy: 0.2775 Precision: 0.2751 Recall: 0.2775\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 38/38 [00:13<00:00,  2.85 batch/s, lr=1.0e-03, Loss=1.3865]\n",
            "[Train] Kappa: 0.4211 Accuracy: 0.4483 Precision: 0.3903 Recall: 0.4483 Loss: 1.3095\n",
            "[Train] Class 0: Precision: 0.7460, Recall: 0.8972\n",
            "[Train] Class 1: Precision: 0.2968, Recall: 0.4292\n",
            "[Train] Class 2: Precision: 0.2781, Recall: 0.2167\n",
            "[Train] Class 3: Precision: 0.2575, Recall: 0.2500\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.26 batch/s]\n",
            "[Val] Kappa: 0.5978 Accuracy: 0.4675 Precision: 0.2707 Recall: 0.4675\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 38/38 [00:13<00:00,  2.82 batch/s, lr=1.0e-03, Loss=1.2489]\n",
            "[Train] Kappa: 0.5348 Accuracy: 0.4708 Precision: 0.4130 Recall: 0.4708 Loss: 1.2358\n",
            "[Train] Class 0: Precision: 0.7292, Recall: 0.9722\n",
            "[Train] Class 1: Precision: 0.3373, Recall: 0.3500\n",
            "[Train] Class 2: Precision: 0.2379, Recall: 0.2042\n",
            "[Train] Class 3: Precision: 0.3127, Recall: 0.3375\n",
            "[Train] Class 4: Precision: 0.1667, Recall: 0.0083\n",
            "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  4.08 batch/s]\n",
            "[Val] Kappa: 0.6771 Accuracy: 0.4950 Precision: 0.4999 Recall: 0.4950\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 38/38 [00:13<00:00,  2.88 batch/s, lr=1.0e-03, Loss=1.3879]\n",
            "[Train] Kappa: 0.5642 Accuracy: 0.4792 Precision: 0.4369 Recall: 0.4792 Loss: 1.2189\n",
            "[Train] Class 0: Precision: 0.7516, Recall: 0.9583\n",
            "[Train] Class 1: Precision: 0.3158, Recall: 0.1750\n",
            "[Train] Class 2: Precision: 0.3017, Recall: 0.2917\n",
            "[Train] Class 3: Precision: 0.3145, Recall: 0.4875\n",
            "[Train] Class 4: Precision: 0.2500, Recall: 0.0083\n",
            "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.20 batch/s]\n",
            "[Val] Kappa: 0.6304 Accuracy: 0.5025 Precision: 0.4261 Recall: 0.5025\n",
            "[Val] Best kappa: 0.6771, Epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet34 in dual mode with Default augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 19/19 [00:12<00:00,  1.49 batch/s, lr=1.0e-03, Loss=1.2195]\n",
            "[Train] Kappa: 0.3323 Accuracy: 0.3767 Precision: 0.3417 Recall: 0.3767 Loss: 1.4594\n",
            "[Train] Class 0: Precision: 0.5845, Recall: 0.7111\n",
            "[Train] Class 1: Precision: 0.3130, Recall: 0.3000\n",
            "[Train] Class 2: Precision: 0.1880, Recall: 0.1833\n",
            "[Train] Class 3: Precision: 0.2806, Recall: 0.3250\n",
            "[Train] Class 4: Precision: 0.1000, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.86 batch/s]\n",
            "[Val] Kappa: -0.0659 Accuracy: 0.2300 Precision: 0.3250 Recall: 0.2300\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 19/19 [00:12<00:00,  1.50 batch/s, lr=1.0e-03, Loss=1.4348]\n",
            "[Train] Kappa: 0.4121 Accuracy: 0.4217 Precision: 0.3852 Recall: 0.4217 Loss: 1.3808\n",
            "[Train] Class 0: Precision: 0.6696, Recall: 0.8444\n",
            "[Train] Class 1: Precision: 0.2137, Recall: 0.2083\n",
            "[Train] Class 2: Precision: 0.2715, Recall: 0.3417\n",
            "[Train] Class 3: Precision: 0.3837, Recall: 0.2750\n",
            "[Train] Class 4: Precision: 0.1053, Recall: 0.0333\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.69 batch/s]\n",
            "[Val] Kappa: 0.1352 Accuracy: 0.3400 Precision: 0.2610 Recall: 0.3400\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 19/19 [00:12<00:00,  1.49 batch/s, lr=1.0e-03, Loss=1.3591]\n",
            "[Train] Kappa: 0.5223 Accuracy: 0.4467 Precision: 0.3845 Recall: 0.4467 Loss: 1.3235\n",
            "[Train] Class 0: Precision: 0.7454, Recall: 0.8944\n",
            "[Train] Class 1: Precision: 0.2556, Recall: 0.1917\n",
            "[Train] Class 2: Precision: 0.2245, Recall: 0.1833\n",
            "[Train] Class 3: Precision: 0.3246, Recall: 0.5167\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.55 batch/s]\n",
            "[Val] Kappa: 0.6146 Accuracy: 0.5250 Precision: 0.3875 Recall: 0.5250\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 19/19 [00:12<00:00,  1.49 batch/s, lr=1.0e-03, Loss=1.4701]\n",
            "[Train] Kappa: 0.4703 Accuracy: 0.4317 Precision: 0.4140 Recall: 0.4317 Loss: 1.3377\n",
            "[Train] Class 0: Precision: 0.7045, Recall: 0.8611\n",
            "[Train] Class 1: Precision: 0.2520, Recall: 0.2583\n",
            "[Train] Class 2: Precision: 0.2650, Recall: 0.2583\n",
            "[Train] Class 3: Precision: 0.2963, Recall: 0.3333\n",
            "[Train] Class 4: Precision: 0.4000, Recall: 0.0333\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.89 batch/s]\n",
            "[Val] Kappa: 0.6125 Accuracy: 0.4800 Precision: 0.3310 Recall: 0.4800\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 19/19 [00:12<00:00,  1.50 batch/s, lr=1.0e-03, Loss=1.2294]\n",
            "[Train] Kappa: 0.5483 Accuracy: 0.4700 Precision: 0.4115 Recall: 0.4700 Loss: 1.2525\n",
            "[Train] Class 0: Precision: 0.7425, Recall: 0.9611\n",
            "[Train] Class 1: Precision: 0.3043, Recall: 0.2333\n",
            "[Train] Class 2: Precision: 0.2828, Recall: 0.2333\n",
            "[Train] Class 3: Precision: 0.3210, Recall: 0.4333\n",
            "[Train] Class 4: Precision: 0.0714, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.89 batch/s]\n",
            "[Val] Kappa: 0.5153 Accuracy: 0.3450 Precision: 0.3109 Recall: 0.3450\n",
            "[Val] Best kappa: 0.6146, Epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet34 in dual mode with With CutOut augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 19/19 [00:12<00:00,  1.48 batch/s, lr=1.0e-03, Loss=1.3646]\n",
            "[Train] Kappa: 0.3419 Accuracy: 0.3733 Precision: 0.3629 Recall: 0.3733 Loss: 1.4685\n",
            "[Train] Class 0: Precision: 0.5687, Recall: 0.6667\n",
            "[Train] Class 1: Precision: 0.3529, Recall: 0.2000\n",
            "[Train] Class 2: Precision: 0.2533, Recall: 0.3167\n",
            "[Train] Class 3: Precision: 0.2500, Recall: 0.3167\n",
            "[Train] Class 4: Precision: 0.2105, Recall: 0.0667\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.57 batch/s]\n",
            "[Val] Kappa: 0.3134 Accuracy: 0.4450 Precision: 0.3433 Recall: 0.4450\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 19/19 [00:12<00:00,  1.48 batch/s, lr=1.0e-03, Loss=1.8302]\n",
            "[Train] Kappa: 0.5124 Accuracy: 0.4500 Precision: 0.4120 Recall: 0.4500 Loss: 1.3473\n",
            "[Train] Class 0: Precision: 0.7378, Recall: 0.9222\n",
            "[Train] Class 1: Precision: 0.2660, Recall: 0.2083\n",
            "[Train] Class 2: Precision: 0.2945, Recall: 0.3583\n",
            "[Train] Class 3: Precision: 0.2677, Recall: 0.2833\n",
            "[Train] Class 4: Precision: 0.2500, Recall: 0.0333\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.65 batch/s]\n",
            "[Val] Kappa: 0.4326 Accuracy: 0.4400 Precision: 0.2892 Recall: 0.4400\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 19/19 [00:12<00:00,  1.49 batch/s, lr=1.0e-03, Loss=1.5628]\n",
            "[Train] Kappa: 0.4649 Accuracy: 0.4000 Precision: 0.3763 Recall: 0.4000 Loss: 1.3831\n",
            "[Train] Class 0: Precision: 0.7368, Recall: 0.7778\n",
            "[Train] Class 1: Precision: 0.2400, Recall: 0.2000\n",
            "[Train] Class 2: Precision: 0.2174, Recall: 0.1667\n",
            "[Train] Class 3: Precision: 0.2632, Recall: 0.4583\n",
            "[Train] Class 4: Precision: 0.1111, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.90 batch/s]\n",
            "[Val] Kappa: 0.2964 Accuracy: 0.4350 Precision: 0.3594 Recall: 0.4350\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 19/19 [00:12<00:00,  1.50 batch/s, lr=1.0e-03, Loss=1.1064]\n",
            "[Train] Kappa: 0.4719 Accuracy: 0.4433 Precision: 0.4147 Recall: 0.4433 Loss: 1.3294\n",
            "[Train] Class 0: Precision: 0.7000, Recall: 0.8944\n",
            "[Train] Class 1: Precision: 0.2784, Recall: 0.2250\n",
            "[Train] Class 2: Precision: 0.2699, Recall: 0.3667\n",
            "[Train] Class 3: Precision: 0.3084, Recall: 0.2750\n",
            "[Train] Class 4: Precision: 0.3333, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.92 batch/s]\n",
            "[Val] Kappa: 0.5978 Accuracy: 0.4650 Precision: 0.5120 Recall: 0.4650\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 19/19 [00:12<00:00,  1.49 batch/s, lr=1.0e-03, Loss=1.3611]\n",
            "[Train] Kappa: 0.4283 Accuracy: 0.4217 Precision: 0.3569 Recall: 0.4217 Loss: 1.3640\n",
            "[Train] Class 0: Precision: 0.6833, Recall: 0.9111\n",
            "[Train] Class 1: Precision: 0.2941, Recall: 0.2083\n",
            "[Train] Class 2: Precision: 0.2044, Recall: 0.2333\n",
            "[Train] Class 3: Precision: 0.2609, Recall: 0.3000\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.53 batch/s]\n",
            "[Val] Kappa: 0.6792 Accuracy: 0.4900 Precision: 0.2907 Recall: 0.4900\n",
            "[Val] Best kappa: 0.6792, Epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet34 in dual mode with With SLO Padding augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 19/19 [00:14<00:00,  1.32 batch/s, lr=1.0e-03, Loss=1.5375]\n",
            "[Train] Kappa: 0.3777 Accuracy: 0.3933 Precision: 0.3618 Recall: 0.3933 Loss: 1.5030\n",
            "[Train] Class 0: Precision: 0.6124, Recall: 0.7111\n",
            "[Train] Class 1: Precision: 0.2621, Recall: 0.2250\n",
            "[Train] Class 2: Precision: 0.2095, Recall: 0.1833\n",
            "[Train] Class 3: Precision: 0.3436, Recall: 0.4667\n",
            "[Train] Class 4: Precision: 0.1500, Recall: 0.0500\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.98 batch/s]\n",
            "[Val] Kappa: 0.1996 Accuracy: 0.3850 Precision: 0.2750 Recall: 0.3850\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 19/19 [00:14<00:00,  1.32 batch/s, lr=1.0e-03, Loss=1.3256]\n",
            "[Train] Kappa: 0.5437 Accuracy: 0.4567 Precision: 0.4304 Recall: 0.4567 Loss: 1.3544\n",
            "[Train] Class 0: Precision: 0.7571, Recall: 0.8833\n",
            "[Train] Class 1: Precision: 0.2920, Recall: 0.3333\n",
            "[Train] Class 2: Precision: 0.2901, Recall: 0.3167\n",
            "[Train] Class 3: Precision: 0.3091, Recall: 0.2833\n",
            "[Train] Class 4: Precision: 0.2500, Recall: 0.0500\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.94 batch/s]\n",
            "[Val] Kappa: 0.4624 Accuracy: 0.3950 Precision: 0.4058 Recall: 0.3950\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 19/19 [00:14<00:00,  1.29 batch/s, lr=1.0e-03, Loss=1.1704]\n",
            "[Train] Kappa: 0.5845 Accuracy: 0.5067 Precision: 0.4871 Recall: 0.5067 Loss: 1.2745\n",
            "[Train] Class 0: Precision: 0.7328, Recall: 0.9444\n",
            "[Train] Class 1: Precision: 0.3553, Recall: 0.4500\n",
            "[Train] Class 2: Precision: 0.3276, Recall: 0.1583\n",
            "[Train] Class 3: Precision: 0.3758, Recall: 0.4667\n",
            "[Train] Class 4: Precision: 0.5556, Recall: 0.0833\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.63 batch/s]\n",
            "[Val] Kappa: 0.6162 Accuracy: 0.4700 Precision: 0.3086 Recall: 0.4700\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 19/19 [00:14<00:00,  1.34 batch/s, lr=1.0e-03, Loss=0.9741]\n",
            "[Train] Kappa: 0.6359 Accuracy: 0.5050 Precision: 0.4766 Recall: 0.5050 Loss: 1.1874\n",
            "[Train] Class 0: Precision: 0.7650, Recall: 0.9222\n",
            "[Train] Class 1: Precision: 0.3646, Recall: 0.2917\n",
            "[Train] Class 2: Precision: 0.2958, Recall: 0.3500\n",
            "[Train] Class 3: Precision: 0.4390, Recall: 0.4500\n",
            "[Train] Class 4: Precision: 0.2727, Recall: 0.1000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.92 batch/s]\n",
            "[Val] Kappa: 0.5824 Accuracy: 0.5200 Precision: 0.3825 Recall: 0.5200\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 19/19 [00:14<00:00,  1.33 batch/s, lr=1.0e-03, Loss=1.4255]\n",
            "[Train] Kappa: 0.5297 Accuracy: 0.4550 Precision: 0.4069 Recall: 0.4550 Loss: 1.2884\n",
            "[Train] Class 0: Precision: 0.7248, Recall: 0.8778\n",
            "[Train] Class 1: Precision: 0.3103, Recall: 0.3750\n",
            "[Train] Class 2: Precision: 0.2258, Recall: 0.1750\n",
            "[Train] Class 3: Precision: 0.3556, Recall: 0.4000\n",
            "[Train] Class 4: Precision: 0.1111, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.70 batch/s]\n",
            "[Val] Kappa: 0.6663 Accuracy: 0.4450 Precision: 0.4995 Recall: 0.4450\n",
            "[Val] Best kappa: 0.6663, Epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet34 in dual mode with With Fundus Rotation augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 19/19 [00:12<00:00,  1.47 batch/s, lr=1.0e-03, Loss=1.2770]\n",
            "[Train] Kappa: 0.3089 Accuracy: 0.3567 Precision: 0.3267 Recall: 0.3567 Loss: 1.4545\n",
            "[Train] Class 0: Precision: 0.6077, Recall: 0.7056\n",
            "[Train] Class 1: Precision: 0.2162, Recall: 0.1333\n",
            "[Train] Class 2: Precision: 0.2000, Recall: 0.2333\n",
            "[Train] Class 3: Precision: 0.2500, Recall: 0.3500\n",
            "[Train] Class 4: Precision: 0.1111, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.91 batch/s]\n",
            "[Val] Kappa: 0.4669 Accuracy: 0.4450 Precision: 0.2726 Recall: 0.4450\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 19/19 [00:12<00:00,  1.46 batch/s, lr=1.0e-03, Loss=1.1044]\n",
            "[Train] Kappa: 0.4802 Accuracy: 0.4367 Precision: 0.4002 Recall: 0.4367 Loss: 1.3585\n",
            "[Train] Class 0: Precision: 0.6695, Recall: 0.8889\n",
            "[Train] Class 1: Precision: 0.3333, Recall: 0.2500\n",
            "[Train] Class 2: Precision: 0.2613, Recall: 0.2417\n",
            "[Train] Class 3: Precision: 0.2712, Recall: 0.2667\n",
            "[Train] Class 4: Precision: 0.2619, Recall: 0.1833\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.89 batch/s]\n",
            "[Val] Kappa: 0.3493 Accuracy: 0.3950 Precision: 0.1923 Recall: 0.3950\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 19/19 [00:12<00:00,  1.47 batch/s, lr=1.0e-03, Loss=1.1978]\n",
            "[Train] Kappa: 0.4598 Accuracy: 0.4233 Precision: 0.4607 Recall: 0.4233 Loss: 1.3462\n",
            "[Train] Class 0: Precision: 0.6937, Recall: 0.8556\n",
            "[Train] Class 1: Precision: 0.2169, Recall: 0.1500\n",
            "[Train] Class 2: Precision: 0.2671, Recall: 0.3250\n",
            "[Train] Class 3: Precision: 0.2789, Recall: 0.3417\n",
            "[Train] Class 4: Precision: 1.0000, Recall: 0.0333\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.88 batch/s]\n",
            "[Val] Kappa: 0.5853 Accuracy: 0.4650 Precision: 0.3334 Recall: 0.4650\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 19/19 [00:12<00:00,  1.47 batch/s, lr=1.0e-03, Loss=1.5828]\n",
            "[Train] Kappa: 0.5469 Accuracy: 0.4567 Precision: 0.4183 Recall: 0.4567 Loss: 1.2890\n",
            "[Train] Class 0: Precision: 0.7185, Recall: 0.9500\n",
            "[Train] Class 1: Precision: 0.2955, Recall: 0.2167\n",
            "[Train] Class 2: Precision: 0.2628, Recall: 0.3417\n",
            "[Train] Class 3: Precision: 0.3056, Recall: 0.2750\n",
            "[Train] Class 4: Precision: 0.3000, Recall: 0.0500\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.36 batch/s]\n",
            "[Val] Kappa: 0.5751 Accuracy: 0.5000 Precision: 0.3471 Recall: 0.5000\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 19/19 [00:13<00:00,  1.45 batch/s, lr=1.0e-03, Loss=1.1759]\n",
            "[Train] Kappa: 0.5243 Accuracy: 0.4233 Precision: 0.3481 Recall: 0.4233 Loss: 1.3097\n",
            "[Train] Class 0: Precision: 0.7149, Recall: 0.9056\n",
            "[Train] Class 1: Precision: 0.1091, Recall: 0.0500\n",
            "[Train] Class 2: Precision: 0.2059, Recall: 0.1167\n",
            "[Train] Class 3: Precision: 0.2905, Recall: 0.5833\n",
            "[Train] Class 4: Precision: 0.1250, Recall: 0.0167\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.77 batch/s]\n",
            "[Val] Kappa: 0.6091 Accuracy: 0.4750 Precision: 0.3256 Recall: 0.4750\n",
            "[Val] Best kappa: 0.6091, Epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet34 in dual mode with Combined augmentations...\n",
            "\n",
            "Epoch 1/5\n",
            "Training: 100%|██████████| 19/19 [00:13<00:00,  1.40 batch/s, lr=1.0e-03, Loss=1.4038]\n",
            "[Train] Kappa: 0.2854 Accuracy: 0.3650 Precision: 0.3218 Recall: 0.3650 Loss: 1.5121\n",
            "[Train] Class 0: Precision: 0.5982, Recall: 0.7278\n",
            "[Train] Class 1: Precision: 0.2611, Recall: 0.3417\n",
            "[Train] Class 2: Precision: 0.1920, Recall: 0.2000\n",
            "[Train] Class 3: Precision: 0.2584, Recall: 0.1917\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.91 batch/s]\n",
            "[Val] Kappa: 0.0000 Accuracy: 0.3000 Precision: 0.0900 Recall: 0.3000\n",
            "\n",
            "Epoch 2/5\n",
            "Training: 100%|██████████| 19/19 [00:13<00:00,  1.44 batch/s, lr=1.0e-03, Loss=1.4376]\n",
            "[Train] Kappa: 0.4972 Accuracy: 0.4450 Precision: 0.3876 Recall: 0.4450 Loss: 1.3758\n",
            "[Train] Class 0: Precision: 0.7091, Recall: 0.8667\n",
            "[Train] Class 1: Precision: 0.2969, Recall: 0.1583\n",
            "[Train] Class 2: Precision: 0.2593, Recall: 0.2917\n",
            "[Train] Class 3: Precision: 0.3184, Recall: 0.4750\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.92 batch/s]\n",
            "[Val] Kappa: 0.5575 Accuracy: 0.5050 Precision: 0.3789 Recall: 0.5050\n",
            "\n",
            "Epoch 3/5\n",
            "Training: 100%|██████████| 19/19 [00:13<00:00,  1.39 batch/s, lr=1.0e-03, Loss=1.3539]\n",
            "[Train] Kappa: 0.4994 Accuracy: 0.4317 Precision: 0.3671 Recall: 0.4317 Loss: 1.3073\n",
            "[Train] Class 0: Precision: 0.7162, Recall: 0.8833\n",
            "[Train] Class 1: Precision: 0.1915, Recall: 0.1500\n",
            "[Train] Class 2: Precision: 0.2655, Recall: 0.2500\n",
            "[Train] Class 3: Precision: 0.3041, Recall: 0.4333\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.56 batch/s]\n",
            "[Val] Kappa: 0.4825 Accuracy: 0.5250 Precision: 0.3686 Recall: 0.5250\n",
            "\n",
            "Epoch 4/5\n",
            "Training: 100%|██████████| 19/19 [00:13<00:00,  1.43 batch/s, lr=1.0e-03, Loss=1.2374]\n",
            "[Train] Kappa: 0.4860 Accuracy: 0.4467 Precision: 0.3941 Recall: 0.4467 Loss: 1.2722\n",
            "[Train] Class 0: Precision: 0.6962, Recall: 0.9167\n",
            "[Train] Class 1: Precision: 0.2714, Recall: 0.3167\n",
            "[Train] Class 2: Precision: 0.3810, Recall: 0.1333\n",
            "[Train] Class 3: Precision: 0.2737, Recall: 0.4083\n",
            "[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.91 batch/s]\n",
            "[Val] Kappa: 0.3447 Accuracy: 0.4700 Precision: 0.3634 Recall: 0.4700\n",
            "\n",
            "Epoch 5/5\n",
            "Training: 100%|██████████| 19/19 [00:13<00:00,  1.44 batch/s, lr=1.0e-03, Loss=1.4329]\n",
            "[Train] Kappa: 0.5165 Accuracy: 0.4483 Precision: 0.4105 Recall: 0.4483 Loss: 1.2579\n",
            "[Train] Class 0: Precision: 0.7330, Recall: 0.9000\n",
            "[Train] Class 1: Precision: 0.3176, Recall: 0.2250\n",
            "[Train] Class 2: Precision: 0.2600, Recall: 0.2167\n",
            "[Train] Class 3: Precision: 0.2842, Recall: 0.4333\n",
            "[Train] Class 4: Precision: 0.1818, Recall: 0.0333\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  2.93 batch/s]\n",
            "[Val] Kappa: 0.6662 Accuracy: 0.5150 Precision: 0.3919 Recall: 0.5150\n",
            "[Val] Best kappa: 0.6662, Epoch 5\n",
            "Best model: resnet18_single_With Fundus Rotation with score 0.734333380064489\n",
            "Results: {'resnet18_single_Default': 0.6571517219981384, 'resnet18_single_With CutOut': 0.6424399740428293, 'resnet18_single_With SLO Padding': 0.7012352772191899, 'resnet18_single_With Fundus Rotation': 0.734333380064489, 'resnet18_single_Combined': 0.648109243697479, 'resnet18_dual_Default': 0.6187845303867403, 'resnet18_dual_With CutOut': 0.6639950678175093, 'resnet18_dual_With SLO Padding': 0.6077799269288631, 'resnet18_dual_With Fundus Rotation': 0.6857142857142857, 'resnet18_dual_Combined': 0.6391752577319587, 'resnet34_single_Default': 0.5947306595066286, 'resnet34_single_With CutOut': 0.6789345000643418, 'resnet34_single_With SLO Padding': 0.6453407510431155, 'resnet34_single_With Fundus Rotation': 0.6238139783509287, 'resnet34_single_Combined': 0.6771183277342292, 'resnet34_dual_Default': 0.6146044624746451, 'resnet34_dual_With CutOut': 0.6792172405251424, 'resnet34_dual_With SLO Padding': 0.6662943311924043, 'resnet34_dual_With Fundus Rotation': 0.6091245376078915, 'resnet34_dual_Combined': 0.6661510698633668}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "models_to_test = ['resnet18', 'resnet34']\n",
        "#vgg_16  'densenet121', 'efficientnet_b0' didnt work expected other input sizes\n",
        "results = {}\n",
        "\n",
        " # Default transform_train for reference\n",
        "default_transform_train = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomCrop((210, 210)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "augmentations = [\n",
        "        (\"Default\", default_transform_train),\n",
        "        (\"With CutOut\", transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.RandomCrop((210, 210)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "            transforms.ToTensor(),\n",
        "            CutOut(mask_size=32, p=0.5),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])),\n",
        "        (\"With SLO Padding\", transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            SLORandomPad((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])),\n",
        "        (\"With Fundus Rotation\", transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.RandomCrop((210, 210)),\n",
        "            FundRandomRotate(prob=0.7, degree=45),  # Increased rotation range for this experiment\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])),\n",
        "        (\"Combined\", transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.RandomCrop((210, 210)),\n",
        "            SLORandomPad((224, 224)),\n",
        "            FundRandomRotate(prob=0.5, degree=30),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ]))\n",
        "    ]\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "for backbone_name in models_to_test:\n",
        "        for mode in ['single', 'dual']:\n",
        "            for aug_name, transform_train in augmentations:\n",
        "                # Load datasets with augmentations\n",
        "                train_dataset = RetinopathyDataset('./DeepDRiD/train.csv', './DeepDRiD/train/', transform_train, mode)\n",
        "                val_dataset = RetinopathyDataset('./DeepDRiD/val.csv', './DeepDRiD/val/', transform_test, mode)\n",
        "                test_dataset = RetinopathyDataset('./DeepDRiD/test.csv', './DeepDRiD/test/', transform_test, mode, test=True)\n",
        "\n",
        "                train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "                val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "                test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "                checkpoint_path = f\"./{backbone_name}_{mode}_{aug_name.replace(' ', '_')}.pth\"\n",
        "                '''\n",
        "                # Check if the checkpoint already exists\n",
        "                if os.path.exists(checkpoint_path):\n",
        "                    print(f\"Checkpoint for {backbone_name} in {mode} mode with {aug_name} augmentation already exists. Skipping training.\")\n",
        "                    # Optionally, you can load the model here if you want to continue from the checkpoint\n",
        "                    # model.load_state_dict(torch.load(checkpoint_path))\n",
        "                    continue  # Skip this iteration if the checkpoint exists\n",
        "                '''\n",
        "                model = get_model(backbone_name, mode).to(device)\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "                lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "                print(f\"Training {backbone_name} in {mode} mode with {aug_name} augmentations...\")\n",
        "                checkpoint_path = f\"./{backbone_name}_{mode}_{aug_name.replace(' ', '_')}.pth\"\n",
        "                model, best_kappa = train_model(model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=5, checkpoint_path=checkpoint_path)\n",
        "                results[f\"{backbone_name}_{mode}_{aug_name}\"] = best_kappa\n",
        "\n",
        " # Summarize results\n",
        "best_model_name = max(results, key=results.get)\n",
        "print(f\"Best model: {best_model_name} with score {results[best_model_name]}\")\n",
        "print(\"Results:\", results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "aGYibi-EJvUs",
        "outputId": "eb1553ab-bc12-42cf-a5ad-56ed5bc53a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------+----------------------+----------+\n",
            "| Model    | Mode   | Augmentation         |    Kappa |\n",
            "+==========+========+======================+==========+\n",
            "| resnet18 | single | With Fundus Rotation | 0.734333 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet18 | single | With SLO Padding     | 0.701235 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet18 | dual   | With Fundus Rotation | 0.685714 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet34 | dual   | With CutOut          | 0.679217 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet34 | single | With CutOut          | 0.678935 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet34 | single | Combined             | 0.677118 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet34 | dual   | With SLO Padding     | 0.666294 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet34 | dual   | Combined             | 0.666151 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet18 | dual   | With CutOut          | 0.663995 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet18 | single | Default              | 0.657152 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet18 | single | Combined             | 0.648109 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet34 | single | With SLO Padding     | 0.645341 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet18 | single | With CutOut          | 0.64244  |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet18 | dual   | Combined             | 0.639175 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet34 | single | With Fundus Rotation | 0.623814 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet18 | dual   | Default              | 0.618785 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet34 | dual   | Default              | 0.614604 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet34 | dual   | With Fundus Rotation | 0.609125 |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet18 | dual   | With SLO Padding     | 0.60778  |\n",
            "+----------+--------+----------------------+----------+\n",
            "| resnet34 | single | Default              | 0.594731 |\n",
            "+----------+--------+----------------------+----------+\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
        "transformed_array = [(a.split('_')[0], a.split('_')[1], a.split('_')[2], b) for a, b in sorted_results]\n",
        "\n",
        "# Print the sorted results as a table\n",
        "print(tabulate(transformed_array, headers=[\"Model\", \"Mode\", \"Augmentation\", \"Kappa\"], tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UTzg7FduJTo"
      },
      "source": [
        "# Task B: Two stage training with additional dataset(s)\n",
        "\n",
        "1.  Choose a diabetic retinopathy dataset from either Kaggle DR Resized or APTOS 2019 Blindness Detection (links are provided below).\n",
        "2.  Fine-tune an ImageNet pretrained model (e.g., ResNet18, ResNet34, VGG, EfficientNet, DenseNet) on the selected dataset by unfreezing all pretrained layers. If you have any difficulties, you can also use pretrained weights of Kaggle DR Resized (pretrained_DR_resize) to skip this step: https://www.kaggle.com/competitions/521153S-3005-final-project/data\n",
        "\n",
        "3.   Next up, fine-tune this trained model on the DeepDRiD dataset (keep all the layers unfrozen) and see how it impacts your Cohen Kappa score.\n",
        "\n",
        "4.   Save the fine-tuned model.\n",
        "The goal of task (b) is to compare the performance of a deep model that is trained and fine-tuned on a task-specific dataset with that of a model that is first trained on a general dataset and then fine-tuned on the same task-specific dataset in task (a).\n",
        "5.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjduCbPrXn4z"
      },
      "source": [
        "Download diabetic retinopathy dataset from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOl4puYE1wI5"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "# Upload the Kaggle API Token (kaggle.json). You should create this token from: https://www.kaggle.com/settings\n",
        "files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5UaMAoaV8AoR"
      },
      "outputs": [],
      "source": [
        "# Move kaggle API token to correct folder and verify\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets list\n",
        "!pip install --upgrade kagglehub\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idfEIpgv-4TG"
      },
      "outputs": [],
      "source": [
        "# download diabetic retinopathy datasetyes\n",
        "!kaggle datasets download -d mariaherrerot/aptos2019 -p /content/dataset_task_b/aptos2019\n",
        "# !kaggle datasets download -d tanlikesmath/diabetic-retinopathy-resized -p /content/dataset_task_b/retinopathy-resized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ImnS2eNzGpOc"
      },
      "outputs": [],
      "source": [
        "!unzip /content/dataset_task_b/aptos2019/*.zip -d /content/dataset_task_b//aptos2019\n",
        "# !unzip /content/dataset_task_b/retinopathy-resized/*.zip -d /content/dataset_task_b/retinopathy-resized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_xDzBM7Ck65"
      },
      "outputs": [],
      "source": [
        "# Remove zip file\n",
        "# !rm /content/dataset_task_b/aptos2019/*.zip\n",
        "# !rm /content/dataset_task_b/retinopathy-resized/*.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAr3cdktdoW1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class APTOS2019Dataset(Dataset):\n",
        "    def __init__(self, csv_file, image_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.data['img_path'] = self.data['id_code'].apply(lambda x: os.path.join(image_dir, f\"{x}.png\"))\n",
        "\n",
        "    def __len__(self):\n",
        "         return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data.iloc[index]\n",
        "        image_path = row['img_path']\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = row['diagnosis']\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiVppyP3bu2-"
      },
      "outputs": [],
      "source": [
        "transform_train_b = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop((210, 210)),\n",
        "    SLORandomPad((224, 224)),\n",
        "    FundRandomRotate(prob=0.5, degree=30),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_test_b = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "def train_model_b(model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=5,\n",
        "                checkpoint_path='model.pth'):\n",
        "    best_model = model.state_dict()\n",
        "    best_epoch = None\n",
        "    best_val_kappa = -1.0\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f'\\nEpoch {epoch}/{num_epochs}')\n",
        "        running_loss = []\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        model.train()\n",
        "\n",
        "        with tqdm(total=len(train_loader), desc=f'Training', unit=' batch', file=sys.stdout) as pbar:\n",
        "            for images, labels in train_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                preds = torch.argmax(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                running_loss.append(loss.item())\n",
        "                pbar.set_postfix({'lr': f'{optimizer.param_groups[0][\"lr\"]:.1e}', 'Loss': f'{loss.item():.4f}'})\n",
        "                pbar.update(1)\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        epoch_loss = sum(running_loss) / len(running_loss)\n",
        "        train_metrics = compute_metrics(all_preds, all_labels, per_class=True)\n",
        "        kappa, accuracy, precision, recall = train_metrics[:4]\n",
        "\n",
        "        print(f'[Train] Kappa: {kappa:.4f} Accuracy: {accuracy:.4f} '\n",
        "              f'Precision: {precision:.4f} Recall: {recall:.4f} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        if len(train_metrics) > 4:\n",
        "            precision_per_class, recall_per_class = train_metrics[4:]\n",
        "            for i, (precision, recall) in enumerate(zip(precision_per_class, recall_per_class)):\n",
        "                print(f'[Train] Class {i}: Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
        "\n",
        "        # Evaluation on the validation set at the end of each epoch\n",
        "        val_metrics = evaluate_model_b(model, val_loader, device)\n",
        "        val_kappa, val_accuracy, val_precision, val_recall = val_metrics[:4]\n",
        "        print(f'[Val] Kappa: {val_kappa:.4f} Accuracy: {val_accuracy:.4f} '\n",
        "              f'Precision: {val_precision:.4f} Recall: {val_recall:.4f}')\n",
        "\n",
        "        if val_kappa > best_val_kappa:\n",
        "            best_val_kappa = val_kappa\n",
        "            best_epoch = epoch\n",
        "            best_model = model.state_dict()\n",
        "            torch.save(best_model, checkpoint_path)\n",
        "\n",
        "    print(f'[Val] Best kappa: {best_val_kappa:.4f}, Epoch {best_epoch}')\n",
        "    return model, best_val_kappa\n",
        "\n",
        "def evaluate_model_b(model, val_loader, device):\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_image_ids = []\n",
        "\n",
        "    with tqdm(total=len(val_loader), desc=f'Evaluating', unit=' batch', file=sys.stdout) as pbar:\n",
        "        for i, data in enumerate(val_loader):\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(images)\n",
        "                preds = torch.argmax(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            start_idx = i * val_loader.batch_size\n",
        "            end_idx = start_idx + len(images)\n",
        "            image_ids = []\n",
        "            for idx in range(start_idx, end_idx):\n",
        "                image_id = val_loader.dataset.data.iloc[idx]['id_code']\n",
        "                image_ids.append(image_id)\n",
        "\n",
        "            all_image_ids.extend(image_ids)\n",
        "            all_labels.extend(labels.numpy())\n",
        "            pbar.update(1)\n",
        "    metrics = compute_metrics(all_preds, all_labels)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def compute_metrics(preds, labels, per_class=False):\n",
        "    kappa = cohen_kappa_score(labels, preds, weights='quadratic')\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
        "\n",
        "    # Calculate and print precision and recall for each class\n",
        "    if per_class:\n",
        "        precision_per_class = precision_score(labels, preds, average=None, zero_division=0)\n",
        "        recall_per_class = recall_score(labels, preds, average=None, zero_division=0)\n",
        "        return kappa, accuracy, precision, recall, precision_per_class, recall_per_class\n",
        "\n",
        "    return kappa, accuracy, precision, recall\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFpUgWm5AqtY"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "models_to_test = ['resnet18']\n",
        "results_2_stage = {}\n",
        "transform_train_b = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop((210, 210)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "transform_test_b = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "default_transform_train = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomCrop((210, 210)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "augmentations = [\n",
        "\n",
        "        (\"With SLO Padding\", transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            SLORandomPad((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ]))\n",
        "\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbEoNP06DB5i"
      },
      "outputs": [],
      "source": [
        "def training_stage1(model_stage1, backbone_name):\n",
        "  print(\"Stage 1: Fine-tuning on the specific dataset\")\n",
        "  # model_stage1 = get_model(backbone_name, mode='single').to(device)\n",
        "  # Unfreeze all layers\n",
        "  for param in model_stage1.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "  checkpoint_stage1 = f\"./{backbone_name}_stage1_specific.pth\"\n",
        "  model_stage1, best_kappa_stage1 = train_model_b(\n",
        "      model_stage1,\n",
        "      train_loader,\n",
        "      val_loader,\n",
        "      device,\n",
        "      criterion,\n",
        "      optimizer,\n",
        "      lr_scheduler,\n",
        "      num_epochs=5,\n",
        "      checkpoint_path=checkpoint_stage1\n",
        "  )\n",
        "\n",
        "  print(f\"Stage 1 completed. Best Kappa score on specific dataset: {best_kappa_stage1}\")\n",
        "  return model_stage1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lpoT1Nbd9yjt"
      },
      "outputs": [],
      "source": [
        "for backbone_name in models_to_test:\n",
        "  for aug_name, transform_train in augmentations:\n",
        "\n",
        "    train_csv_stage1 = './dataset_task_b/aptos2019/train_1.csv'\n",
        "    val_csv_stage1 = './dataset_task_b/aptos2019/valid.csv'\n",
        "    dataset_path_stage1 =  './dataset_task_b/aptos2019'\n",
        "\n",
        "    train_dataset_stage1 = APTOS2019Dataset(csv_file=train_csv_stage1, image_dir=f\"{dataset_path_stage1}/train_images/train_images\", transform=transform_train_b)\n",
        "    val_dataset_stage1 = APTOS2019Dataset(csv_file = val_csv_stage1, image_dir =  f\"{dataset_path_stage1}/val_images/val_images\", transform = transform_test_b )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset_stage1, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset_stage1, batch_size=32, shuffle=False)\n",
        "\n",
        "    train_dataset_stage2 = RetinopathyDataset('./DeepDRiD/train.csv', './DeepDRiD/train/', transform_train, mode='single')\n",
        "    val_dataset_stage2 = RetinopathyDataset('./DeepDRiD/val.csv', './DeepDRiD/val/', transform_test_b, mode='single')\n",
        "    train_loader_stage2 = DataLoader(train_dataset_stage2, batch_size=32, shuffle=True)\n",
        "    val_loader_stage2 = DataLoader(val_dataset_stage2, batch_size=32, shuffle=False)\n",
        "    model_stage1 = get_model(backbone_name, mode='single').to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model_stage1.parameters(), lr=0.001)\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "    print(f\"Training {backbone_name} in single mode with {aug_name} augmentations...\")\n",
        "\n",
        "    # train stage1: Fine-tune the trained model on optos2019\n",
        "    model_stage1 = training_stage1(model_stage1, backbone_name)\n",
        "    # train stage2: Fine-tune the trained model on DeepDRiD dataset\n",
        "    print(\"Stage 2: Fine-tuning on the DeepDRiD dataset\")\n",
        "\n",
        "    model_stage2 = model_stage1\n",
        "    optimizer_stage2 = optim.Adam(model_stage2.parameters(), lr=0.0001)\n",
        "    lr_scheduler_stage2 = optim.lr_scheduler.StepLR(optimizer_stage2, step_size=10, gamma=0.1)\n",
        "\n",
        "    checkpoint_stage2 = f\"./{backbone_name}_stage2_deepdrid.pth\"\n",
        "    model_stage2, best_kappa_stage2 = train_model(\n",
        "        model_stage2,\n",
        "        train_loader_stage2,\n",
        "        val_loader_stage2,\n",
        "        device,\n",
        "        criterion,\n",
        "        optimizer_stage2,\n",
        "        lr_scheduler_stage2,\n",
        "        num_epochs=5,\n",
        "        checkpoint_path=checkpoint_stage2\n",
        "    )\n",
        "\n",
        "    print(f\"Stage 2 completed. Best Kappa score on DeepDRiD dataset: {best_kappa_stage2}\")\n",
        "    results_2_stage[f\"{backbone_name}_single_{aug_name}\"] = best_kappa_stage2\n",
        "    torch.save(model_stage2.state_dict(), f\"./{backbone_name}_stage2_final.pth\")\n",
        "    best_model_name = max(results_2_stage, key=results_2_stage.get)\n",
        "    print(f\"Best model: {best_model_name} with score {results_2_stage[best_model_name]}\")\n",
        "    print(\"Results:\", results_2_stage)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpagKfyzPkQc"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "sorted_results = sorted(results_2_stage.items(), key=lambda x: x[1], reverse=True)\n",
        "transformed_array = [(a.split('_')[0], a.split('_')[1], a.split('_')[2], b) for a, b in sorted_results]\n",
        "\n",
        "# Print the sorted results as a table\n",
        "print(tabulate(transformed_array, headers=[\"Model\", \"Mode\", \"Augmentation\", \"Kappa\"], tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMCWFtF2HqYG"
      },
      "source": [
        "# Task B Implementation2: Two stage training *with* pretrained weights\n",
        "\n",
        "Same as task B but using pretrained weights to skip training stage 1 to save time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PJbx3-SILA5m"
      },
      "outputs": [],
      "source": [
        "# download pretained weights folder\n",
        "!pip install gdown\n",
        "file_id = \"1vjR2rCvkv-BdUCRd-KD_49fXZ0QlXo1q\"\n",
        "url = f\"https://drive.google.com/uc?id=1vjR2rCvkv-BdUCRd-KD_49fXZ0QlXo1q\"\n",
        "output = \"pretrained.zip\"\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skabeVQRQJ29"
      },
      "outputs": [],
      "source": [
        "!unzip pretrained.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyApvg_EO0Ag"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# models_to_test = ['resnet18', 'resnet34', 'vgg16']\n",
        "models_to_test = ['resnet34']\n",
        "results_with_pretained = {}\n",
        "\n",
        " # Default transform_train for reference\n",
        "default_transform_train = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomCrop((210, 210)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "augmentations = [\n",
        "        (\"With SLO Padding\", transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            SLORandomPad((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ]))\n",
        "    ]\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "for backbone_name in models_to_test:\n",
        "        for mode in ['single']:\n",
        "            for aug_name, transform_train in augmentations:\n",
        "\n",
        "                # Load datasets with augmentations\n",
        "                train_dataset = RetinopathyDataset('./DeepDRiD/train.csv', './DeepDRiD/train/', transform_train, mode)\n",
        "                val_dataset = RetinopathyDataset('./DeepDRiD/val.csv', './DeepDRiD/val/', transform_test, mode)\n",
        "                test_dataset = RetinopathyDataset('./DeepDRiD/test.csv', './DeepDRiD/test/', transform_test, mode, test=True)\n",
        "\n",
        "                train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "                val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "                test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "                checkpoint_path = f\"./{backbone_name}_{mode}_{aug_name.replace(' ', '_')}.pth\"\n",
        "                '''\n",
        "                # Check if the checkpoint already exists\n",
        "                if os.path.exists(checkpoint_path):\n",
        "                    print(f\"Checkpoint for {backbone_name} in {mode} mode with {aug_name} augmentation already exists. Skipping training.\")\n",
        "                    # Optionally, you can load the model here if you want to continue from the checkpoint\n",
        "                    # model.load_state_dict(torch.load(checkpoint_path))\n",
        "                    continue  # Skip this iteration if the checkpoint exists\n",
        "                '''\n",
        "                model = get_model(backbone_name, mode).to(device)\n",
        "\n",
        "\n",
        "                print(\"Model keys:\", model.state_dict().keys())\n",
        "\n",
        "                state_dict = torch.load(f\"pretrained/{backbone_name}.pth\")\n",
        "                print(\"Weight keys:\", state_dict.keys())\n",
        "\n",
        "\n",
        "                # load weights on model\n",
        "                state_dict = torch.load(f\"pretrained/{backbone_name}.pth\")\n",
        "                renamed_state_dict = {f\"backbone.{k}\": v for k, v in state_dict.items()}\n",
        "                model.load_state_dict(renamed_state_dict, strict=False)\n",
        "\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "                lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "                print(f\"Training {backbone_name} in {mode} mode with {aug_name} augmentations...\")\n",
        "                checkpoint_path = f\"./{backbone_name}_{mode}_{aug_name.replace(' ', '_')}.pth\"\n",
        "                model, best_kappa = train_model(model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=25, checkpoint_path=checkpoint_path)\n",
        "                results_with_pretained[f\"{backbone_name}_{mode}_{aug_name}\"] = best_kappa\n",
        "\n",
        " # Summarize results\n",
        "best_model_name_b = max(results_with_pretained, key=results_with_pretained.get)\n",
        "print(f\"Best model: {best_model_name_b} with score { results_with_pretained[best_model_name_b]}\")\n",
        "print(\"Results:\", results_with_pretained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA7OFq3Gb4WK"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "sorted_results = sorted(results_with_pretained.items(), key=lambda x: x[1], reverse=True)\n",
        "transformed_array = [(a.split('_')[0], a.split('_')[1], a.split('_')[2], b) for a, b in sorted_results]\n",
        "\n",
        "# Print the sorted results as a table\n",
        "print(tabulate(transformed_array, headers=[\"Model\", \"Mode\", \"Augmentation\", \"Kappa\"], tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48Z3MSZdoDi-"
      },
      "source": [
        "Combine with task E for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVP_rEn6C3Q9"
      },
      "outputs": [],
      "source": [
        "# Plot the result for analysis\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "evaluate_model(model, test_loader, device, test_only=True)\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history['train_accuracy'], label='Train Accuracy', marker='o')\n",
        "    plt.plot(history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss', marker='o')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvrrGjr9Cbcf"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=25,\n",
        "                checkpoint_path='model.pth'):\n",
        "    best_model = model.state_dict()\n",
        "    best_epoch = None\n",
        "    best_val_kappa = -1.0  # Initialize the best kappa score\n",
        "\n",
        "    # Initialize containers for storing metrics\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_accuracy': [],\n",
        "        'val_loss': [],\n",
        "        'val_accuracy': [],\n",
        "        'val_kappa': [],  # Store kappa scores for validation\n",
        "        'best_kappa': None,  # Best kappa score\n",
        "        'best_epoch': None,  # Epoch with best kappa\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f'\\nEpoch {epoch}/{num_epochs}')\n",
        "        running_loss = []\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        with tqdm(total=len(train_loader), desc=f'Training', unit=' batch', file=sys.stdout) as pbar:\n",
        "            for images, labels in train_loader:\n",
        "                if not isinstance(images, list):\n",
        "                    images = images.to(device)  # single image case\n",
        "                else:\n",
        "                    images = [x.to(device) for x in images]  # dual images case\n",
        "\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                preds = torch.argmax(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "                pbar.set_postfix({'lr': f'{optimizer.param_groups[0][\"lr\"]:.1e}', 'Loss': f'{loss.item():.4f}'})\n",
        "                pbar.update(1)\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # Calculate epoch-level metrics\n",
        "        epoch_loss = sum(running_loss) / len(running_loss)\n",
        "        train_metrics = compute_metrics(all_preds, all_labels, per_class=False)\n",
        "        train_kappa, train_accuracy, _, _ = train_metrics[:4]\n",
        "\n",
        "        print(f'[Train] Kappa: {train_kappa:.4f} Accuracy: {train_accuracy:.4f} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        # Store training metrics\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_accuracy'].append(train_accuracy)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_metrics = evaluate_model(model, val_loader, device, return_loss=True)\n",
        "        val_kappa, val_accuracy, _, _ = val_metrics[:4]\n",
        "        print(f'[Val] Kappa: {val_kappa:.4f} Accuracy: {val_accuracy:.4f} Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Store validation metrics\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_accuracy'].append(val_accuracy)\n",
        "        history['val_kappa'].append(val_kappa)\n",
        "\n",
        "        # Save the best model\n",
        "        if val_kappa > best_val_kappa:\n",
        "            best_val_kappa = val_kappa\n",
        "            best_epoch = epoch\n",
        "            best_model = model.state_dict()\n",
        "            torch.save(best_model, checkpoint_path)\n",
        "\n",
        "    # Store best kappa and epoch in history\n",
        "    history['best_kappa'] = best_val_kappa\n",
        "    history['best_epoch'] = best_epoch\n",
        "\n",
        "    print(f'[Val] Best kappa: {best_val_kappa:.4f}, Epoch {best_epoch}')\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, device, test_only=False, prediction_path='./test_predictions.csv', return_loss=False):\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_image_ids = []\n",
        "    running_loss = []  # To calculate loss if return_loss is True\n",
        "\n",
        "    with tqdm(total=len(test_loader), desc=f'Evaluating', unit=' batch', file=sys.stdout) as pbar:\n",
        "        for i, data in enumerate(test_loader):\n",
        "\n",
        "            if test_only:\n",
        "                images = data\n",
        "            else:\n",
        "                images, labels = data\n",
        "\n",
        "            if not isinstance(images, list):\n",
        "                images = images.to(device)  # single image case\n",
        "            else:\n",
        "                images = [x.to(device) for x in images]  # dual images case\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(images)\n",
        "                preds = torch.argmax(outputs, 1)\n",
        "\n",
        "                if return_loss and not test_only:\n",
        "                    loss = nn.CrossEntropyLoss()(outputs, labels.to(device).long())\n",
        "                    running_loss.append(loss.item())\n",
        "\n",
        "            if not isinstance(images, list):\n",
        "                # Single image case\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                image_ids = [\n",
        "                    os.path.basename(test_loader.dataset.data[idx]['img_path']) for idx in\n",
        "                    range(i * test_loader.batch_size, i * test_loader.batch_size + len(images))\n",
        "                ]\n",
        "                all_image_ids.extend(image_ids)\n",
        "                if not test_only:\n",
        "                    all_labels.extend(labels.numpy())\n",
        "            else:\n",
        "                # Dual images case\n",
        "                for k in range(2):\n",
        "                    all_preds.extend(preds.cpu().numpy())\n",
        "                    image_ids = [\n",
        "                        os.path.basename(test_loader.dataset.data[idx][f'img_path{k + 1}']) for idx in\n",
        "                        range(i * test_loader.batch_size, i * test_loader.batch_size + len(images[k]))\n",
        "                    ]\n",
        "                    all_image_ids.extend(image_ids)\n",
        "                    if not test_only:\n",
        "                        all_labels.extend(labels.numpy())\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    # Save predictions to CSV file for Kaggle online evaluation\n",
        "    if test_only:\n",
        "        df = pd.DataFrame({\n",
        "            'ID': all_image_ids,\n",
        "            'TARGET': all_preds\n",
        "        })\n",
        "        df.to_csv(prediction_path, index=False)\n",
        "        print(f'[Test] Predictions saved to {os.path.abspath(prediction_path)}')\n",
        "    else:\n",
        "        metrics = compute_metrics(all_preds, all_labels)\n",
        "        if return_loss:\n",
        "            avg_loss = sum(running_loss) / len(running_loss) if running_loss else None\n",
        "            return avg_loss, metrics\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Faflw8FXoA0x"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# models_to_test = ['resnet18', 'resnet34', 'vgg16']\n",
        "models_to_test = ['resnet18']\n",
        "results_with_pretained = {}\n",
        "\n",
        " # Default transform_train for reference\n",
        "default_transform_train = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomCrop((210, 210)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "augmentations = [\n",
        "        (\"With SLO Padding\", transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            SLORandomPad((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ]))\n",
        "    ]\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "for backbone_name in models_to_test:\n",
        "        for mode in ['single']:\n",
        "            for aug_name, transform_train in augmentations:\n",
        "\n",
        "                # Load datasets with augmentations\n",
        "                train_dataset = RetinopathyDataset('./DeepDRiD/train.csv', './DeepDRiD/train/', transform_train, mode)\n",
        "                val_dataset = RetinopathyDataset('./DeepDRiD/val.csv', './DeepDRiD/val/', transform_test, mode)\n",
        "                test_dataset = RetinopathyDataset('./DeepDRiD/test.csv', './DeepDRiD/test/', transform_test, mode, test=True)\n",
        "\n",
        "                train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "                val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "                test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "                checkpoint_path = f\"./{backbone_name}_{mode}_{aug_name.replace(' ', '_')}.pth\"\n",
        "                '''\n",
        "                # Check if the checkpoint already exists\n",
        "                if os.path.exists(checkpoint_path):\n",
        "                    print(f\"Checkpoint for {backbone_name} in {mode} mode with {aug_name} augmentation already exists. Skipping training.\")\n",
        "                    # Optionally, you can load the model here if you want to continue from the checkpoint\n",
        "                    # model.load_state_dict(torch.load(checkpoint_path))\n",
        "                    continue  # Skip this iteration if the checkpoint exists\n",
        "                '''\n",
        "                model = get_model(backbone_name, mode).to(device)\n",
        "\n",
        "\n",
        "                print(\"Model keys:\", model.state_dict().keys())\n",
        "\n",
        "                state_dict = torch.load(f\"pretrained/{backbone_name}.pth\")\n",
        "                print(\"Weight keys:\", state_dict.keys())\n",
        "\n",
        "\n",
        "                # load weights on model\n",
        "                state_dict = torch.load(f\"pretrained/{backbone_name}.pth\")\n",
        "                renamed_state_dict = {f\"backbone.{k}\": v for k, v in state_dict.items()}\n",
        "                model.load_state_dict(renamed_state_dict, strict=False)\n",
        "\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "                lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "                print(f\"Training {backbone_name} in {mode} mode with {aug_name} augmentations...\")\n",
        "                checkpoint_path = f\"./{backbone_name}_{mode}_{aug_name.replace(' ', '_')}.pth\"\n",
        "                model, history = train_model(model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=25, checkpoint_path=checkpoint_path)\n",
        "                results_with_pretained[f\"{backbone_name}_{mode}_{aug_name}\"] =history['best_kappa']\n",
        "                # Plot history\n",
        "                evaluate_model(model, test_loader, device, test_only=True)\n",
        "                plot_history(history)\n",
        "\n",
        " # Summarize results\n",
        "best_model_name_b = max(results_with_pretained, key=results_with_pretained.get)\n",
        "print(f\"Best model: {best_model_name_b} with score { results_with_pretained[best_model_name_b]}\")\n",
        "print(\"Results:\", results_with_pretained)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdRi8Zdsu4C3"
      },
      "source": [
        "# Task C: Incorporate attention mechanisms in the model\n",
        "1. Implement attention mechanisms (e.g., self-attention, channel attention, or spatial attention)\n",
        "in your DeepDRiD model architecture.\n",
        "2. Evaluate the impact of attention mechanisms on model performance.\n",
        "\n",
        "The goal of task C is to apply and see how attention is affecting the model and its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N63WOPswu6fO"
      },
      "outputs": [],
      "source": [
        "# Self-Attention Model\n",
        "class MyModelSelfAttention(nn.Module):\n",
        "    def __init__(self, backbone, num_classes=5, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = backbone\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        self.query = nn.Conv2d(512, 64, kernel_size=1)\n",
        "        self.key = nn.Conv2d(512, 64, kernel_size=1)\n",
        "        self.value = nn.Conv2d(512, 512, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def self_attention(self, x):\n",
        "        batch_size, C, H, W = x.size()\n",
        "        query = self.query(x).view(batch_size, -1, H * W).permute(0, 2, 1)\n",
        "        key = self.key(x).view(batch_size, -1, H * W)\n",
        "        energy = torch.bmm(query, key)\n",
        "        attention = F.softmax(energy, dim=-1)\n",
        "        value = self.value(x).view(batch_size, -1, H * W)\n",
        "\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, H, W)\n",
        "        return self.gamma * out + x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.self_attention(x.unsqueeze(-1).unsqueeze(-1))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Channel-Attention Model\n",
        "class MyModelChannelAttention(nn.Module):\n",
        "    def __init__(self, backbone, num_classes=5, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = backbone\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        self.fc1 = nn.Conv2d(512, 32, kernel_size=1)\n",
        "        self.fc2 = nn.Conv2d(32, 512, kernel_size=1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def channel_attention(self, x):\n",
        "        avg_pool = F.adaptive_avg_pool2d(x, 1)\n",
        "        max_pool = F.adaptive_max_pool2d(x, 1)\n",
        "        avg_out = self.fc2(F.relu(self.fc1(avg_pool)))\n",
        "        max_out = self.fc2(F.relu(self.fc1(max_pool)))\n",
        "        scale = torch.sigmoid(avg_out + max_out)\n",
        "        return x * scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.channel_attention(x.unsqueeze(-1).unsqueeze(-1))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Spatial-Attention Model\n",
        "class MyModelSpatialAttention(nn.Module):\n",
        "    def __init__(self, backbone, num_classes=5, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = backbone\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def spatial_attention(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        scale = torch.sigmoid(self.conv(torch.cat([avg_out, max_out], dim=1)))\n",
        "        return x * scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.spatial_attention(x.unsqueeze(-1).unsqueeze(-1))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLko7yBcyKQ1"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from tabulate import tabulate\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Fixed parameters\n",
        "backbone_name = \"resnet18\"\n",
        "mode = \"single\"  # Assuming single input for simplicity\n",
        "aug_name = \"With SLO Padding\"\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    SLORandomPad((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = RetinopathyDataset('./DeepDRiD/train.csv', './DeepDRiD/train/', transform_train, mode)\n",
        "val_dataset = RetinopathyDataset('./DeepDRiD/val.csv', './DeepDRiD/val/', transform_test, mode)\n",
        "test_dataset = RetinopathyDataset('./DeepDRiD/test.csv', './DeepDRiD/test/', transform_test, mode, test=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define attention-enhanced models\n",
        "attention_models = {\n",
        "    \"SelfAttention\": MyModelSelfAttention,\n",
        "    \"ChannelAttention\": MyModelChannelAttention,\n",
        "    \"SpatialAttention\": MyModelSpatialAttention\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for attn_name, model_class in attention_models.items():\n",
        "    print(f\"Testing {backbone_name} with {attn_name} attention...\")\n",
        "\n",
        "    # Instantiate model\n",
        "    backbone = get_model(backbone_name, mode)  # Load backbone (e.g., ResNet18)\n",
        "    model = model_class(backbone=backbone).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "    # Define checkpoint path\n",
        "    checkpoint_path = f\"./{backbone_name}_{mode}_{aug_name.replace(' ', '_')}_{attn_name}.pth\"\n",
        "\n",
        "    # Skip training if checkpoint exists\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Checkpoint for {attn_name} attention already exists. Skipping training.\")\n",
        "        model.load_state_dict(torch.load(checkpoint_path))\n",
        "    else:\n",
        "        # Train the model\n",
        "        model, best_kappa = train_model(model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=5, checkpoint_path=checkpoint_path)\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        results[attn_name] = best_kappa\n",
        "        print(f\"{attn_name} Attention Test Kappa: {best_kappa:.4f}\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(model, test_loader, device, test_only=True)\n",
        "\n",
        "\n",
        "# Summarize results\n",
        "best_attention = max(results, key=results.get)\n",
        "print(f\"Best Attention Mechanism: {best_attention} with Kappa: {results[best_attention]:.4f}\")\n",
        "print(\"Results:\")\n",
        "print(tabulate([(k, v) for k, v in results.items()], headers=[\"Attention\", \"Kappa\"], tablefmt=\"pretty\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sljqvivDbVhz"
      },
      "outputs": [],
      "source": [
        "# Instantiate model\n",
        "backbone = get_model(backbone_name, mode)  # Load backbone (e.g., ResNet18)\n",
        "model = MyModelSelfAttention(backbone=backbone).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "\n",
        "        # Train the model\n",
        "model, best_kappa = train_model(model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=num_epochs, checkpoint_path=checkpoint_path)\n",
        "torch.save(model.state_dict(), checkpoint_path)\n",
        "results[attn_name] = best_kappa\n",
        "print(f\"{attn_name} Attention Test Kappa: {best_kappa:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader, device, test_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2eUv5nefzOK"
      },
      "source": [
        "# Task D: Compare the performance of different models and strategies\n",
        "1. Use at least three transfer models that you've trained using task B and perform ensemble learning. Try out the following ensemble techniques (Stacking, Boosting, Weighted Average, Max Voting, Bagging) and analyze whether the performance increases or not.\n",
        "2. Try out different image preprocessing techniques such as, Ben Graham, Circle Cropping, CLAHE, adding gaussian blur, sharpening up the images etc.\n",
        "\n",
        "The goal of task D is to perform ensemble learning by training various models and combining their predictions and analyzing whether it boosts the performance. Along with that, applying multiple preprocessing techniques to see if that has any effect on the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIE8VMzK0Fwl"
      },
      "outputs": [],
      "source": [
        "# Download the model weights trained from Task B\n",
        "import gdown\n",
        "file_id = \"1lsF9AD0s9-cLdBa4ROj9_x-JGpB4stcu\"\n",
        "url = f\"https://drive.google.com/uc?id=1lsF9AD0s9-cLdBa4ROj9_x-JGpB4stcu\"\n",
        "output = \"saved_model_task_b.zip\"\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOCtvBu-0jUs"
      },
      "outputs": [],
      "source": [
        "!unzip saved_model_task_b.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bqbtcA0JSH-"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name, num_classes, device):\n",
        "    model = get_model(model_name, 'single', num_classes)  # assuming mode is always 'single'\n",
        "    return model.to(device)\n",
        "\n",
        "def load_model_weights(model, weights_path, device='cpu'):\n",
        "    # Load the saved state dict into the model\n",
        "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
        "    model.to(device)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3byIxhLf5q6"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Task D: Compare performance using preprocessing and ensemble learning\n",
        "\n",
        "mode = 'single'\n",
        "\n",
        "# Load and Train Models with Preprocessing and Ensembles\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Preprocessing Functions\n",
        "def ben_graham_preprocessing(image):\n",
        "    return cv2.GaussianBlur(image, (5, 5), 0)\n",
        "\n",
        "def circle_cropping(image):\n",
        "    h, w = image.shape[:2]\n",
        "    center = (int(w / 2), int(h / 2))\n",
        "    radius = min(center[0], center[1], h - center[1], w - center[0])\n",
        "    mask = np.zeros((h, w), dtype=np.uint8)\n",
        "    cv2.circle(mask, center, radius, (255, 255, 255), -1)\n",
        "    return cv2.bitwise_and(image, image, mask=mask)\n",
        "\n",
        "def apply_clahe(image):\n",
        "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    cl = clahe.apply(l)\n",
        "    return cv2.merge((cl, a, b))\n",
        "\n",
        "def sharpen_image(image):\n",
        "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
        "    return cv2.filter2D(image, -1, kernel)\n",
        "\n",
        "# Ensemble Model\n",
        "class EnsembleModel(nn.Module):\n",
        "    def __init__(self, models, ensemble_type='weighted_average', weights=None):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.models = nn.ModuleList(models)  # Ensure models are registered as submodules\n",
        "        self.ensemble_type = ensemble_type\n",
        "        self.weights = weights if weights else [1 / len(models)] * len(models)\n",
        "\n",
        "    def forward(self, x):\n",
        "        predictions = [model(x) for model in self.models]\n",
        "\n",
        "        if self.ensemble_type == 'weighted_average':\n",
        "            weighted_preds = torch.stack(predictions, dim=0) * torch.tensor(self.weights).view(-1, 1, 1).to(x.device)\n",
        "            return weighted_preds.sum(dim=0)\n",
        "        elif self.ensemble_type == 'max_voting':\n",
        "            stacked_preds = torch.stack(predictions, dim=0)\n",
        "            return torch.mode(stacked_preds, dim=0)[0]\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown ensemble type: {self.ensemble_type}\")\n",
        "\n",
        "# Load datasets (Assumed that RetinopathyDataset is already implemented)\n",
        "train_dataset = RetinopathyDataset('./DeepDRiD/train.csv', './DeepDRiD/train/', transform_train, mode)\n",
        "val_dataset = RetinopathyDataset('./DeepDRiD/val.csv', './DeepDRiD/val/', transform_test, mode)\n",
        "test_dataset = RetinopathyDataset('./DeepDRiD/test.csv', './DeepDRiD/test/', transform_test, mode, test=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Preprocessing functions (not used here directly but can be applied on images)\n",
        "preprocess_functions = [\n",
        "    (\"Ben Graham\", ben_graham_preprocessing),\n",
        "    (\"Circle Cropping\", circle_cropping),\n",
        "    (\"CLAHE\", apply_clahe),\n",
        "    (\"Sharpening\", sharpen_image)\n",
        "]\n",
        "\n",
        "# List of models to test\n",
        "models_list = [\n",
        "    get_model('resnet18', 'single', num_classes=5),  # Use get_model to create the model\n",
        "    get_model('resnet34', 'single', num_classes=5),\n",
        "    get_model('vgg16', 'single', num_classes=5)\n",
        "]\n",
        "\n",
        "# Load model weights for each model\n",
        "model_weights_paths = [\n",
        "    'saved_model_task_b/resnet18_single_With_SLO_Padding.pth',\n",
        "    'saved_model_task_b/resnet34_single_With_SLO_Padding.pth',\n",
        "    'saved_model_task_b/vgg16_single_With_SLO_Padding.pth'\n",
        "]\n",
        "\n",
        "# Load weights for each model and ensure they are on the correct device\n",
        "for model, weights_path in zip(models_list, model_weights_paths):\n",
        "    model = load_model_weights(model, weights_path, device)\n",
        "    model.to(device)  # Ensure the model is on the correct device\n",
        "\n",
        "# Ensemble types to test\n",
        "ensemble_results = []\n",
        "ensemble_types = ['weighted_average']\n",
        "for ensemble_type in ensemble_types:\n",
        "    print(f\"Evaluating with ensemble type: {ensemble_type}\")\n",
        "\n",
        "    # Initialize ensemble model\n",
        "    ensemble_model = EnsembleModel(models_list, ensemble_type).to(device)\n",
        "\n",
        "    # Set optimizer for the ensemble model (which now correctly has parameters)\n",
        "    optimizer = torch.optim.Adam(ensemble_model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    # Train ensemble model using your `train_model` function\n",
        "    ensemble_model, history = train_model(\n",
        "        ensemble_model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=15,\n",
        "        checkpoint_path='ensemble_model.pth'\n",
        "    )\n",
        "\n",
        "    # Evaluate ensemble model using `evaluate_model` function\n",
        "    val_loss, val_metrics = evaluate_model(ensemble_model, val_loader, device, return_loss=True)\n",
        "    val_kappa, val_accuracy, _, _ = val_metrics[:4]\n",
        "\n",
        "    print(f\"[Val] Ensemble Model Kappa: {val_kappa:.4f} Accuracy: {val_accuracy:.4f} Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Store results\n",
        "    ensemble_results.append({\n",
        "        'type': ensemble_type,\n",
        "        'kappa': val_kappa,\n",
        "        'accuracy': val_accuracy,\n",
        "        'loss': val_loss,\n",
        "        'history': history\n",
        "    })\n",
        "    evaluate_model(ensemble_model, test_loader, device, test_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxJzCL2BbKIz"
      },
      "outputs": [],
      "source": [
        "# Sort results by kappa\n",
        "sorted_results = sorted(ensemble_results, key=lambda x: x['kappa'], reverse=True)\n",
        "\n",
        "# Print sorted kappas\n",
        "print(\"\\nSorted Ensemble Model Kappa Scores:\")\n",
        "for result in sorted_results:\n",
        "    print(f\"Ensemble Type: {result['type']} | Kappa: {result['kappa']:.4f}\")\n",
        "\n",
        "# Plot accuracy and loss for each ensemble type\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Accuracy Plot\n",
        "for result in ensemble_results:\n",
        "    history = result['history']\n",
        "    axes[0].plot(history['train_accuracy'], label=f\"Train ({result['type']})\")\n",
        "    axes[0].plot(history['val_accuracy'], label=f\"Val ({result['type']})\", linestyle='--')\n",
        "axes[0].set_title(\"Accuracy over Epochs\")\n",
        "axes[0].set_xlabel(\"Epochs\")\n",
        "axes[0].set_ylabel(\"Accuracy\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Loss Plot\n",
        "for result in ensemble_results:\n",
        "    history = result['history']\n",
        "    axes[1].plot(history['train_loss'], label=f\"Train ({result['type']})\")\n",
        "    axes[1].plot(history['val_loss'], label=f\"Val ({result['type']})\", linestyle='--')\n",
        "axes[1].set_title(\"Loss over Epochs\")\n",
        "axes[1].set_xlabel(\"Epochs\")\n",
        "axes[1].set_ylabel(\"Loss\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFilter\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Custom Transformations\n",
        "class CLAHETransform:\n",
        "    def __init__(self, clip_limit=1.0, tile_grid_size=(8, 8)):  # Less aggressive CLAHE\n",
        "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = np.array(img)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        img = self.clahe.apply(img)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "        return Image.fromarray(img)\n",
        "\n",
        "class GaussianBlurTransform:\n",
        "    def __init__(self, radius=1):  # Reduced blur radius\n",
        "        self.radius = radius\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return img.filter(ImageFilter.GaussianBlur(self.radius))\n",
        "\n",
        "class CircleCropTransform:\n",
        "    def __init__(self, size=(224, 224), crop_fraction=0.8):  # Slightly less aggressive circle crop\n",
        "        self.size = size\n",
        "        self.crop_fraction = crop_fraction\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = img.resize(self.size)\n",
        "        np_img = np.array(img)\n",
        "        mask = np.zeros_like(np_img)\n",
        "        center = (np_img.shape[1]//2, np_img.shape[0]//2)\n",
        "        radius = int(min(center) * self.crop_fraction)  # Reducing the mask size\n",
        "        cv2.circle(mask, center, radius, (1, 1, 1), -1)\n",
        "        np_img = np_img * mask\n",
        "        return Image.fromarray(np_img)\n",
        "\n",
        "class SharpenTransform:\n",
        "    def __init__(self, strength=1):  # Reduced sharpening strength\n",
        "        self.kernel = np.array([[-1, -1, -1], [-1, 9 * strength,-1], [-1, -1, -1]])\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = np.array(img)\n",
        "        img = cv2.filter2D(img, -1, self.kernel)\n",
        "        return Image.fromarray(img)\n",
        "\n",
        "# Setup transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    SLORandomPad((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "    CLAHETransform(clip_limit=0.5),  # Milder CLAHE\n",
        "    #GaussianBlurTransform(radius=0.5),  # Less aggressive blur\n",
        "    #FundRandomRotate(prob=0.7, degree=45),\n",
        "    CircleCropTransform(crop_fraction=0.5),  # Milder circle cropping\n",
        "    #SharpenTransform(strength=0.5),  # Less sharpening\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Instantiate model and other components\n",
        "backbone_name = \"resnet18\"\n",
        "mode = \"single\"  # Assuming single input for simplicity\n",
        "aug_name = \"SLO_Gaussian\"\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = RetinopathyDataset('./DeepDRiD/train.csv', './DeepDRiD/train/', transform_train, mode)\n",
        "val_dataset = RetinopathyDataset('./DeepDRiD/val.csv', './DeepDRiD/val/', transform_test, mode)\n",
        "test_dataset = RetinopathyDataset('./DeepDRiD/test.csv', './DeepDRiD/test/', transform_test, mode, test=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "model = get_model(backbone_name, mode).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Train the model and record history\n",
        "checkpoint_path = f\"./{backbone_name}{mode}{aug_name.replace(' ', '_')}.pth\"\n",
        "model, history = train_model(\n",
        "    model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=10, checkpoint_path=checkpoint_path\n",
        ")\n",
        "torch.save(model.state_dict(), checkpoint_path)\n"
      ],
      "metadata": {
        "id": "7d9QB3Qy4nJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9wWyjP_k8hA"
      },
      "source": [
        "# Task E Creating Visualizations and Explainable AI.\n",
        "\n",
        "\n",
        "\n",
        "1.   Implement visualizations (e.g., scatter plots and line graphs) for your models' losses and accuracies on training and validation datasets to analyze the convergence and overall performance of the model.\n",
        "2.   Use Explainable AI techniques such as GradCAM to analyze what features in the image are contributing the most and the least in the model's decision-making process. Please also include a few visualization results in the report.\n",
        "The goal of task(e) is to visualize the performance\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hmXjqKHlXZn"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=25,\n",
        "                checkpoint_path='model.pth'):\n",
        "    best_model = model.state_dict()\n",
        "    best_epoch = None\n",
        "    best_val_kappa = -1.0  # Initialize the best kappa score\n",
        "\n",
        "    # Initialize containers for storing metrics\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_accuracy': [],\n",
        "        'val_loss': [],\n",
        "        'val_accuracy': [],\n",
        "        'val_kappa': [],  # Store kappa scores for validation\n",
        "        'best_kappa': None,  # Best kappa score\n",
        "        'best_epoch': None,  # Epoch with best kappa\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f'\\nEpoch {epoch}/{num_epochs}')\n",
        "        running_loss = []\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        with tqdm(total=len(train_loader), desc=f'Training', unit=' batch', file=sys.stdout) as pbar:\n",
        "            for images, labels in train_loader:\n",
        "                if not isinstance(images, list):\n",
        "                    images = images.to(device)  # single image case\n",
        "                else:\n",
        "                    images = [x.to(device) for x in images]  # dual images case\n",
        "\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                preds = torch.argmax(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "                pbar.set_postfix({'lr': f'{optimizer.param_groups[0][\"lr\"]:.1e}', 'Loss': f'{loss.item():.4f}'})\n",
        "                pbar.update(1)\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # Calculate epoch-level metrics\n",
        "        epoch_loss = sum(running_loss) / len(running_loss)\n",
        "        train_metrics = compute_metrics(all_preds, all_labels, per_class=False)\n",
        "        train_kappa, train_accuracy, _, _ = train_metrics[:4]\n",
        "\n",
        "        print(f'[Train] Kappa: {train_kappa:.4f} Accuracy: {train_accuracy:.4f} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        # Store training metrics\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_accuracy'].append(train_accuracy)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_metrics = evaluate_model(model, val_loader, device, return_loss=True)\n",
        "        val_kappa, val_accuracy, _, _ = val_metrics[:4]\n",
        "        print(f'[Val] Kappa: {val_kappa:.4f} Accuracy: {val_accuracy:.4f} Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Store validation metrics\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_accuracy'].append(val_accuracy)\n",
        "        history['val_kappa'].append(val_kappa)\n",
        "\n",
        "        # Save the best model\n",
        "        if val_kappa > best_val_kappa:\n",
        "            best_val_kappa = val_kappa\n",
        "            best_epoch = epoch\n",
        "            best_model = model.state_dict()\n",
        "            torch.save(best_model, checkpoint_path)\n",
        "\n",
        "    # Store best kappa and epoch in history\n",
        "    history['best_kappa'] = best_val_kappa\n",
        "    history['best_epoch'] = best_epoch\n",
        "\n",
        "    print(f'[Val] Best kappa: {best_val_kappa:.4f}, Epoch {best_epoch}')\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, device, test_only=False, prediction_path='./test_predictions.csv', return_loss=False):\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_image_ids = []\n",
        "    running_loss = []  # To calculate loss if return_loss is True\n",
        "\n",
        "    with tqdm(total=len(test_loader), desc=f'Evaluating', unit=' batch', file=sys.stdout) as pbar:\n",
        "        for i, data in enumerate(test_loader):\n",
        "\n",
        "            if test_only:\n",
        "                images = data\n",
        "            else:\n",
        "                images, labels = data\n",
        "\n",
        "            if not isinstance(images, list):\n",
        "                images = images.to(device)  # single image case\n",
        "            else:\n",
        "                images = [x.to(device) for x in images]  # dual images case\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(images)\n",
        "                preds = torch.argmax(outputs, 1)\n",
        "\n",
        "                if return_loss and not test_only:\n",
        "                    loss = nn.CrossEntropyLoss()(outputs, labels.to(device).long())\n",
        "                    running_loss.append(loss.item())\n",
        "\n",
        "            if not isinstance(images, list):\n",
        "                # Single image case\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                image_ids = [\n",
        "                    os.path.basename(test_loader.dataset.data[idx]['img_path']) for idx in\n",
        "                    range(i * test_loader.batch_size, i * test_loader.batch_size + len(images))\n",
        "                ]\n",
        "                all_image_ids.extend(image_ids)\n",
        "                if not test_only:\n",
        "                    all_labels.extend(labels.numpy())\n",
        "            else:\n",
        "                # Dual images case\n",
        "                for k in range(2):\n",
        "                    all_preds.extend(preds.cpu().numpy())\n",
        "                    image_ids = [\n",
        "                        os.path.basename(test_loader.dataset.data[idx][f'img_path{k + 1}']) for idx in\n",
        "                        range(i * test_loader.batch_size, i * test_loader.batch_size + len(images[k]))\n",
        "                    ]\n",
        "                    all_image_ids.extend(image_ids)\n",
        "                    if not test_only:\n",
        "                        all_labels.extend(labels.numpy())\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    # Save predictions to CSV file for Kaggle online evaluation\n",
        "    if test_only:\n",
        "        df = pd.DataFrame({\n",
        "            'ID': all_image_ids,\n",
        "            'TARGET': all_preds\n",
        "        })\n",
        "        df.to_csv(prediction_path, index=False)\n",
        "        print(f'[Test] Predictions saved to {os.path.abspath(prediction_path)}')\n",
        "    else:\n",
        "        metrics = compute_metrics(all_preds, all_labels)\n",
        "        if return_loss:\n",
        "            avg_loss = sum(running_loss) / len(running_loss) if running_loss else None\n",
        "            return avg_loss, metrics\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EKg7LL9lciJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Setup transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    SLORandomPad((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Instantiate model and other components\n",
        "backbone_name = \"resnet18\"\n",
        "mode = \"single\"  # Assuming single input for simplicity\n",
        "aug_name = \"With SLO Padding\"\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = RetinopathyDataset('./DeepDRiD/train.csv', './DeepDRiD/train/', transform_train, mode)\n",
        "val_dataset = RetinopathyDataset('./DeepDRiD/val.csv', './DeepDRiD/val/', transform_test, mode)\n",
        "test_dataset = RetinopathyDataset('./DeepDRiD/test.csv', './DeepDRiD/test/', transform_test, mode, test=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "backbone = get_model(backbone_name, mode)  # Load backbone (e.g., ResNet34)\n",
        "model = MyModelSelfAttention(backbone=backbone).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Train the model and record history\n",
        "checkpoint_path = f\"./{backbone_name}{mode}{aug_name.replace(' ', '_')}.pth\"\n",
        "model, history = train_model(\n",
        "    model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=10, checkpoint_path=checkpoint_path\n",
        ")\n",
        "torch.save(model.state_dict(), checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3QxpeXn_ty3"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history['train_accuracy'], label='Train Accuracy', marker='o')\n",
        "    plt.plot(history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss', marker='o')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qG8Y-Ii3ywi"
      },
      "outputs": [],
      "source": [
        "evaluate_model(model, test_loader, device, test_only=True)\n",
        "plot_history(history)\n",
        "# Plot training and validation accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t8ZKuUblSB9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SflZYg9QvV0D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer_name):\n",
        "        self.model = model\n",
        "        self.target_layer_name = target_layer_name\n",
        "        self.activations = None\n",
        "        self.gradients = None\n",
        "\n",
        "        # Register hooks\n",
        "        self.hook_layers()\n",
        "\n",
        "    def hook_layers(self):\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations = output.detach()  # Store activations\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients = grad_output[0]  # Store gradients\n",
        "\n",
        "        target_layer = dict(self.model.named_modules())[self.target_layer_name]\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def __call__(self, input_tensor, target_class=None):\n",
        "        input_tensor.requires_grad = True\n",
        "        output = self.model(input_tensor)\n",
        "\n",
        "        if target_class is None:\n",
        "            target_class = torch.argmax(output, dim=1).item()\n",
        "\n",
        "        # Backward pass to get gradients\n",
        "        self.model.zero_grad()\n",
        "        target = output[0][target_class]\n",
        "        target.backward()\n",
        "\n",
        "        # Check if gradients and activations are captured\n",
        "        if self.gradients is None or self.activations is None:\n",
        "            raise ValueError(\"Gradients or activations are None. Make sure hooks are correctly set.\")\n",
        "\n",
        "        heatmap = self.generate_heatmap(self.gradients, self.activations)\n",
        "        return heatmap\n",
        "\n",
        "    def generate_heatmap(self, gradients, activations):\n",
        "        # Pool gradients across spatial dimensions\n",
        "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
        "\n",
        "        # Weight activations by gradients\n",
        "        activations = activations[0]  # Remove batch dimension\n",
        "        for i in range(len(pooled_gradients)):\n",
        "            activations[i, :, :] *= pooled_gradients[i]\n",
        "\n",
        "        # Create heatmap by averaging the weighted activations\n",
        "        heatmap = torch.mean(activations, dim=0).cpu().numpy()\n",
        "        heatmap = np.maximum(heatmap, 0)  # Apply ReLU to heatmap\n",
        "\n",
        "        # Resize heatmap to match the input image size (224, 224)\n",
        "        heatmap = Image.fromarray(heatmap)  # Convert to PIL Image\n",
        "        heatmap = heatmap.resize((224, 224), Image.ANTIALIAS)  # Resize without cv2\n",
        "        heatmap = np.array(heatmap)  # Convert back to numpy array\n",
        "\n",
        "        # Normalize heatmap\n",
        "        heatmap = heatmap / np.max(heatmap)\n",
        "        return heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P09Wq-6rvre3"
      },
      "outputs": [],
      "source": [
        "def visualize_gradcam(input_image, heatmap, alpha=0.5):\n",
        "    # Convert heatmap to RGB\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = np.stack([heatmap] * 3, axis=2)  # Convert to 3-channel\n",
        "    heatmap = to_pil_image(heatmap)\n",
        "    heatmap = heatmap.resize(input_image.size, resample=Image.BILINEAR)\n",
        "\n",
        "    # Superimpose heatmap on the original image\n",
        "    heatmap = np.array(heatmap)\n",
        "    input_image = np.array(input_image)\n",
        "    superimposed_image = np.uint8(input_image * (1 - alpha) + heatmap * alpha)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.imshow(input_image)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"Grad-CAM Heatmap\")\n",
        "    plt.imshow(heatmap)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"Overlay\")\n",
        "    plt.imshow(superimposed_image)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MRY95GVzEmA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_gradcam_full(image, heatmaps, layer_names, num_columns=5):\n",
        "    \"\"\"\n",
        "    Visualize multiple GradCAM heatmaps in a grid layout with a specified number of columns.\n",
        "\n",
        "    Args:\n",
        "        image: The original image.\n",
        "        heatmaps: A list of heatmaps generated by GradCAM for each layer.\n",
        "        layer_names: A list of layer names corresponding to the heatmaps.\n",
        "        num_columns: Number of columns for the grid layout (default is 3).\n",
        "    \"\"\"\n",
        "    num_layers = len(heatmaps)\n",
        "    rows = (num_layers // num_columns) + (num_layers % num_columns != 0)  # Calculate rows for given columns\n",
        "\n",
        "    # Create a figure with a grid of subplots\n",
        "    fig, axes = plt.subplots(rows, num_columns, figsize=(num_columns * 5, 5 * rows))  # Adjust the size as needed\n",
        "    axes = axes.flatten()  # Flatten the axes to make it easier to index\n",
        "\n",
        "    # Show the original image in the first subplot\n",
        "    axes[0].imshow(np.array(image))\n",
        "    axes[0].set_title('Original Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Loop through the layers and their heatmaps\n",
        "    for i, (heatmap, layer_name) in enumerate(zip(heatmaps, layer_names)):\n",
        "        ax = axes[i + 1]  # Start from index 1, since index 0 is for the original image\n",
        "        ax.imshow(heatmap, cmap='jet', alpha=0.5)  # Overlay the heatmap on the image\n",
        "        ax.set_title(f\"Heatmap: {layer_name}\")\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Hide any unused subplots (if number of heatmaps is not a multiple of num_columns)\n",
        "    for j in range(num_layers + 1, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    # Adjust layout and show the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSAPFWx0vyKq"
      },
      "outputs": [],
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpTXivGnvvTV"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "\n",
        "# Assuming 'device' is defined somewhere as 'cuda' or 'cpu'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load and preprocess image\n",
        "image_path = r\"DeepDRiD\\test\\347\\347_l1.jpg\"\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "# Add batch dimension (unsqueeze(0)) and move the tensor to the device\n",
        "input_tensor = transform(image).unsqueeze(0).to(device)  # Now shape will be [1, 3, 224, 224]\n",
        "print(input_tensor.shape)\n",
        "# Ensure the model is on the same device (GPU or CPU)\n",
        "model = model.to(device)\n",
        "\n",
        "# Initialize GradCAM and generate heatmap\n",
        "gradcam = GradCAM(model=model, target_layer_name='backbone.layer1')  # Adjust target_layer_name if needed\n",
        "heatmap = gradcam(input_tensor)  # Pass the correct shaped tensor to GradCAM\n",
        "\n",
        "# Visualize the result\n",
        "visualize_gradcam(image, heatmap)  # Assuming visualize_gradcam is defined\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NnkzBZRvwOp"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store heatmaps and layer names\n",
        "heatmaps = []\n",
        "layer_names = []\n",
        "\n",
        "# Loop through all the layers of the model\n",
        "for name, module in model.named_modules():\n",
        "    print(f\"Processing layer: {name}\")\n",
        "\n",
        "    try:\n",
        "        # Initialize GradCAM for the current layer\n",
        "        gradcam = GradCAM(model=model, target_layer_name=name)  # Use the current layer name\n",
        "\n",
        "        # Generate heatmap for the current layer\n",
        "        heatmap = gradcam(input_tensor)  # Pass the correct shaped tensor to GradCAM\n",
        "\n",
        "        # Store the heatmap and corresponding layer name\n",
        "        heatmaps.append(heatmap)\n",
        "        layer_names.append(name)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Print the error and continue with the next layer\n",
        "        print(f\"Error processing layer {name}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Visualize the heatmaps in a 3-column grid\n",
        "visualize_gradcam_full(image, heatmaps, layer_names)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}